{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Homework 1A - Task 24\n",
        "\n",
        "In this homework we aim to create a JSON file, starting from existing datas in order to be used from Generative Language Models.\n",
        "\n",
        "So strating from existing datasets we will convert them into a format useful for the evaluation of LLM."
      ],
      "metadata": {
        "id": "gwTmXQcG4CDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SENTIPOLC (SENTIment POLarity Classification) is a task presented at Evalita 2016, the fourth evaluation campaign of Natural Language Processing and Speech tools for Italian\n",
        "\n",
        "The main goal of SENTIPOLC is sentiment classification at message level on Italian tweets.\n",
        "\n",
        "The task is divided into three sub-tasks with an increasing level of complexity.\n",
        "\n",
        "1. **Subjectivity Classification**: Given a message, decide whether the message is subjective or objective.\n",
        "\n",
        "2. **Polarity Classification**: Given a message, decide whether the message is of positive, negative, neutral or mixed sentiment (i.e. conveying both a positive and negative sentiment).\n",
        "\n",
        "3. **Irony Detection**: Given a message, decide whether the message is ironic or not\n",
        "\n",
        "\n",
        "    id: 122449983151669248\n",
        "\n",
        "    labels: 1\t0\t1\t0\t0\t1\t1\n",
        "\n",
        "    text: Intanto la partita per Via Nazionale si complica. #Saccomanni dice che \"mica tutti sono Mario #Monti\" http://t.co/xPtNz4X7 via @linkiesta\n",
        "\n",
        "The labels correspond respectively to:\n",
        "\n",
        "* subj Subjectivity: possible values are 0 and 1.\n",
        "A subjective tweet will have subj = 1;\n",
        "an objective tweet subj = 0.\n",
        "\n",
        "* opos Positive overall polarity: possible values are 0 and 1.\n",
        "A tweet exhibiting positive polarity will have opos = 1;\n",
        "a tweet without positive polarity will have opos = 0.\n",
        "\n",
        "* oneg Negative overall polarity: possible values are 0 and 1.\n",
        "A tweet exhibiting negative polarity will have neg = 1;\n",
        "a tweet without negative polarity will have neg = 0.\n",
        "\n",
        "* iro Irony: possible values are 0 and 1.\n",
        "A tweet with an ironic twist will have iro = 1,\n",
        "otherwise iro = 0."
      ],
      "metadata": {
        "id": "0J0ine_g4FV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Imports"
      ],
      "metadata": {
        "id": "_Z4ucHHxCOxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import zipfile\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "\n",
        "url = [\"http://www.di.unito.it/~tutreeb/sentipolc-evalita16/training_set_sentipolc16.csv.zip\", \"http://www.di.unito.it/~tutreeb/sentipolc-evalita16/test_set_sentipolc16_gold2000.csv.zip\"]\n",
        "\n",
        "for u in url:\n",
        "  response = requests.get(u)\n",
        "\n",
        "  if response.status_code == 200:\n",
        "\n",
        "      with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
        "          zip_ref.extractall(\"/content\")\n",
        "      print(\"File \"+u+\" succesfully downloaded\")\n",
        "  else:\n",
        "      print(\"Error\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSopelCvuICi",
        "outputId": "42135428-f624-48f3-ac3d-fc60c1b3b45f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File http://www.di.unito.it/~tutreeb/sentipolc-evalita16/training_set_sentipolc16.csv.zip succesfully downloaded\n",
            "File http://www.di.unito.it/~tutreeb/sentipolc-evalita16/test_set_sentipolc16_gold2000.csv.zip succesfully downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Dataset"
      ],
      "metadata": {
        "id": "xnnLuT2kCSI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_path =\"/content/test_set_sentipolc16_gold2000.csv\"\n",
        "train_path = \"/content/training_set_sentipolc16.csv\"\n",
        "\n",
        "\n",
        "def import_data(path, train=True):\n",
        "  data=[]\n",
        "  if train:\n",
        "    with open(path, 'r') as file:\n",
        "      for line in file:\n",
        "        data.append([line[1:19], line[21:35], line[35:-1]])\n",
        "    return data\n",
        "  else:\n",
        "    with open(path, 'r') as file:\n",
        "      for line in file:\n",
        "        data.append([line[1:19], line[21:35].replace('\"', ''), line[49:-1]])\n",
        "        break\n",
        "    return data\n",
        "\n",
        "train_data = import_data(train_path, train=True)\n",
        "train_data.pop(0)\n",
        "test_data = import_data(test_path, train=False)"
      ],
      "metadata": {
        "id": "t8vKAFt4KXRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#First Task JSON"
      ],
      "metadata": {
        "id": "8hvfR36uCWFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def first_task_json(data, t):\n",
        "\n",
        "    all_records = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        record = {\n",
        "            \"id\": i,\n",
        "            \"text\": data[i][2],\n",
        "            \"choices\": ['Oggettiva', 'Soggettiva'],\n",
        "            \"label\": data[i][1][0],\n",
        "\n",
        "        }\n",
        "        all_records.append(record)\n",
        "\n",
        "    filename = \"haspeede3-task1-\" + t + \"-data.jsonl\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(all_records, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    return filename\n",
        "\n",
        "first_task_train = first_task_json(train_data, \"train\")\n",
        "first_task_test = first_task_json(test_data, \"test\")"
      ],
      "metadata": {
        "id": "MO9rK2LdR0NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Second Task JSON"
      ],
      "metadata": {
        "id": "8i9rHbQYCZ7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def second_task_json(data, t):\n",
        "\n",
        "    all_records = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        record = {\n",
        "            \"id\": i,\n",
        "            \"text\": data[i][2],\n",
        "            \"choices\": [\"Neutrale\" , \"Negativo\", \"Positivo\", \"Misto\"],\n",
        "            \"label\": int(data[i][1][2]) + (2 * int(data[i][1][4])),\n",
        "        }\n",
        "        all_records.append(record)\n",
        "\n",
        "    filename = \"haspeede3-task2-\" + t + \"-data.jsonl\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(all_records, json_file, ensure_ascii=False, indent=4)\n",
        "    return filename\n",
        "\n",
        "second_task_train = second_task_json(train_data, \"train\")\n",
        "second_task_test = second_task_json(test_data, \"test\")"
      ],
      "metadata": {
        "id": "zbPa67AvYjKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Third task JSON"
      ],
      "metadata": {
        "id": "hMa3eegoCcOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def third_task_json(data, t):\n",
        "\n",
        "    all_records = []\n",
        "\n",
        "    for i in range(len(data)):\n",
        "\n",
        "        record = {\n",
        "            \"id\": i,\n",
        "            \"text\": data[i][2],\n",
        "            \"choices\": ['Non Ironico', 'Ironico'],\n",
        "            \"label\": data[i][1][0],\n",
        "        }\n",
        "        all_records.append(record)\n",
        "\n",
        "    filename = \"haspeede3-task3-\" + t + \"-data.jsonl\"\n",
        "\n",
        "    with open(filename, 'w', encoding='utf-8') as json_file:\n",
        "        json.dump(all_records, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "    return filename\n",
        "\n",
        "third_task_train = third_task_json(train_data, \"train\")\n",
        "third_task_test = third_task_json(test_data, \"test\")"
      ],
      "metadata": {
        "id": "A9bm8ui28tgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Show input JSON"
      ],
      "metadata": {
        "id": "il5CPpsrCjep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path=\"/content\"\n",
        "\n",
        "for file in os.listdir(path):\n",
        "  if file.endswith(\".jsonl\"):\n",
        "    with open(file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        element = data[0]\n",
        "        print(element)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChP-WlOPCZWq",
        "outputId": "92a2a4d4-119f-45dc-80b3-cc58ff2c6f24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 0, 'text': '\"Tra 5 minuti presentazione piano scuola del governo #Renzi. #passodopopasso #labuonascuola Stay tuned\"', 'choices': ['Oggettiva', 'Soggettiva'], 'label': '0'}\n",
            "{'id': 0, 'text': '\"Intanto la partita per Via Nazionale si complica. #Saccomanni dice che \"\"mica tutti sono Mario #Monti\"\" http://t.co/xPtNz4X7 via @linkiesta\"', 'choices': ['Non Ironico', 'Ironico'], 'label': '1'}\n",
            "{'id': 0, 'text': '\"Tra 5 minuti presentazione piano scuola del governo #Renzi. #passodopopasso #labuonascuola Stay tuned\"', 'choices': ['Non Ironico', 'Ironico'], 'label': '0'}\n",
            "{'id': 0, 'text': '\"Intanto la partita per Via Nazionale si complica. #Saccomanni dice che \"\"mica tutti sono Mario #Monti\"\" http://t.co/xPtNz4X7 via @linkiesta\"', 'choices': ['Neutrale', 'Negativo', 'Positivo', 'Misto'], 'label': 2}\n",
            "{'id': 0, 'text': '\"Tra 5 minuti presentazione piano scuola del governo #Renzi. #passodopopasso #labuonascuola Stay tuned\"', 'choices': ['Neutrale', 'Negativo', 'Positivo', 'Misto'], 'label': 0}\n",
            "{'id': 0, 'text': '\"Intanto la partita per Via Nazionale si complica. #Saccomanni dice che \"\"mica tutti sono Mario #Monti\"\" http://t.co/xPtNz4X7 via @linkiesta\"', 'choices': ['Oggettiva', 'Soggettiva'], 'label': '1'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Create Prompt"
      ],
      "metadata": {
        "id": "HtrxplMkCpaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts1 = {\n",
        "    \"prompt_1\":\"Considerando la frase {text} questa frase è (Oggettiva) o (Soggettiva) ?\",\n",
        "    \"prompt_2\":\"Considerando l'affermazione {text}, trasmette una prospettiva (Oggettiva) o (Soggettiva) ?\",\n",
        "    \"prompt_3\":\"Considerando l'affermazione {text}, riflette sentimenti (Oggettiva) o (Soggettiva) ? \",\n",
        "}\n",
        "\n",
        "prompts2 = {\n",
        "    \"prompt_1\":\"Data la frase '{text}' questa esprime un sentimento (Neutro), (Negativo), (Positivo) o (Misto)?\",\n",
        "    \"prompt_2\":\"Considerando l'affermazione '{text}', questa esprime un sentimento (Neutro), (Negativo), (Positivo) o (Misto)?\",\n",
        "    \"prompt_3\":\"Considerando l'affermazione '{text}', riflette un sentimento (Neutro), (Negativo), (Positivo) o (Misto)?\",\n",
        "}\n",
        "\n",
        "prompts3 = {\n",
        "    \"prompt_1\":\"Considerando la frase '{text}' questa è (Non ironico) o (Ironico)?\",\n",
        "    \"prompt_2\":\"Considerando l'affermazione '{text}', è (Non Ironico) o (Ironico)?\",\n",
        "    \"prompt_3\":\"Considerando l'affermazione '{text}', riflette il contesto (Non Ironico) o (Ironico)?\",\n",
        "}\n",
        "prompts=[prompts1, prompts2, prompts3 ]\n",
        "\n",
        "for i in range(len(prompts)):\n",
        "  output_file = \"/content/haspeede3-task\" + str(i+1) + \"-prompt.jsonl\"\n",
        "\n",
        "  with open(output_file, 'w') as json_file:\n",
        "      json.dump(prompts[i], json_file, indent=2)"
      ],
      "metadata": {
        "id": "yXUO17_YxLR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test the prompt"
      ],
      "metadata": {
        "id": "uPIHbfj6zoLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768c881f-14f1-44cf-aa83-d8d4db0da7c8",
        "id": "SMNJd4REzoLw"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m409.9/409.9 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Collecting auto-gptq\n",
            "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.7.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.28.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.25.2)\n",
            "Collecting rouge (from auto-gptq)\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Collecting gekko (from auto-gptq)\n",
            "  Downloading gekko-1.1.0-py3-none-any.whl (13.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.2.1+cu121)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.39.3)\n",
            "Requirement already satisfied: peft>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.26.0->auto-gptq) (0.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->auto-gptq) (12.4.99)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
            "Installing collected packages: rouge, gekko, auto-gptq\n",
            "Successfully installed auto-gptq-0.7.1+cu118 gekko-1.1.0 rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fMUDx_LMzoLw",
        "outputId": "2c1f41d4-32eb-4f7f-8db8-57b25796bc86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import logging\n",
        "\n",
        "logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "    torch.cuda.set_device(device)\n",
        "    dev = 0\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    dev = -1\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-it-en\", device=dev)\n",
        "classifier = pipeline(\"zero-shot-classification\", model=\"xlm-roberta-large\", device=dev)\n",
        "\n",
        "path = \"/content/\""
      ],
      "metadata": {
        "id": "A9L1YMmq8yzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runtimeFlag = device\n",
        "cache_dir = None\n",
        "scaling_factor = 1.0"
      ],
      "metadata": {
        "id": "Q5ViDhRKHrQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Translation and Classification"
      ],
      "metadata": {
        "id": "KnFNULIM8wMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_results(prompt_path, task_path):\n",
        "\n",
        "  count_array = []\n",
        "  correct_array = []\n",
        "\n",
        "  with open(prompt_path, 'r') as file:\n",
        "\n",
        "      prompts = json.loads(file.read())\n",
        "\n",
        "  for i in range(1, len(prompts)+1):\n",
        "\n",
        "    with open(task_path, 'r') as file: #testare per tutti i file\n",
        "\n",
        "        inputs = json.loads(file.read())\n",
        "\n",
        "        count_input = 0\n",
        "        correct = 0\n",
        "\n",
        "        for input in inputs:\n",
        "\n",
        "          template = prompts['prompt_'+ str(i)]\n",
        "\n",
        "          text_to_insert = input['text']\n",
        "\n",
        "          compiled_string = template.format(text=text_to_insert)\n",
        "          compiled_string = translator(compiled_string, max_length=1024)[0]['translation_text']\n",
        "\n",
        "          token = tokenizer_llama(compiled_string, return_tensors=\"pt\").to(device)\n",
        "\n",
        "          output = model_llama(**token)\n",
        "\n",
        "          logits = torch.softmax(torch.tensor(output.logits[0].detach().cpu().numpy()), -1).detach().cpu().numpy()\n",
        "          count_input += 1\n",
        "\n",
        "          if str(np.argmax(logits)) == str(input['label']):\n",
        "            correct += 1\n",
        "\n",
        "          if count_input==100:\n",
        "\n",
        "            count_array.append(count_input)\n",
        "            correct_array.append(correct)\n",
        "\n",
        "            break\n",
        "  return  count_array,correct_array"
      ],
      "metadata": {
        "id": "0OFlO1ZBfO1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_results(correct_array, count_array, task):\n",
        "  print(\"File:\", task)\n",
        "  print(\"Accuracy with prompt 1\", correct_array[0] / count_array[0] *100)\n",
        "  print(\"Accuracy with prompt 2\", correct_array[1] / count_array[1] *100)\n",
        "  print(\"Accuracy with prompt 3\", correct_array[2] / count_array[2] *100)"
      ],
      "metadata": {
        "id": "bW-38E9Z5MJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_array, correct_array = compute_results(path + \"haspeede3-task1-prompt.jsonl\", path + first_task_train)\n",
        "print_results(correct_array, count_array, first_task_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bde926b8-777b-4141-a9db-e036579cdafc",
        "id": "nYAc1brl88_f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: first_task_train.json\n",
            "Accuracy with prompt 1 40.0\n",
            "Accuracy with prompt 2 28.000000000000004\n",
            "Accuracy with prompt 3 36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_array, correct_array = compute_results(path + \"haspeede3-task2-prompt.jsonl\", path + second_task_train)\n",
        "print_results(correct_array, count_array, second_task_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7537a47c-d563-425a-9505-c0858191ac26",
        "id": "bmMnwJkI88_z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: second_task_train.json\n",
            "Accuracy with prompt 1 23.0\n",
            "Accuracy with prompt 2 28.000000000000004\n",
            "Accuracy with prompt 3 27.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_array, correct_array = compute_results(path + \"haspeede3-task3-prompt.jsonl\", path + third_task_train)\n",
        "print_results(correct_array, count_array, third_task_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67d72409-b1c7-4dbd-8162-b3dd2ba3fe96",
        "id": "nKXV35Xr88_z"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: third_task_train.json\n",
            "Accuracy with prompt 1 47.0\n",
            "Accuracy with prompt 2 48.0\n",
            "Accuracy with prompt 3 56.00000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Zero shot classification"
      ],
      "metadata": {
        "id": "HTAbkNgg8z2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_results2(prompt_path, task_path, candidate_labels):\n",
        "\n",
        "  count_array = []\n",
        "  correct_array = []\n",
        "\n",
        "  with open(prompt_path, 'r') as file:\n",
        "\n",
        "      prompts = json.loads(file.read())\n",
        "\n",
        "  for i in range(1, len(prompts)+1):\n",
        "\n",
        "    with open(task_path, 'r') as file: #testare per tutti i file\n",
        "\n",
        "        inputs = json.loads(file.read())\n",
        "\n",
        "        count_input = 0\n",
        "        correct = 0\n",
        "\n",
        "        for input in inputs:\n",
        "\n",
        "          template = prompts['prompt_'+ str(i)]\n",
        "\n",
        "          text_to_insert = input['text']\n",
        "\n",
        "          compiled_string = template.format(text=text_to_insert)\n",
        "\n",
        "          output = classifier(compiled_string, candidate_labels=candidate_labels)\n",
        "\n",
        "          count_input += 1\n",
        "\n",
        "          if output['labels'][0] == candidate_labels[int(input['label'])]:\n",
        "            correct += 1\n",
        "\n",
        "          if count_input==100:\n",
        "\n",
        "            count_array.append(count_input)\n",
        "            correct_array.append(correct)\n",
        "\n",
        "            break\n",
        "\n",
        "  return  count_array,correct_array"
      ],
      "metadata": {
        "id": "9Lgbai0356GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_array, correct_array = compute_results2(path + \"haspeede3-task1-prompt.jsonl\", path + first_task_train, ['Oggettiva', 'Soggettiva'])\n",
        "print_results(correct_array, count_array, first_task_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74f14f7c-be57-43c7-d1e3-236869281bcd",
        "id": "AoVRnbBl5SbA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: first_task_train.json\n",
            "Accuracy with prompt 1 43.0\n",
            "Accuracy with prompt 2 59.0\n",
            "Accuracy with prompt 3 55.00000000000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_array, correct_array = compute_results2(path + \"haspeede3-task2-prompt.jsonl\", path + second_task_train, [\"Neutrale\" , \"Negativo\", \"Positivo\", \"Misto\"])\n",
        "print_results(correct_array, count_array, second_task_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14314e19-9e1b-472e-dc9e-b2c17c47a09f",
        "id": "_Ep0dDkM5SbB"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: second_task_train.json\n",
            "Accuracy with prompt 1 16.0\n",
            "Accuracy with prompt 2 12.0\n",
            "Accuracy with prompt 3 16.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_array, correct_array = compute_results2(path + \"haspeede3-task3-prompt.jsonl\", path + third_task_train, ['Non Ironico', 'Ironico'])\n",
        "print_results(correct_array, count_array, third_task_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01754b9d-924b-4aac-a554-a227fbdf99ac",
        "id": "5Y8_RQxl5SbW"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: third_task_train.json\n",
            "Accuracy with prompt 1 69.0\n",
            "Accuracy with prompt 2 68.0\n",
            "Accuracy with prompt 3 59.0\n"
          ]
        }
      ]
    }
  ]
}