{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0sPER-ui9HA"
      },
      "source": [
        "# 1.0 Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83qtMqTUka4o",
        "outputId": "a7f9eb6c-a8c7-45f7-f874-463329d80341"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langid in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.1.6)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from langid) (1.26.0)\n",
            "Requirement already satisfied: gensim in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (1.26.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (1.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: spacy in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.28.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy) (65.5.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy) (1.26.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\runpy.py\", line 188, in _run_module_as_main\n",
            "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\runpy.py\", line 147, in _get_module_details\n",
            "    return _get_module_details(pkg_main_name, error)\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\runpy.py\", line 111, in _get_module_details\n",
            "    __import__(pkg_name)\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\__init__.py\", line 13, in <module>\n",
            "    from . import pipeline  # noqa: F401\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\pipeline\\__init__.py\", line 1, in <module>\n",
            "    from .attributeruler import AttributeRuler\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\", line 8, in <module>\n",
            "    from ..language import Language\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\language.py\", line 43, in <module>\n",
            "    from .pipe_analysis import analyze_pipes, print_pipe_analysis, validate_attrs\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\pipe_analysis.py\", line 6, in <module>\n",
            "    from .tokens import Doc, Span, Token\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\tokens\\__init__.py\", line 1, in <module>\n",
            "    from ._serialize import DocBin\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\tokens\\_serialize.py\", line 14, in <module>\n",
            "    from ..vocab import Vocab\n",
            "  File \"spacy\\vocab.pyx\", line 1, in init spacy.vocab\n",
            "  File \"spacy\\tokens\\doc.pyx\", line 49, in init spacy.tokens.doc\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\schemas.py\", line 287, in <module>\n",
            "    class TokenPattern(BaseModel):\n",
            "  File \"pydantic\\main.py\", line 299, in pydantic.main.ModelMetaclass.__new__\n",
            "  File \"pydantic\\fields.py\", line 411, in pydantic.fields.ModelField.infer\n",
            "  File \"pydantic\\fields.py\", line 342, in pydantic.fields.ModelField.__init__\n",
            "  File \"pydantic\\fields.py\", line 451, in pydantic.fields.ModelField.prepare\n",
            "  File \"pydantic\\fields.py\", line 545, in pydantic.fields.ModelField._type_analysis\n",
            "  File \"pydantic\\fields.py\", line 550, in pydantic.fields.ModelField._type_analysis\n",
            "  File \"c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\typing.py\", line 852, in __subclasscheck__\n",
            "    return issubclass(cls, self.__origin__)\n",
            "TypeError: issubclass() arg 1 must be a class\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy_fastlang in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.1.0)\n",
            "Requirement already satisfied: fasttext-wheel<0.10.0,>=0.9.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy_fastlang) (0.9.2)\n",
            "Requirement already satisfied: spacy<4.0.0,>=3.0.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy_fastlang) (3.7.4)\n",
            "Requirement already satisfied: pybind11>=2.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fasttext-wheel<0.10.0,>=0.9.2->spacy_fastlang) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.10_3.10.3056.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from fasttext-wheel<0.10.0,>=0.9.2->spacy_fastlang) (65.5.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fasttext-wheel<0.10.0,>=0.9.2->spacy_fastlang) (1.26.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (4.64.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.28.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (3.1.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from spacy<4.0.0,>=3.0.0->spacy_fastlang) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (3.0.9)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.0.0->spacy_fastlang) (4.8.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (2022.9.24)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.1.4)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.4.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from typer<0.10.0,>=0.3.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (8.1.3)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<4.0.0,>=3.0.0->spacy_fastlang) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->spacy<4.0.0,>=3.0.0->spacy_fastlang) (2.1.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install langid\n",
        "!pip install gensim\n",
        "!pip install -U spacy\n",
        "!python -m spacy download it_core_news_sm\n",
        "# !wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "# !tar -xzvf s2v_reddit_2015_md.tar.gz\n",
        "# !pip install sense2vec\n",
        "!pip install spacy_fastlang"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-3BMXNIjGo0",
        "outputId": "4f4029be-8466-4362-9d69-eb3f50431962"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "issubclass() arg 1 must be a class",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlangid\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Word2Vec\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy_fastlang\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# from sense2vec import Sense2Vec\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, prefer_gpu, require_cpu, require_gpu  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabout\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\pipeline\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattributeruler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AttributeRuler\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdep_parser\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DependencyParser\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01medit_tree_lemmatizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EditTreeLemmatizer\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m util\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlanguage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Language\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Matcher\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\language.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlang\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenizer_exceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_EXCEPTIONS, URL_MATCH\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlookups\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_lookups\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpipe_analysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m analyze_pipes, print_pipe_analysis, validate_attrs\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschemas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     45\u001b[0m     ConfigSchema,\n\u001b[0;32m     46\u001b[0m     ConfigSchemaInit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m     validate_init_settings,\n\u001b[0;32m     50\u001b[0m )\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Scorer\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\pipe_analysis.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwasabi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m msg\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc, Span, Token\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dot_to_dict\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# This lets us add type hints for mypy etc. without causing circular imports\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\tokens\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_serialize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DocBin\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Doc\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmorphanalysis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MorphAnalysis\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\tokens\\_serialize.py:14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Errors\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleFrozenList, ensure_path\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dict_proxies\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SpanGroups\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdoc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DOCBIN_ALL_ATTRS \u001b[38;5;28;01mas\u001b[39;00m ALL_ATTRS\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\vocab.pyx:1\u001b[0m, in \u001b[0;36minit spacy.vocab\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\tokens\\doc.pyx:49\u001b[0m, in \u001b[0;36minit spacy.tokens.doc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\spacy\\schemas.py:287\u001b[0m\n\u001b[0;32m    281\u001b[0m UnderscoreValue \u001b[38;5;241m=\u001b[39m Union[\n\u001b[0;32m    282\u001b[0m     TokenPatternString, TokenPatternNumber, \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    283\u001b[0m ]\n\u001b[0;32m    284\u001b[0m IobValue \u001b[38;5;241m=\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m--> 287\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTokenPattern\u001b[39;00m(BaseModel):\n\u001b[0;32m    288\u001b[0m     orth: Optional[StringValue] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    289\u001b[0m     text: Optional[StringValue] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\pydantic\\main.py:299\u001b[0m, in \u001b[0;36mpydantic.main.ModelMetaclass.__new__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\pydantic\\fields.py:411\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.infer\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\pydantic\\fields.py:342\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.__init__\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\pydantic\\fields.py:451\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField.prepare\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\pydantic\\fields.py:545\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\pydantic\\fields.py:550\u001b[0m, in \u001b[0;36mpydantic.fields.ModelField._type_analysis\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\typing.py:852\u001b[0m, in \u001b[0;36m_SpecialGenericAlias.__subclasscheck__\u001b[1;34m(self, cls)\u001b[0m\n\u001b[0;32m    850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__origin__, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__origin__)\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mcls\u001b[39m, _GenericAlias):\n\u001b[1;32m--> 852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43missubclass\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__origin__\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__subclasscheck__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n",
            "\u001b[1;31mTypeError\u001b[0m: issubclass() arg 1 must be a class"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "\n",
        "import re\n",
        "import json\n",
        "import gensim.downloader as api\n",
        "\n",
        "import langid\n",
        "from gensim.models import Word2Vec\n",
        "import spacy\n",
        "import spacy_fastlang\n",
        "\n",
        "# from sense2vec import Sense2Vec\n",
        "import random\n",
        "\n",
        "import torch\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import RobertaForMultipleChoice\n",
        "from torch.distributions import Categorical\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# import nltk\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('wordnet')\n",
        "# from nltk.corpus import wordnet as wn\n",
        "\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Italian vocabulary has been extracted and saved to italian_vocabulary.txt\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from langdetect import detect\n",
        "\n",
        "def is_italian(word):\n",
        "    try:\n",
        "        # Check if the detected language is Italian\n",
        "        return detect(word) == 'it'\n",
        "    except:\n",
        "        # If language detection fails, assume it's not Italian\n",
        "        return False\n",
        "\n",
        "def filter_italian_vocabulary(input_file, output_file):\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    \n",
        "    italian_words = [word.strip() for word in lines if is_italian(word.strip())]\n",
        "    \n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(italian_words))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"vocabulary/1B.italian.vocabulary.txt\"  # Change this to your input vocabulary file\n",
        "    output_file = \"italian_vocabulary.txt\"  # Change this to the desired output file\n",
        "    \n",
        "    filter_italian_vocabulary(input_file, output_file)\n",
        "    print(\"Italian vocabulary has been extracted and saved to\", output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWomv2Y0g-l_"
      },
      "source": [
        "# 2.0 Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "M9yPG6KyH1dx"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path, gold_path):\n",
        "  count = 0\n",
        "  hypernyms_dict = {}\n",
        "  with open(data_path, \"r\", encoding = 'utf-8') as data_file, open(gold_path, \"r\", encoding = 'utf-8') as gold_file:\n",
        "    for data_line, gold_line in zip(data_file, gold_file):\n",
        "      term_list = [term for term in data_line.split()[:-1]]\n",
        "      term = \" \".join(term_list)\n",
        "      hypernyms = [hypernym.replace(\"\\n\", \"\") for hypernym in gold_line.split(\"\\t\")]\n",
        "      hypernyms_dict[term] = hypernyms\n",
        "      count += 1\n",
        "\n",
        "      if count == 20:\n",
        "        break\n",
        "\n",
        "  return hypernyms_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oWNvGVp9XdIx"
      },
      "outputs": [],
      "source": [
        "#  PARTIAL italian training data\n",
        "train_hypernyms = load_data(\"1B.italian.training.data.txt\", \"1B.italian.training.gold.txt\")\n",
        "\n",
        "# PARTIAL italian test data\n",
        "test_hypernyms = load_data(\"1B.italian.test.data.txt\", \"1B.italian.test.gold.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff5gVEnfg6Lp"
      },
      "source": [
        "# 3.0 Find Distractors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SIts6mWLj_1"
      },
      "source": [
        "## 3.1 Fasttext\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OHRYBJYpLpYi",
        "outputId": "12affec6-2bb2-425f-f3bd-0c1c250fbf15"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/facebookresearch/fastText.git\n",
        "# !cd fastText\n",
        "# !sudo python setup.py install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVpKhfWuYzKN"
      },
      "source": [
        "### Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "K6SnidfZLq7S"
      },
      "outputs": [],
      "source": [
        "import fasttext.util\n",
        "# fasttext.util.download_model('it', if_exists='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xh530uT4Lwjn",
        "outputId": "a65e7903-4ede-4c6a-ae2d-64964387b926"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        }
      ],
      "source": [
        "model = fasttext.load_model('cc.it.300.bin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Define the find_distractors function with filtering for diminutives and augmentatives\n",
        "def find_distractors(word, num_distractors, model):\n",
        "    distractors = []\n",
        "\n",
        "    # Convert the target word to lowercase\n",
        "    word = word.lower()\n",
        "\n",
        "    # Get the nearest neighbors of the word from the FastText model\n",
        "    nearest_neighbors = model.get_nearest_neighbors(word, k=num_distractors * 10)  # Fetch more neighbors to account for filtering\n",
        "\n",
        "    # Filter out diminutives and augmentatives, and ensure semantic relevance\n",
        "    for neighbor in nearest_neighbors:\n",
        "        candidate_word = neighbor[1].lower()  # Convert candidate word to lowercase\n",
        "        # Filter out words with diminutive or augmentative suffixes and words containing the original word as a substring\n",
        "        if (not re.search(r'(ino|etto|ello|one|accio|astro)$', candidate_word)) and (word not in candidate_word):\n",
        "            # Ensure semantic relevance by measuring cosine similarity with target word\n",
        "            similarity = model.get_word_vector(word).dot(model.get_word_vector(candidate_word))\n",
        "            if similarity >= 0.1 and candidate_word != word:  # Adjust the threshold as needed\n",
        "                distractors.append(candidate_word)\n",
        "\n",
        "        if len(distractors) == num_distractors:\n",
        "            break\n",
        "\n",
        "    return distractors[:num_distractors]  # Return only the specified number of distractors\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training and Finetunning fasttext - Didn't work because our custom vocabulary is to sparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import fasttext\n",
        "\n",
        "# # Load the existing FastText model\n",
        "# existing_model_path = 'cc.it.300.bin'\n",
        "# existing_model = fasttext.load_model(existing_model_path)\n",
        "\n",
        "# # Load the new dictionary\n",
        "# new_dictionary_path = 'italian_vocabulary.txt'\n",
        "# with open(new_dictionary_path, 'r', encoding='utf-8') as f:\n",
        "#     new_dictionary = [word.strip() for word in f]\n",
        "\n",
        "# # Extract word embeddings for words in the new dictionary from the existing model\n",
        "# new_embeddings = {}\n",
        "# for word in new_dictionary:\n",
        "#     if word in existing_model.words:\n",
        "#         new_embeddings[word] = existing_model.get_word_vector(word)\n",
        "\n",
        "# # Write the word embeddings to a temporary file\n",
        "# temp_training_data_path = 'temp_training_data.txt'\n",
        "# with open(temp_training_data_path, 'w', encoding='utf-8') as f:\n",
        "#     for word, embedding in new_embeddings.items():\n",
        "#         embedding_str = ' '.join(str(x) for x in embedding)\n",
        "#         f.write(f'{word} {embedding_str}\\n')\n",
        "\n",
        "# # Train an unsupervised FastText model using the word embeddings\n",
        "# unsupervised_model_path = 'unsupervised_model.bin'\n",
        "# model = fasttext.train_unsupervised(temp_training_data_path, model='skipgram')\n",
        "\n",
        "# # Save the unsupervised model\n",
        "# model.save_model(unsupervised_model_path)\n",
        "\n",
        "# print(\"Unsupervised FastText model trained and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine tunning fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import fasttext\n",
        "\n",
        "# # Load the existing FastText model\n",
        "# existing_model_path = 'cc.it.300.bin'\n",
        "\n",
        "# existing_model = fasttext.load_model(existing_model_path)\n",
        "# print(existing_model.get_dimension())\n",
        "\n",
        "# # Reduce the dimensionality of the existing model\n",
        "# fasttext.util.reduce_model(existing_model, 582)\n",
        "# print(existing_model.get_dimension())\n",
        "\n",
        "# # Path to the new training data file\n",
        "# new_training_data_path = 'italian_vocabulary.txt'\n",
        "\n",
        "# # Specify the reduced dimension\n",
        "# pretrained_dim = 582\n",
        "\n",
        "# # Fine-tune the existing model with new training data\n",
        "# updated_model = fasttext.train_supervised(\n",
        "#     input=new_training_data_path,\n",
        "#     pretrainedVectors=existing_model_path,\n",
        "#     dim=pretrained_dim,\n",
        "#     epoch=10,\n",
        "#     lr=0.1,\n",
        "#     wordNgrams=2\n",
        "# )\n",
        "\n",
        "# # Save the updated model\n",
        "# updated_model_path = 'updated_model.bin'\n",
        "# updated_model.save_model(updated_model_path)\n",
        "\n",
        "# print(\"Updated FastText model saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# import fasttext.util\n",
        "# model = fasttext.load_model('updated_model.bin')\n",
        "# # Define the word for which you want to find distractors\n",
        "# word = \"numero ordinale\"\n",
        "# distractors = find_distractors(word, 3, model)\n",
        "\n",
        "# # Print the distractors\n",
        "# print(\"Distractors for '{}':\".format(word))\n",
        "# for i, distractor in enumerate(distractors, 1):\n",
        "#     print(\"{}. {}\".format(i, distractor))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Filter in order to use only hypernims "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import fasttext\n",
        "# import re\n",
        "\n",
        "# # Load FastText model\n",
        "# model = fasttext.load_model('cc.it.300.bin')\n",
        "\n",
        "# # Define the find_distractors function with filtering for similar words using FastText\n",
        "# def find_distractors(word, num_distractors, model):\n",
        "#     distractors = []\n",
        "\n",
        "#     # Get the nearest neighbors of the word from the FastText model\n",
        "#     nearest_neighbors = model.get_nearest_neighbors(word, k=num_distractors * 10)  # Fetch more neighbors to account for filtering\n",
        "\n",
        "#     # Filter out the original word and ensure semantic relevance\n",
        "#     for neighbor in nearest_neighbors:\n",
        "#         candidate_word = neighbor[1]\n",
        "#         if candidate_word != word:  # Exclude the word itself\n",
        "#             distractors.append(candidate_word)\n",
        "\n",
        "#         if len(distractors) == num_distractors:\n",
        "#             break\n",
        "\n",
        "#     return distractors[:num_distractors]  # Return only the specified number of distractors\n",
        "\n",
        "# # Example usage:\n",
        "# word = \"numero ordinale\"\n",
        "# num_distractors = 5\n",
        "# distractors = find_distractors(word, num_distractors, model)\n",
        "# print(\"Distractors:\", distractors)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhD8vekoRgIL"
      },
      "source": [
        "### Example usage of fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYnIStdDL1Is",
        "outputId": "6b156975-5570-487f-a0ba-7feba2b9fec2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.69635009765625, 'dinale'),\n",
              " (0.6637539267539978, 'tecnicoeconomica'),\n",
              " (0.6575295329093933, 'numeroe'),\n",
              " (0.6210983991622925, 'ordinal'),\n",
              " (0.617618203163147, 'Ordinale'),\n",
              " (0.6075699925422668, 'Numeroordinale'),\n",
              " (0.6015861630439758, 'Numerocardinale'),\n",
              " (0.5819660425186157, 'estetologo'),\n",
              " (0.5808798670768738, 'Sindacatore'),\n",
              " (0.5799951553344727, 'anticardinale')]"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_nearest_neighbors('numero ordinale')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMYnnwyzL5n6",
        "outputId": "af942918-4bb0-4d08-e337-28e57ef7e214"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0.521864652633667, 'iperonimi'),\n",
              " (0.4659070670604706, 'iperonimia'),\n",
              " (0.45994022488594055, 'iperone'),\n",
              " (0.447393000125885, 'numeronimo'),\n",
              " (0.4446645975112915, 'alnumero'),\n",
              " (0.43778160214424133, 'scombro'),\n",
              " (0.4214670956134796, 'iponimo'),\n",
              " (0.4148273766040802, 'dizionarietto'),\n",
              " (0.41480669379234314, 'ilnumero'),\n",
              " (0.413175493478775, 'coronimo')]"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.get_analogies(\"numero\", \"ordinale\", \"iperonimo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM9zDcnbL-ut"
      },
      "source": [
        "# 4.0 Create entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "QMejxfx46FGi"
      },
      "outputs": [],
      "source": [
        "def save_jsonl(file_path, data):\n",
        "  id_seq = 0\n",
        "  with open(file_path, \"w\") as output_file:\n",
        "    for term, hypernyms in data.items():\n",
        "      for hypernym in hypernyms:\n",
        "        distractors = find_distractors(hypernym, 3, model)\n",
        "        entries = (hypernym, *distractors)\n",
        "        choices = list(entries)\n",
        "        random.shuffle(choices) # to create randomness\n",
        "        reformatted_json_data = {\n",
        "              'id' : id_seq,\n",
        "              'text': term,\n",
        "              'choices': choices,\n",
        "              'label' : choices.index(hypernym)\n",
        "        }\n",
        "        json.dump(reformatted_json_data, output_file)\n",
        "        output_file.write(\"\\n\")\n",
        "        id_seq +=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_lines_jsonl(file_path, num_lines):\n",
        "  with open(file_path, 'r') as f:\n",
        "    json_list = list(f)\n",
        "    for line in json_list[:num_lines]:\n",
        "      data = json.loads(line)\n",
        "      print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bfbw5MqpVqb",
        "outputId": "b6a2945e-d66f-4595-ba15-06b7af6cbc57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'text': 'sesto', 'choices': ['capace', 'capaci', 'grado', 'poter'], 'label': 2}\n",
            "{'id': 1, 'text': 'sesto', 'choices': ['ordinal', 'dinale', 'numeroe', 'numero ordinale'], 'label': 3}\n",
            "{'id': 2, 'text': 'sesto', 'choices': ['frazioncina', 'borgata', 'frazione', 'frazioni'], 'label': 2}\n",
            "{'id': 3, 'text': 'sesto', 'choices': ['carica', 'batteria', 'ricoperta', 'cariche'], 'label': 0}\n",
            "{'id': 4, 'text': 'Sigillo', 'choices': ['denominatore', 'comunale', 'comune', 'comuni'], 'label': 2}\n",
            "{'id': 5, 'text': 'Sigillo', 'choices': ['municipalita', 'municipalit', 'municipalità', 'municipalita'], 'label': 2}\n",
            "{'id': 6, 'text': 'Sigillo', 'choices': ['comune italiano', 'monale', 'comune-di.it', 'sirente'], 'label': 0}\n",
            "{'id': 7, 'text': 'Sigillo', 'choices': ['frazioncina', 'frazioni', 'frazione', 'borgata'], 'label': 2}\n",
            "{'id': 8, 'text': 'Sigillo', 'choices': ['borgo', 'paese', 'villaggio', 'paesotto'], 'label': 1}\n",
            "{'id': 9, 'text': 'Sigillo', 'choices': ['periferia', 'quartiere', 'quartieri', 'sobborgo'], 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "save_jsonl(\"hypernym_discovery-task26-train-data.jsonl\", train_hypernyms)\n",
        "read_lines_jsonl(\"hypernym_discovery-task26-train-data.jsonl\", num_lines = 10) # preview of the first 10 lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "UGJEipDX2L7s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'text': 'sesto', 'choices': ['capaci', 'poter', 'grado', 'capace'], 'label': 2}\n",
            "{'id': 1, 'text': 'sesto', 'choices': ['dinale', 'numeroe', 'numero ordinale', 'ordinal'], 'label': 2}\n",
            "{'id': 2, 'text': 'sesto', 'choices': ['frazioncina', 'frazioni', 'frazione', 'borgata'], 'label': 2}\n",
            "{'id': 3, 'text': 'sesto', 'choices': ['ricoperta', 'batteria', 'cariche', 'carica'], 'label': 3}\n",
            "{'id': 4, 'text': 'Sigillo', 'choices': ['denominatore', 'comuni', 'comune', 'comunale'], 'label': 2}\n",
            "{'id': 5, 'text': 'Sigillo', 'choices': ['municipalità', 'municipalita', 'municipalit', 'municipalita'], 'label': 0}\n",
            "{'id': 6, 'text': 'Sigillo', 'choices': ['monale', 'comune italiano', 'sirente', 'comune-di.it'], 'label': 1}\n",
            "{'id': 7, 'text': 'Sigillo', 'choices': ['frazioni', 'frazione', 'borgata', 'frazioncina'], 'label': 1}\n",
            "{'id': 8, 'text': 'Sigillo', 'choices': ['paesotto', 'villaggio', 'paese', 'borgo'], 'label': 2}\n",
            "{'id': 9, 'text': 'Sigillo', 'choices': ['quartiere', 'periferia', 'sobborgo', 'quartieri'], 'label': 0}\n"
          ]
        }
      ],
      "source": [
        "# test jsonl file\n",
        "save_jsonl(\"hypernym_discovery-task26-test-data.jsonl\", train_hypernyms)\n",
        "read_lines_jsonl(\"hypernym_discovery-task26-test-data.jsonl\", num_lines = 10) # preview of the first 10 lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpnSwrev0jUA"
      },
      "source": [
        "# 5.0 Prompt formulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvgSvHol0mLk"
      },
      "outputs": [],
      "source": [
        "prompts = [\n",
        "    \"Il termine '{text}' può essere iperonimo di: \\n a) {option1} \\n b) {option2} \\n c) {option3} \\n d) {option4}\",\n",
        "    \"Dato il termine '{text}', quale tra le seguenti parole è un suo iperonimo? \\n a) {option1} \\n b) {option2} \\n c) {option3} \\n d) {option4}\",\n",
        "    \"Scegli l'iperonimo del termine '{text}': \\n a) {option1} \\n b) {option2} \\n c) {option3} \\n d) {option4}\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoZJb-dE7lkN"
      },
      "outputs": [],
      "source": [
        "print(' '.join(prompt + '\\n\\n' for prompt in prompts), end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqqCjLLSAUMy"
      },
      "outputs": [],
      "source": [
        "def save_prompts_jsonl(prompts, file_path):\n",
        "  json_prompts = []\n",
        "  for prompt in prompts:\n",
        "    json_prompts.append({\"prompt\": prompt})\n",
        "\n",
        "  with open(file_path, \"w\") as output_file:\n",
        "    for json_prompt in json_prompts:\n",
        "      json.dump(json_prompt, output_file)\n",
        "      output_file.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMnn5M3GBVOS"
      },
      "outputs": [],
      "source": [
        "save_prompts_jsonl(prompts, \"hypernym_discovery-task26-json.jsonl\")\n",
        "read_lines_jsonl(\"hypernym_discovery-task26-json.jsonl\", num_lines = 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0sLMr7iCJJy"
      },
      "source": [
        "# 6.0 Prompts Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31T-DzisjeHA",
        "outputId": "d5f41f9e-390c-47b1-ffe3-7d264d26c2e2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31xt8WrSjhTH"
      },
      "outputs": [],
      "source": [
        "def evaluate_prompt(tokenizer, model, prompt, data_file, zero_shot_classification = False):\n",
        "  formatted_prompts, y_true, y_pred, score = [], [], [], []\n",
        "  lines = 0\n",
        "\n",
        "  with open(data_file, \"r\") as f:\n",
        "    json_data = list(f)\n",
        "    for line in json_data:\n",
        "      pair = json.loads(line)\n",
        "      id = pair['id']\n",
        "      text = pair['text']\n",
        "      choices = pair['choices']\n",
        "      label = pair['label']\n",
        "\n",
        "      formatted_prompt = prompt.format(text = text, option1 = choices[0], option2 = choices[1], option3 = choices[2], option4 = choices[3])\n",
        "      inputs = tokenizer([formatted_prompt] * len(choices), choices,\n",
        "                         padding = True, return_tensors = \"pt\").to(device)\n",
        "\n",
        "      if zero_shot_classification:\n",
        "        output = model(inputs, candidate_labels=choices, hypothesis_template=\"Questo esempio è {}.\")\n",
        "        predicted_output = output['labels'][0]\n",
        "        predicted_label = choices.index(predicted_output)\n",
        "        prediction_score = output['scores'][0]\n",
        "        lines += 1\n",
        "\n",
        "        if lines == 20:\n",
        "          break\n",
        "\n",
        "      else:\n",
        "        labels = torch.tensor(len(choices) - 1).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "          output = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels = labels)\n",
        "\n",
        "        probabilities = torch.softmax(output[\"logits\"][0], -1).tolist()\n",
        "        predicted_label = np.argmax(probabilities)\n",
        "        prediction_score = probabilities[predicted_label]\n",
        "        lines += 1\n",
        "\n",
        "        if lines == 20:\n",
        "          break\n",
        "\n",
        "      formatted_prompts.append(formatted_prompt)\n",
        "      y_true.append(label)\n",
        "      y_pred.append(predicted_label)\n",
        "      score.append(prediction_score)\n",
        "\n",
        "    return formatted_prompts, y_true, y_pred, score\n",
        "\n",
        "\n",
        "def visualize_results(results, num_results):\n",
        "  for prompt in range(len(results)):\n",
        "    for n in range(num_results):\n",
        "      print(\"Prompt: \", results[prompt][0][n])\n",
        "      print(\"True label: \", results[prompt][1][n])\n",
        "      print(\"Predicted label: \", results[prompt][2][n])\n",
        "      print(\"Prediction score: \", round(results[prompt][3][n], 3))\n",
        "      print(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beXPGvlB9MTB"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(y_true, y_pred):\n",
        "  accuracy = accuracy_score(y_true, y_pred)\n",
        "  precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "  recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "  f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
        "  cf_matrix = confusion_matrix(y_true, y_pred)\n",
        "  return accuracy, precision, recall, f1, cf_matrix\n",
        "\n",
        "\n",
        "def print_confusion_matrix(metrics, type):\n",
        "  for n in range(len(metrics)):\n",
        "    print(f\"{type} Confusion Matrix for the Prompt {n}\")\n",
        "    print(\"Prompt: \", prompts[n])\n",
        "\n",
        "    # Confusion Matrix Plot\n",
        "    cf_matrix = metrics[n][4]\n",
        "    fig, ax = plt.subplots(figsize=(5, 3))\n",
        "    sns.heatmap(cf_matrix, annot = True, fmt = '.0f')\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Total Predictions: \", np.sum(cf_matrix))\n",
        "    print(\"Correct Predictions: \", np.trace(cf_matrix))\n",
        "    print(\"----------------------------------------------------------------\")\n",
        "    print(\" \")\n",
        "\n",
        "def print_overall_statistics(train_metrics, test_metrics, prompts):\n",
        "  comparison_table = []\n",
        "  for id, prompt in enumerate(prompts):\n",
        "    data = {}\n",
        "    for dtype, metrics in zip(['Train', 'Test'], [train_metrics, test_metrics]):\n",
        "      accuracy, precision, recall, f1, cf_matrix = metrics[id]\n",
        "      data[f'{dtype} Accuracy'] = round(accuracy, 3)\n",
        "      data[f'{dtype} Precision'] = round(precision, 3)\n",
        "      data[f'{dtype} Recall'] = round(recall, 3)\n",
        "      data[f'{dtype} F1-score'] = round(f1, 3)\n",
        "    comparison_table.append(data)\n",
        "  return pd.DataFrame(comparison_table).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSHhbtQhQAXY"
      },
      "source": [
        "## 6.1 RoBERTa For Multiple Choice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zq2H7HsuWse"
      },
      "outputs": [],
      "source": [
        "model_name = \"LIAMF-USP/roberta-large-finetuned-race\"\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
        "roberta_model = RobertaForMultipleChoice.from_pretrained(model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "so51vzKd8LnZ",
        "outputId": "dbc29402-342d-4b53-de1c-65c35dc7e1b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time:  1.9569334228833517 minutes\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "roberta_train_results, roberta_test_results = [], []\n",
        "\n",
        "for prompt in prompts:\n",
        "  formatted_prompt, y_true, y_pred, score = evaluate_prompt(roberta_tokenizer, roberta_model, prompt, \"hypernym_discovery-task26-train-data.jsonl\")\n",
        "  roberta_train_results.append([formatted_prompt, y_true, y_pred, score])\n",
        "\n",
        "  formatted_prompt, y_true, y_pred, score = evaluate_prompt(roberta_tokenizer, roberta_model, prompt, \"hypernym_discovery-task26-test-data.jsonl\")\n",
        "  roberta_test_results.append([formatted_prompt, y_true, y_pred, score])\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Execution Time: \", (end_time - start_time)/60, \"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1oXAKZBsZ72"
      },
      "outputs": [],
      "source": [
        "visualize_results(roberta_train_results, num_results = 2) # preview of the first 2 train results for each prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfk8jfMDvyo3"
      },
      "outputs": [],
      "source": [
        "roberta_train_metrics, roberta_test_metrics = [], []\n",
        "\n",
        "for prompt in range(len(prompts)):\n",
        "  accuracy, precision, recall, f1, cf_matrix = compute_metrics(roberta_train_results[prompt][1], roberta_train_results[prompt][2])\n",
        "  roberta_train_metrics.append([accuracy, precision, recall, f1, cf_matrix])\n",
        "\n",
        "  accuracy, precision, recall, f1, cf_matrix = compute_metrics(roberta_test_results[prompt][1], roberta_test_results[prompt][2])\n",
        "  roberta_test_metrics.append([accuracy, precision, recall, f1, cf_matrix])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNljtPY8PxS"
      },
      "source": [
        "### Overall Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mseiH-1S8RCE"
      },
      "outputs": [],
      "source": [
        "# Train Statistics for each prompt\n",
        "print_confusion_matrix(roberta_train_metrics, \"Train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8AfPh9T8S5A"
      },
      "outputs": [],
      "source": [
        "# Test Statistics for each prompt\n",
        "print_confusion_matrix(roberta_test_metrics, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-oflTOA8bFt"
      },
      "outputs": [],
      "source": [
        "print_overall_statistics(roberta_train_metrics, roberta_test_metrics, prompts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9YoFnhIcsEx"
      },
      "source": [
        "## 6.2 Zero Shot Text Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFeCM8kZSlRY"
      },
      "outputs": [],
      "source": [
        "classifier_names = [\"xlm-roberta-large\", \"facebook/bart-large-mnli\"]\n",
        "#roberta_classifier = pipeline(\"zero-shot-classification\", model=classifier_names[0], batch_size = 8, truncation=True, device = device)\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
        "bert_classifier = pipeline(\"zero-shot-classification\", model=classifier_names[1], batch_size = 8, truncation=True, device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wyqqRtO-51n",
        "outputId": "0a6a8bf0-0f79-4ec6-a3e0-e1907865f374"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time:  0.9809847831726074 minutes\n"
          ]
        }
      ],
      "source": [
        "start_time = time.time()\n",
        "classifier_train_results, classifier_test_results = [], []\n",
        "\n",
        "for prompt in prompts:\n",
        "  formatted_prompt, y_true, y_pred, score = evaluate_prompt(bert_tokenizer, bert_classifier, prompt, \"hypernym_discovery-task26-train-data.jsonl\", True)\n",
        "  classifier_train_results.append([formatted_prompt, y_true, y_pred, score])\n",
        "\n",
        "  formatted_prompt, y_true, y_pred, score = evaluate_prompt(bert_tokenizer, bert_classifier, prompt, \"hypernym_discovery-task26-test-data.jsonl\", True)\n",
        "  classifier_test_results.append([formatted_prompt, y_true, y_pred, score])\n",
        "\n",
        "end_time = time.time()\n",
        "print(\"Execution Time: \", (end_time - start_time)/60, \"minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqIUvwZFhEHe"
      },
      "outputs": [],
      "source": [
        "visualize_results(classifier_train_results, num_results = 2) # preview of the first 2 train results for each prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PAkEwGFgBlH"
      },
      "outputs": [],
      "source": [
        "classifier_train_metrics, classifier_test_metrics = [], []\n",
        "\n",
        "for prompt in range(len(prompts)):\n",
        "  accuracy, precision, recall, f1, cf_matrix = compute_metrics(classifier_train_results[prompt][1], classifier_train_results[prompt][2])\n",
        "  classifier_train_metrics.append([accuracy, precision, recall, f1, cf_matrix])\n",
        "\n",
        "  accuracy, precision, recall, f1, cf_matrix = compute_metrics(classifier_test_results[prompt][1], classifier_test_results[prompt][2])\n",
        "  classifier_test_metrics.append([accuracy, precision, recall, f1, cf_matrix])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQSyzYZjuga5"
      },
      "source": [
        "### Overall Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixERGlZ4gYEW"
      },
      "outputs": [],
      "source": [
        "# Train Confusion Matrix for each prompt\n",
        "print_confusion_matrix(classifier_train_metrics, \"Train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qTYO4Yr3vOK"
      },
      "outputs": [],
      "source": [
        "# Test Statistics for each prompt\n",
        "print_confusion_matrix(classifier_test_metrics, \"Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGF0DnMDkWHh"
      },
      "outputs": [],
      "source": [
        "print_overall_statistics(classifier_train_metrics, classifier_test_metrics, prompts)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
