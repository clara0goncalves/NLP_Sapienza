{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FvZqaH1qRX"
      },
      "source": [
        "# **Homework 1 - task 25**\n",
        "The main goal of this homework is to transform existing evaluation datasets into a format suitable for evaluating the linguistic skills of Large Language Models (LLMs) by reframing tasks as multi-choice Question Answering (QA) tasks, providing effective prompts, and generating distractors where necessary, all formatted in JSON Lines standard for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwL2mdH9KgG",
        "outputId": "2b4cad03-1874-4172-e207-09efca0537f0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_rtvMA3HO2"
      },
      "source": [
        "# **1. Data Loading:**\n",
        "\n",
        "\n",
        "First we need to download the datasets from evalita to our python eviroment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc5CmH4wIq",
        "outputId": "0ded34fb-6853-4b38-81cf-d7ddc76fd932"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip task-25-textentail-20240322T151548Z-001.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemLwOEanl_P"
      },
      "source": [
        "## 1.1 visualize initial data\n",
        "\n",
        "**dev.xml** is the training (development) dataset.\n",
        "\n",
        "\n",
        "**test_gold.xml** is the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PypMtd44ij",
        "outputId": "49483cc0-909e-4732-d177-4d3cee53c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pair entailment=\"YES\" id=\"1001\" task=\"WIKI\">\n",
            "<t>'Pieralfonso Fratta Pasini' è un imprenditore e un politico italiano.</t>\n",
            "<h>'Pieralfonso Fratta Pasini' è un imprenditore e politico italiano.</h>\n",
            "</pair>\n",
            "<pair entailment=\"YES\" id=\"1002\" task=\"WIKI\">\n",
            "<t>Del gruppo di Ravenna, facevano parte Alberto Acquacalda di Ravenna, Turiddu Candoli di Cervia, e Manoni.</t>\n",
            "<h>Del gruppo di Ravenna, facevano parte Turiddu Candoli di Cervia, Acquacalda e Manoni.</h>\n",
            "</pair>\n",
            "<pair entailment=\"YES\" id=\"1003\" task=\"WIKI\">\n"
          ]
        }
      ],
      "source": [
        "def vizualize_original_data(file_path):\n",
        "  # Open the file and read its contents\n",
        "  with open(file_path, 'r') as file:\n",
        "      count = 0\n",
        "      print_line = False\n",
        "\n",
        "      for line in file:\n",
        "          if '<pair' in line:\n",
        "              print_line = True\n",
        "\n",
        "              count += 1\n",
        "\n",
        "          if '</pair>' in line:\n",
        "              print(line.strip())\n",
        "              print_line = False\n",
        "\n",
        "          if print_line:\n",
        "              print(line.strip())\n",
        "\n",
        "          if count == 3:\n",
        "              break\n",
        "\n",
        "# Check info in dev.xml\n",
        "file_path = 'task-25-textentail/dev.xml'\n",
        "vizualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3stMZxk6ZPK",
        "outputId": "890bf8b7-0a86-419d-e438-eca20df4c7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<pair id=\"1401\" task=\"WIKI\" entailment=\"NO\">\n",
            "<t>Il 15 febbraio 2009 viene chiamato ad allenare il grosseto, dopo l'esonero di Gustinetti.</t>\n",
            "<h>Il 15 febbraio 2009 viene chiamato ad allenare il grosseto calcio, dopo l'esonero di Gustinetti.</h>\n",
            "</pair>\n",
            "<pair id=\"1402\" task=\"WIKI\" entailment=\"NO\">\n",
            "<t>Il 9 giugno 2009 Cirillo rescinde il contratto con la società amaranto.</t>\n",
            "<h>Il 9 giugno 2009 Cirillo rescinde consensualmente il proprio contratto con la società amaranto.</h>\n",
            "</pair>\n",
            "<pair id=\"1403\" task=\"WIKI\" entailment=\"YES\">\n"
          ]
        }
      ],
      "source": [
        "# Check info in test_gold.xml\n",
        "file_path = 'task-25-textentail/test_gold.xml'\n",
        "vizualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRnPwyNp7H9S"
      },
      "source": [
        "## 1.2 Data Reframing:\n",
        "  First we need to change the type of the file from xml to json. Then we want to convert that json into the following format:\n",
        "```JSON\n",
        "{\n",
        "    \"id\":           int,\n",
        "    \"text\":         str,\n",
        "    \"hypothesis\":   str,\n",
        "    \"choices\":      list[str],\n",
        "    \"label\":        int\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxqt1Jkc75Iq",
        "outputId": "737e1828-4a5e-406e-fa90-2c107ab25b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xmltodict is already installed.\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "# Check if the package is installed, and install it if it is not\n",
        "def check_and_install(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"{package} is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"{package} is not installed. Installing...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", package])\n",
        "\n",
        "check_and_install('xmltodict')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "31jYexPXZnYI"
      },
      "outputs": [],
      "source": [
        "import xmltodict\n",
        "import json\n",
        "\n",
        "# Define a function to convert an XML file to JSON Lines format\n",
        "def xml_to_jsonl(xml_file, jsonl_file):\n",
        "    with open(xml_file, 'r') as f:\n",
        "        xml_data = f.read()\n",
        "\n",
        "    # Parse XML into a dictionary\n",
        "    data_dict = xmltodict.parse(xml_data)\n",
        "\n",
        "    # Determine the root element of the XML and handle accordingly\n",
        "    root_key = next(iter(data_dict))  # Get the first key in the dictionary\n",
        "    root_element = data_dict[root_key]\n",
        "\n",
        "    with open(jsonl_file, 'w') as f:\n",
        "        # If the root element contains multiple elements, iterate over them\n",
        "        if isinstance(root_element, list):\n",
        "            for element in root_element:\n",
        "                # Write each element as a separate line in JSON Lines format\n",
        "                print(json.dumps(element, indent=4, sort_keys=True), file=f)\n",
        "        else:\n",
        "            # Write the single root element as a JSON object on a single line\n",
        "            print(json.dumps(root_element, indent=4, sort_keys=True), file=f)\n",
        "\n",
        "\n",
        "\n",
        "input_xml_path = 'task-25-textentail/dev.xml'\n",
        "output_json_path = 'dev_out.jsonl'\n",
        "xml_to_jsonl(input_xml_path, output_json_path)\n",
        "\n",
        "input_xml_path = 'task-25-textentail/test_gold.xml'\n",
        "output_json_path = 'test_gold_out.jsonl'\n",
        "xml_to_jsonl(input_xml_path, output_json_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXDo77YX9bA"
      },
      "source": [
        "## 1.3 Visualize jsonl information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuqfLHqczdA",
        "outputId": "6d7ea7af-81ab-4927-ad97-6b2e227c132a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 1001\n",
            "Entailment: YES\n",
            "Task: WIKI\n",
            "T: 'Pieralfonso Fratta Pasini' è un imprenditore e un politico italiano.\n",
            "H: 'Pieralfonso Fratta Pasini' è un imprenditore e politico italiano.\n",
            "\n",
            "ID: 1002\n",
            "Entailment: YES\n",
            "Task: WIKI\n",
            "T: Del gruppo di Ravenna, facevano parte Alberto Acquacalda di Ravenna, Turiddu Candoli di Cervia, e Manoni.\n",
            "H: Del gruppo di Ravenna, facevano parte Turiddu Candoli di Cervia, Acquacalda e Manoni.\n",
            "\n",
            "ID: 1003\n",
            "Entailment: YES\n",
            "Task: WIKI\n",
            "T: Viene eletto sindaco di Catanzaro al ballottaggio del giugno del 2006, con il 50,8% dei voti.\n",
            "H: Viene eletto sindaco di Catanzaro al ballottaggio del giugno 2006, con il 50,8% dei voti.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def visualize_data(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if 'pair' in data:\n",
        "        pairs = data['pair'][:3]\n",
        "        for pair in pairs:\n",
        "            print(\"ID:\", pair[\"@id\"])\n",
        "            print(\"Entailment:\", pair[\"@entailment\"])\n",
        "            print(\"Task:\", pair[\"@task\"])\n",
        "            print(\"T:\", pair[\"t\"])\n",
        "            print(\"H:\", pair[\"h\"])\n",
        "            print()\n",
        "\n",
        "json_file = 'dev_out.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvfeYF7L8cuz",
        "outputId": "227a2ba4-268f-4e1f-f5fc-c4fddfc72101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 1401\n",
            "Entailment: NO\n",
            "Task: WIKI\n",
            "T: Il 15 febbraio 2009 viene chiamato ad allenare il grosseto, dopo l'esonero di Gustinetti.\n",
            "H: Il 15 febbraio 2009 viene chiamato ad allenare il grosseto calcio, dopo l'esonero di Gustinetti.\n",
            "\n",
            "ID: 1402\n",
            "Entailment: NO\n",
            "Task: WIKI\n",
            "T: Il 9 giugno 2009 Cirillo rescinde il contratto con la società amaranto.\n",
            "H: Il 9 giugno 2009 Cirillo rescinde consensualmente il proprio contratto con la società amaranto.\n",
            "\n",
            "ID: 1403\n",
            "Entailment: YES\n",
            "Task: WIKI\n",
            "T: Bruschini però non si perse d'animo e, tornato sulle rive del lago di Como, nel 1967, riprese a giocare nelle fila del FC Chiasso nella Serie A Svizzera.\n",
            "H: Bruschini però non si perse d'animo e, tornato sulle rive del lago di Lecco, nel 1967, tornò a giocare nelle fila del FC Chiasso nella Serie A Svizzera.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "json_file = 'test_gold_out.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8cn4OBDRlj"
      },
      "source": [
        "\n",
        "## 1.4 Format jsonl\n",
        "\n",
        "In order to get the json file with the desired format:\n",
        "\n",
        "1. Rename @id to id.\n",
        "2. Rename @entailment to label and change YES or NO to 0 or 1 respectivly\n",
        "3. Rename @task to remove it since it's not present in the desired format.\n",
        "4. Replace t with text.\n",
        "5. Replace h with hypothesis.\n",
        "6. Add a choices key with entailed or not entailed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "crG0YoFVg-YI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Rearrange the JSON file to match the required format\n",
        "def rearrange_json(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    for pair in data[\"pair\"]:\n",
        "        if '@id' in pair and 't' in pair and 'h' in pair and '@entailment' in pair:\n",
        "            # Rearrange the keys\n",
        "            new_pair = {\n",
        "                \"id\": int(pair['@id']),\n",
        "                \"text\": pair['t'],\n",
        "                \"hypothesis\": pair['h'],\n",
        "                \"choices\": [\"entailed\", \"not entailed\"],\n",
        "                \"label\": 0 if pair['@entailment'] == \"YES\" else 1\n",
        "            }\n",
        "            pair.clear()  \n",
        "            pair.update(new_pair)  \n",
        "\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(data, file, indent=2)\n",
        "\n",
        "\n",
        "file_path_test_gold = \"test_gold_out.jsonl\"\n",
        "rearrange_json(file_path_test_gold)\n",
        "\n",
        "file_path_dev = \"dev_out.jsonl\"\n",
        "rearrange_json(file_path_dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdUFiAoYJDi"
      },
      "source": [
        "## 1.5 Visualize final jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoTfnvAj-KS",
        "outputId": "559cd3c1-f6ed-4dc9-ff7a-d683b5c8ace0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id: 1001\n",
            "text: 'Pieralfonso Fratta Pasini' è un imprenditore e un politico italiano.\n",
            "hypothesis: 'Pieralfonso Fratta Pasini' è un imprenditore e politico italiano.\n",
            "choices ['entailed', 'not entailed']\n",
            "label: 0\n",
            "\n",
            "id: 1002\n",
            "text: Del gruppo di Ravenna, facevano parte Alberto Acquacalda di Ravenna, Turiddu Candoli di Cervia, e Manoni.\n",
            "hypothesis: Del gruppo di Ravenna, facevano parte Turiddu Candoli di Cervia, Acquacalda e Manoni.\n",
            "choices ['entailed', 'not entailed']\n",
            "label: 0\n",
            "\n",
            "id: 1003\n",
            "text: Viene eletto sindaco di Catanzaro al ballottaggio del giugno del 2006, con il 50,8% dei voti.\n",
            "hypothesis: Viene eletto sindaco di Catanzaro al ballottaggio del giugno 2006, con il 50,8% dei voti.\n",
            "choices ['entailed', 'not entailed']\n",
            "label: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to visualize the data in the JSON file\n",
        "def visualize_data(json_file):\n",
        "    with open(json_file, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    if 'pair' in data:\n",
        "        pairs = data['pair'][:3]\n",
        "        for pair in pairs:\n",
        "            print(\"id:\", pair[\"id\"])\n",
        "            print(\"text:\", pair[\"text\"])\n",
        "            print(\"hypothesis:\", pair[\"hypothesis\"])\n",
        "            print(\"choices\", pair[\"choices\"])\n",
        "            print(\"label:\", pair[\"label\"])\n",
        "            print()\n",
        "\n",
        "json_file = 'dev_out.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id: 1401\n",
            "text: Il 15 febbraio 2009 viene chiamato ad allenare il grosseto, dopo l'esonero di Gustinetti.\n",
            "hypothesis: Il 15 febbraio 2009 viene chiamato ad allenare il grosseto calcio, dopo l'esonero di Gustinetti.\n",
            "choices ['entailed', 'not entailed']\n",
            "label: 1\n",
            "\n",
            "id: 1402\n",
            "text: Il 9 giugno 2009 Cirillo rescinde il contratto con la società amaranto.\n",
            "hypothesis: Il 9 giugno 2009 Cirillo rescinde consensualmente il proprio contratto con la società amaranto.\n",
            "choices ['entailed', 'not entailed']\n",
            "label: 1\n",
            "\n",
            "id: 1403\n",
            "text: Bruschini però non si perse d'animo e, tornato sulle rive del lago di Como, nel 1967, riprese a giocare nelle fila del FC Chiasso nella Serie A Svizzera.\n",
            "hypothesis: Bruschini però non si perse d'animo e, tornato sulle rive del lago di Lecco, nel 1967, tornò a giocare nelle fila del FC Chiasso nella Serie A Svizzera.\n",
            "choices ['entailed', 'not entailed']\n",
            "label: 0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "json_file = 'test_gold_out.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdoWdSVkZ6o"
      },
      "source": [
        "## **2. Promt formulation**\n",
        "\n",
        "In order to use this dataset we need to generate three prompts that can be used to get if a text is entailed to a hypothesis and then insert them into a json file. The three prompts are:\n",
        "\n",
        "**Template 1:**\n",
        "\n",
        "\n",
        "Prompt: \"La frase '{text}' sostiene la frase '{hypothesis}'?\"\n",
        "\n",
        "Translation: \"Does the statement '{text}' entail the statement '{hypothesis}'?\"4\n",
        "\n",
        "**Template 2:**\n",
        "\n",
        "\n",
        "Prompt: \"La frase '{text}' implica la frase '{hypothesis}'?\"\n",
        "\n",
        "Translation: \"Does the statement '{text}' imply the statement '{hypothesis}'?\"\n",
        "\n",
        "**Template 3:**\n",
        "\n",
        "\n",
        "Prompt: \"La frase '{hypothesis}' può essere dedotta dalla frase '{text}'?\"\n",
        "\n",
        "Translation: \"Can the statement '{hypothesis}' be deduced from the statement '{text}'?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3r6Gp2kqX6",
        "outputId": "25bb080b-bcda-4ecc-ac78-7971eb511538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"prompt\": \"La frase {{text}} sostiene la frase {{hypothesis}}?\"\n",
            "    },\n",
            "    {\n",
            "        \"prompt\": \"La frase {{text}} implica la frase {{hypothesis}}?\"\n",
            "    },\n",
            "    {\n",
            "        \"prompt\": \"La frase {{hypothesis}} pu\\u00f2 essere dedotta dalla frase {{text}}?\"\n",
            "    }\n",
            "]\n",
            "JSON file 'prompts.jsonl' generated successfully with 3 prompts.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate a JSON Lines file with a list of prompts\n",
        "def generate_json(prompts, output_file):\n",
        "    data = []\n",
        "    for prompt in prompts:\n",
        "        data.append({\"prompt\": prompt})\n",
        "\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as json_file:\n",
        "        print(json_file.read())\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"La frase {{text}} sostiene la frase {{hypothesis}}?\",\n",
        "    \"La frase {{text}} implica la frase {{hypothesis}}?\",\n",
        "    \"La frase {{hypothesis}} può essere dedotta dalla frase {{text}}?\"\n",
        "]\n",
        "\n",
        "output_file = \"prompts.jsonl\"\n",
        "generate_json(prompts, output_file)\n",
        "print(f\"JSON file '{output_file}' generated successfully with {len(prompts)} prompts.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpbmKeG4uai"
      },
      "source": [
        "# **3. Llama 2 set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2HXdEu6827D",
        "outputId": "3a39d619-ec8a-4785-c2c4-f62f718177a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V1tsB0SwOl4B"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runtimeFlag = device #Run on GPU (you can't run GPTQ on cpu)\n",
        "cache_dir = None # by default, don't set a cache directory. This is automatically updated if you connect Google Drive.\n",
        "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## 3.1 Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-HYbuJ6JCgd",
        "outputId": "1d078b01-2e78-41cc-86fc-d24885b4fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.28.0)\n",
            "Requirement already satisfied: datasets in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
            "Requirement already satisfied: rouge in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.7)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.2.1+cu118)\n",
            "Requirement already satisfied: safetensors in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.39.1)\n",
            "Requirement already satisfied: peft>=0.5.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from tqdm->auto-gptq) (0.4.6)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (2.2.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "038f994b-971a-46bf-e1f6-6f908ef1d874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "##3.2 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iO6WB6xkWCrN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# **4. Evaluate Homework prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbdUQxHVD8D"
      },
      "source": [
        "## 4.1. Test response of Promtps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSKyQeQrml_W",
        "outputId": "856c87fa-c12a-4092-fc33-1a0533429aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [0.5765582  0.32311448 0.10032736]\n",
            "Prompt: La frase Il 15 febbraio 2009 viene chiamato ad allenare il grosseto, dopo l'esonero di Gustinetti. sostiene la frase Il 15 febbraio 2009 viene chiamato ad allenare il grosseto calcio, dopo l'esonero di Gustinetti.?\n",
            "Prediction: {'label': 'entailed', 'confidence': 57.7}\n",
            "Actual Label: not entailed\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.13443957 0.8531037  0.01245672]\n",
            "Prompt: La frase Il 9 giugno 2009 Cirillo rescinde il contratto con la società amaranto. sostiene la frase Il 9 giugno 2009 Cirillo rescinde consensualmente il proprio contratto con la società amaranto.?\n",
            "Prediction: {'label': 'not entailed', 'confidence': 85.3}\n",
            "Actual Label: not entailed\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.00485643 0.0193718  0.9757717 ]\n",
            "Prompt: La frase Bruschini però non si perse d'animo e, tornato sulle rive del lago di Como, nel 1967, riprese a giocare nelle fila del FC Chiasso nella Serie A Svizzera. sostiene la frase Bruschini però non si perse d'animo e, tornato sulle rive del lago di Lecco, nel 1967, tornò a giocare nelle fila del FC Chiasso nella Serie A Svizzera.?\n",
            "Prediction: {'label': 'not entailed', 'confidence': 1.9}\n",
            "Actual Label: entailed\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.4692032  0.3290667  0.20173018]\n",
            "Prompt: La frase Giuseppe Cavanna è stato un calciatore, attivo negli anni '30, nel ruolo di portiere. sostiene la frase Giovanni Cavanna è stato un calciatore, attivo negli anni '30, nel ruolo di portiere.?\n",
            "Prediction: {'label': 'entailed', 'confidence': 46.9}\n",
            "Actual Label: not entailed\n",
            "Correct Prediction: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load your JSON file and process each pair\n",
        "with open('test_gold_out.jsonl', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if 'pair' in data:\n",
        "        pairs = data['pair'][:4]\n",
        "        for pair in pairs:\n",
        "            id = pair[\"id\"]\n",
        "            text = pair[\"text\"]\n",
        "            hypothesis = pair[\"hypothesis\"]\n",
        "            choices = pair[\"choices\"]\n",
        "            label = pair[\"label\"]\n",
        "\n",
        "\n",
        "            # Format the prompt with text and hypothesis\n",
        "            prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
        "\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            print(f\"Probabilities: {probabilities}\")\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "            prediction_confidence = probabilities[prediction_index] * 100\n",
        "            prediction = {\"label\": prediction_label, \"confidence\": round(prediction_confidence, 1)}\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == choices[label]\n",
        "\n",
        "            # Visualize the formatted prompt and the prediction details\n",
        "            print(\"Prompt:\", prompt_formatted)\n",
        "            print(\"Prediction:\", prediction)\n",
        "            print(\"Actual Label:\", choices[label])\n",
        "            print(\"Correct Prediction:\", is_correct)\n",
        "            print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_9dG1CVHo0"
      },
      "source": [
        "##4.2. Compare the diferent prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-4DiwuQPpb",
        "outputId": "1d5a9e15-7957-424f-d7a9-0a73ff3056ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Statistics:\n",
            "Total Pairs: 400\n",
            "Total Correct: 227\n",
            "Accuracy: 56.75 %\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "total_correct = 0\n",
        "total_pairs = 0\n",
        "\n",
        "# Load the JSONL file and process each pair\n",
        "with open('test_gold_out.jsonl', 'r') as file:\n",
        "    data = json.load(file)\n",
        "    if 'pair' in data:\n",
        "        pairs = data['pair']\n",
        "        for pair in pairs:\n",
        "            id = pair[\"id\"]\n",
        "            text = pair[\"text\"]\n",
        "            hypothesis = pair[\"hypothesis\"]\n",
        "            choices = pair[\"choices\"]\n",
        "            label = pair[\"label\"]\n",
        "\n",
        "            # Format the prompt with actual text and hypothesis\n",
        "            prompt_formatted = f\"La frase {hypothesis} può essere dedotta dalla frase {text}?\"\n",
        "\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == choices[label]\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "\n",
        "# Calculate overall statistics only if at least one pair was processed\n",
        "if total_pairs > 0:\n",
        "    accuracy = total_correct / total_pairs * 100\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"Overall Statistics:\")\n",
        "    print(\"Total Pairs:\", total_pairs)\n",
        "    print(\"Total Correct:\", total_correct)\n",
        "    print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "else:\n",
        "    print(\"No pairs found in the JSONL file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: La frase {{text}} sostiene la frase {{hypothesis}}?\n",
            "Total Pairs: 400\n",
            "Total Correct: 291\n",
            "Accuracy: 72.75 %\n",
            "Mean Confidence: 0.59\n",
            "\n",
            "Prompt: La frase {{text}} implica la frase {{hypothesis}}?\n",
            "Total Pairs: 400\n",
            "Total Correct: 292\n",
            "Accuracy: 73.0 %\n",
            "Mean Confidence: 0.69\n",
            "\n",
            "Prompt: La frase {{hypothesis}} può essere dedotta dalla frase {{text}}?\n",
            "Total Pairs: 400\n",
            "Total Correct: 227\n",
            "Accuracy: 56.75 %\n",
            "Mean Confidence: 0.6\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def prompt_testing(prompt):\n",
        "    total_correct = 0\n",
        "    total_pairs = 0\n",
        "    total_confidence = 0\n",
        "\n",
        "    # Load the JSONL file and process each pair\n",
        "    with open('test_gold_out.jsonl', 'r') as file:\n",
        "        data = json.load(file)\n",
        "        if 'pair' in data:\n",
        "            pairs = data['pair']\n",
        "            for pair in pairs:\n",
        "                id = pair[\"id\"]\n",
        "                text = pair[\"text\"]\n",
        "                hypothesis = pair[\"hypothesis\"]\n",
        "                choices = pair[\"choices\"]\n",
        "                label = pair[\"label\"]\n",
        "                \n",
        "                # Format with text and hypothesis \n",
        "                prompt_formatted = prompt.replace('{{text}}', text).replace('{{hypothesis}}', hypothesis)\n",
        "                input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "                output = model_llama(**input)\n",
        "                logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "                # Obtain the predicted label directly from logits\n",
        "                probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "                prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "                prediction_label = choices[prediction_index]\n",
        "\n",
        "                # Compare prediction with actual label\n",
        "                is_correct = prediction_label == choices[label]\n",
        "\n",
        "                # Accumulate statistics\n",
        "                total_correct += is_correct\n",
        "                total_pairs += 1\n",
        "                total_confidence += probabilities[prediction_index]\n",
        "\n",
        "    # Calculate statistics \n",
        "    if total_pairs > 0:\n",
        "        accuracy = total_correct / total_pairs * 100\n",
        "        mean_confidence = total_confidence / total_pairs\n",
        "\n",
        "        # Print statistics\n",
        "        print(\"Total Pairs:\", total_pairs)\n",
        "        print(\"Total Correct:\", total_correct)\n",
        "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "        print(\"Mean Confidence:\", round(mean_confidence, 2))\n",
        "    else:\n",
        "        print(\"No pairs found in the JSONL file.\")\n",
        "\n",
        "# Load prompts from the JSONL file and test each prompt\n",
        "with open('prompts.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = json.load(prompts_file)\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        prompt_testing(prompt)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
