{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the prompt with different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with DAMO-NLP-SG / zero-shot-classify-SSTuning-XLM-R \n",
    "\n",
    "https://huggingface.co/DAMO-NLP-SG/zero-shot-classify-SSTuning-XLM-R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch, string, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"DAMO-NLP-SG/zero-shot-classify-SSTuning-XLM-R\" # @param [\"DAMO-NLP-SG/zero-shot-classify-SSTuning-base\", \"DAMO-NLP-SG/zero-shot-classify-SSTuning-large\", \"DAMO-NLP-SG/zero-shot-classify-SSTuning-ALBERT\", \"DAMO-NLP-SG/zero-shot-classify-SSTuning-XLM-R\"]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "list_ABC = [x for x in string.ascii_uppercase]\n",
    "\n",
    "def add_prefix(text, list_label, shuffle=False):\n",
    "    # Append a period '.' to each label. This will improve the accuracy\n",
    "    list_label = [x+'.' if x[-1] not in ['.','!'] else x for x in list_label]\n",
    "\n",
    "    # Extend the list_label with padding tokens to have a length of 20\n",
    "    list_label_new = list_label + [tokenizer.pad_token]* (2 - len(list_label))\n",
    "\n",
    "    if shuffle:\n",
    "        # Shuffle the order of elements in list_label_new if shuffle flag is True\n",
    "        random.shuffle(list_label_new)\n",
    "\n",
    "    # Create a string representation of label options by combining each label with its corresponding index\n",
    "    s_option = ' '.join(['('+list_ABC[i]+') '+list_label_new[i] for i in range(len(list_label_new))])\n",
    "\n",
    "    # Return the modified text with label options and the list_label_new\n",
    "    return f'{s_option} {tokenizer.sep_token} {text}', list_label_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_text(model, text, list_label, shuffle=False):\n",
    "    # Add prefixes to the text using the add_prefix function\n",
    "    text, list_label_new = add_prefix(text, list_label, shuffle=shuffle)\n",
    "\n",
    "    # Set the model to evaluation mode and move it to the appropriate device\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Perform tokenization and encoding of the text\n",
    "    encoding = tokenizer([text], truncation=True, max_length=512)\n",
    "\n",
    "    # Create a dictionary of tensors for the encoded text\n",
    "    item = {key: torch.tensor(val).to(device) for key, val in encoding.items()}\n",
    "\n",
    "    # Generate logits from the model\n",
    "    logits = model(**item).logits\n",
    "\n",
    "    # Select a subset of logits based on shuffle flag\n",
    "    logits = logits if shuffle else logits[:, 0:len(list_label)]\n",
    "\n",
    "    # Convert logits to probabilities using softmax\n",
    "    probs = torch.nn.functional.softmax(logits, dim=-1).tolist()\n",
    "\n",
    "    # Get the predicted label index\n",
    "    predicted_index = torch.argmax(logits, dim=-1).item()\n",
    "    predicted_label = list_label[predicted_index]\n",
    "\n",
    "    # Get the probability of the predicted label\n",
    "    probability = probs[0][predicted_index] * 100\n",
    "\n",
    "    # Return the predicted label and probability\n",
    "    return predicted_label, probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test only with one prompt as example\n",
    "import json\n",
    "\n",
    "total_correct = 0\n",
    "total_pairs = 0\n",
    "\n",
    "# Load the JSONL file and process each pair\n",
    "with open(\"textual_entailment-task1-test-data.jsonl\", 'r') as file:\n",
    "    for line in file:\n",
    "        pair = json.loads(line)\n",
    "        text = pair[\"text\"]\n",
    "        hypothesis = pair[\"hypothesis\"]\n",
    "        choices = pair[\"choices\"]\n",
    "        label = pair[\"label\"]\n",
    "\n",
    "        # Format the prompt with text and hypothesis\n",
    "        prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "        \n",
    "        output = classifier(prompt_formatted, choices, multi_label=False)\n",
    "        sequence = output['sequence']\n",
    "        selected_label = output['labels'][0] if output['scores'][0] > output['scores'][1] else output['labels'][1]\n",
    "        accuracy = max(output['scores'])\n",
    "\n",
    "        # Accumulate statistics\n",
    "        if selected_label == choices[label]:\n",
    "            total_correct += 1\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Calculate overall statistics only if at least one pair was processed\n",
    "    if total_pairs > 0:\n",
    "        accuracy = total_correct / total_pairs * 100\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"Overall Statistics:\")\n",
    "        print(\"Total Pairs:\", total_pairs)\n",
    "        print(\"Total Correct:\", total_correct)\n",
    "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
    "    else:\n",
    "        print(\"No pairs found in the JSONL file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with XLM-roBERTa-large-it-mnli\n",
    "\n",
    "https://huggingface.co/Jiva/xlm-roberta-large-it-mnli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"zero-shot-classification\",\n",
    "                      model=\"Jiva/xlm-roberta-large-it-mnli\", device=0, use_fast=True, multi_label=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will classify the following wikipedia entry about Sardinia\"\n",
    "sequence_to_classify = \"La frase 'Pieralfonso Fratta Pasini' è un imprenditore e un politico italiano. sostiene la frase 'Pieralfonso Fratta Pasini' è un imprenditore e politico italiano.?\"\n",
    "# we can specify candidate labels in Italian:\n",
    "candidate_labels = [\"implicato, non implicato\"]\n",
    "classifier(sequence_to_classify, candidate_labels)\n",
    "# {'labels': ['geografia', 'moda', 'politica', 'macchine', 'cibo'],\n",
    "# 'scores': [0.38871392607688904, 0.22633370757102966, 0.19398456811904907, 0.13735772669315338, 0.13708525896072388]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Function to process each pair in the JSONL file\n",
    "def visualize_prompt_response(jsonl_file):\n",
    "    with open(jsonl_file, 'r') as file:\n",
    "        count = 0  # Counter to track processed pairs\n",
    "        for line in file:\n",
    "            if count >= 5:  # Break the loop if 5 pairs have been processed\n",
    "                break\n",
    "            pair = json.loads(line)\n",
    "            id = pair[\"id\"]\n",
    "            text = pair[\"text\"]\n",
    "            hypothesis = pair[\"hypothesis\"]\n",
    "            choices = pair[\"choices\"]\n",
    "            label = pair[\"label\"]\n",
    "\n",
    "            # Format the prompt with text and hypothesis\n",
    "            prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "            \n",
    "            output = classifier(prompt_formatted, choices, multi_label=False)\n",
    "            sequence = output['sequence']\n",
    "            selected_label = output['labels'][0] if output['scores'][0] > output['scores'][1] else output['labels'][1]\n",
    "            accuracy = max(output['scores'])\n",
    "            \n",
    "            # Print the required information\n",
    "            print(\"ID:\", id)\n",
    "            print(\"Sequence:\", sequence)\n",
    "            print(\"Selected Label:\", selected_label)\n",
    "            print(\"Accuracy:\", round(accuracy, 3))\n",
    "            print(\"Is Selected Label Correct:\", selected_label == choices[label])\n",
    "            print()\n",
    "\n",
    "            count += 1  # Increment the counter for processed pairs\n",
    "\n",
    "jsonl_file = 'textual_entailment-task1-train-data.jsonl'\n",
    "visualize_prompt_response(jsonl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Always getting CUDA memory exceed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First test only with one prompt as example\n",
    "import json\n",
    "\n",
    "total_correct = 0\n",
    "total_pairs = 0\n",
    "\n",
    "# Load the JSONL file and process each pair\n",
    "with open(\"textual_entailment-task1-test-data.jsonl\", 'r') as file:\n",
    "    for line in file:\n",
    "        pair = json.loads(line)\n",
    "        text = pair[\"text\"]\n",
    "        hypothesis = pair[\"hypothesis\"]\n",
    "        choices = pair[\"choices\"]\n",
    "        label = pair[\"label\"]\n",
    "\n",
    "        # Format the prompt with text and hypothesis\n",
    "        prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "        \n",
    "        output = classifier(prompt_formatted, choices, multi_label=False)\n",
    "        sequence = output['sequence']\n",
    "        selected_label = output['labels'][0] if output['scores'][0] > output['scores'][1] else output['labels'][1]\n",
    "        accuracy = max(output['scores'])\n",
    "\n",
    "        # Accumulate statistics\n",
    "        if selected_label == choices[label]:\n",
    "            total_correct += 1\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Calculate overall statistics only if at least one pair was processed\n",
    "    if total_pairs > 0:\n",
    "        accuracy = total_correct / total_pairs * 100\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"Overall Statistics:\")\n",
    "        print(\"Total Pairs:\", total_pairs)\n",
    "        print(\"Total Correct:\", total_correct)\n",
    "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
    "    else:\n",
    "        print(\"No pairs found in the JSONL file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with comprehend-it-multilang-base\n",
    "\n",
    "https://huggingface.co/knowledgator/comprehend_it-multilingual-t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from liqfit.pipeline import ZeroShotClassificationPipeline\n",
    "from liqfit.models import T5ForZeroShotClassification\n",
    "from transformers import T5Tokenizer\n",
    "\n",
    "model = T5ForZeroShotClassification.from_pretrained('knowledgator/comprehend_it-multilingual-t5-base')\n",
    "tokenizer = T5Tokenizer.from_pretrained('knowledgator/comprehend_it-multilingual-t5-base')\n",
    "classifier_comprehend = ZeroShotClassificationPipeline(model=model, tokenizer=tokenizer,\n",
    "                                                      hypothesis_template = '{}', encoder_decoder = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_to_classify = \"one day I will see the world\"\n",
    "candidate_labels = ['travel', 'cooking', 'dancing']\n",
    "classifier_comprehend(sequence_to_classify, candidate_labels, multi_label=False)\n",
    "# {'sequence': 'one day I will see the world',\n",
    "#  'labels': ['travel', 'cooking', 'dancing'],\n",
    "#  'scores': [0.7350383996963501, 0.1484801471233368, 0.1164814680814743]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "# Function to process each pair in the JSONL file\n",
    "def visualize_prompt_response(jsonl_file):\n",
    "    with open(jsonl_file, 'r') as file:\n",
    "        count = 0  # Counter to track processed pairs\n",
    "        for line in file:\n",
    "            if count >= 5:  # Break the loop if 5 pairs have been processed\n",
    "                break\n",
    "            pair = json.loads(line)\n",
    "            id = pair[\"id\"]\n",
    "            text = pair[\"text\"]\n",
    "            hypothesis = pair[\"hypothesis\"]\n",
    "            choices = pair[\"choices\"]\n",
    "            label = pair[\"label\"]\n",
    "\n",
    "            # Format the prompt with text and hypothesis\n",
    "            prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "            \n",
    "            output = classifier_comprehend(prompt_formatted, choices, multi_label=False)\n",
    "            sequence = output['sequence']\n",
    "            selected_label = output['labels'][0] if output['scores'][0] > output['scores'][1] else output['labels'][1]\n",
    "            accuracy = max(output['scores'])\n",
    "            \n",
    "            # Print the required information\n",
    "            print(\"ID:\", id)\n",
    "            print(\"Sequence:\", sequence)\n",
    "            print(\"Selected Label:\", selected_label)\n",
    "            print(\"Accuracy:\", round(accuracy, 3))\n",
    "            print(\"Is Selected Label Correct:\", selected_label == choices[label])\n",
    "            print()\n",
    "\n",
    "            count += 1  # Increment the counter for processed pairs\n",
    "\n",
    "jsonl_file = 'textual_entailment-task1-train-data.jsonl'\n",
    "visualize_prompt_response(jsonl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total_correct = 0\n",
    "total_pairs = 0\n",
    "\n",
    "# Load the JSONL file and process each pair\n",
    "with open(\"textual_entailment-task1-test-data.jsonl\", 'r') as file:\n",
    "    for line in file:\n",
    "        pair = json.loads(line)\n",
    "        text = pair[\"text\"]\n",
    "        hypothesis = pair[\"hypothesis\"]\n",
    "        choices = pair[\"choices\"]\n",
    "        label = pair[\"label\"]\n",
    "\n",
    "        # Format the prompt with text and hypothesis\n",
    "        prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "        \n",
    "        # Assuming classifier is a function that returns the output sequence and scores\n",
    "        output = classifier(prompt_formatted, choices, multi_label=False)\n",
    "    \n",
    "        # Check and convert tensor precision\n",
    "        for key in output.keys():\n",
    "            if torch.is_tensor(output[key]) and output[key].dtype == torch.float16:\n",
    "                output[key] = output[key].to(torch.float32)\n",
    "\n",
    "        sequence = output['sequence']\n",
    "        selected_label = output['labels'][0] if output['scores'][0] > output['scores'][1] else output['labels'][1]\n",
    "        accuracy = max(output['scores'])\n",
    "\n",
    "        # Accumulate statistics\n",
    "        if selected_label == choices[label]:\n",
    "            total_correct += 1\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Calculate overall statistics only if at least one pair was processed\n",
    "    if total_pairs > 0:\n",
    "        accuracy = total_correct / total_pairs * 100\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"Overall Statistics:\")\n",
    "        print(\"Total Pairs:\", total_pairs)\n",
    "        print(\"Total Correct:\", total_correct)\n",
    "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
    "    else:\n",
    "        print(\"No pairs found in the JSONL file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model in order to get better acuuracy from pormpts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load the pretrained model and tokenizer\n",
    "model_name = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Define your dataset loading and processing functions\n",
    "def load_dataset(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            example = json.loads(line)\n",
    "            dataset.append(example)\n",
    "    return dataset\n",
    "\n",
    "# Define training parameters\n",
    "epochs = 100\n",
    "#learning_rate = 2e-5\n",
    "learning_rate = 0.1\n",
    "batch_size = 8  # Reduce batch size\n",
    "accumulation_steps = 4  # Define accumulation steps\n",
    "\n",
    "# Prepare optimizer and loss function\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Map label strings to integers\n",
    "label_map = {'implicato': 0, 'non implicato': 1}\n",
    "\n",
    "# Fine-tuning loop\n",
    "for epoch in range(epochs):\n",
    "    # Load training data\n",
    "    train_dataset = load_dataset(\"textual_entailment-task1-train-data.jsonl\")\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for batch_start in range(0, len(train_dataset), batch_size):\n",
    "        batch = train_dataset[batch_start:batch_start+batch_size]\n",
    "        texts = [example[\"text\"] for example in batch]\n",
    "        hypotheses = [example[\"hypothesis\"] for example in batch]\n",
    "        labels = [label_map.get(example[\"label\"], example[\"label\"]) for example in batch]  # Convert label strings to integers\n",
    "        labels = torch.tensor(labels)  # Convert labels to tensor\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = tokenizer(texts, hypotheses, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient accumulation\n",
    "        if (batch_start + 1) % accumulation_steps == 0 or batch_start == len(train_dataset) - 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / len(train_dataset)}\")\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"trained_model/\")\n",
    "tokenizer.save_pretrained(\"trained_model/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "model_path = \"trained_model/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "def classifier_comprehend(prompt, choices, multi_label=False):\n",
    "    results = []\n",
    "    for choice in choices:\n",
    "        inputs = tokenizer(prompt, choice, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        with torch.no_grad():  # Disable gradient calculation\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = logits.softmax(dim=1)\n",
    "        \n",
    "        # For binary classification, return the label with the highest probability\n",
    "        if not multi_label:\n",
    "            label = torch.argmax(probabilities, dim=1).item()\n",
    "            results.append({'sequence': prompt, 'label': choice, 'score': probabilities[0][label].item()})\n",
    "        else:\n",
    "            # For multi-label classification, return all labels with probabilities\n",
    "            scores, labels = torch.topk(probabilities, k=len(choices), dim=1)\n",
    "            labels = [choices[i] for i in labels.squeeze().tolist()]\n",
    "            results.append({'sequence': prompt, 'label': labels, 'score': scores.squeeze().tolist()})\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process each pair in the JSONL file\n",
    "def visualize_prompt_response(jsonl_file):\n",
    "    with open(jsonl_file, 'r') as file:\n",
    "        count = 0  # Counter to track processed pairs\n",
    "        for line in file:\n",
    "            if count >= 5:  # Break the loop if 5 pairs have been processed\n",
    "                break\n",
    "            pair = json.loads(line)\n",
    "            id = pair[\"id\"]\n",
    "            text = pair[\"text\"]\n",
    "            hypothesis = pair[\"hypothesis\"]\n",
    "            choices = pair[\"choices\"]\n",
    "            label = pair[\"label\"]\n",
    "\n",
    "            # Format the prompt with text and hypothesis\n",
    "            prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "            \n",
    "            outputs = classifier_comprehend(prompt_formatted, choices, multi_label=False)\n",
    "            for output in outputs:\n",
    "                selected_label = output['label']\n",
    "                accuracy = output['score']\n",
    "                \n",
    "                # Print the required information\n",
    "                print(\"ID:\", id)\n",
    "                print(\"Sequence:\", prompt_formatted)\n",
    "                print(\"Selected Label:\", selected_label)\n",
    "                print(\"Accuracy:\", round(accuracy, 3))\n",
    "                print(\"Is Selected Label Correct:\", selected_label == choices[label])\n",
    "                print()\n",
    "\n",
    "            count += 1  # Increment the counter for processed pairs\n",
    "\n",
    "jsonl_file = 'textual_entailment-task1-train-data.jsonl'\n",
    "visualize_prompt_response(jsonl_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "total_correct = 0\n",
    "total_pairs = 0\n",
    "\n",
    "# Load the JSONL file and process each pair\n",
    "with open(\"textual_entailment-task1-test-data.jsonl\", 'r') as file:\n",
    "    for line in file:\n",
    "        pair = json.loads(line)\n",
    "        text = pair[\"text\"]\n",
    "        hypothesis = pair[\"hypothesis\"]\n",
    "        choices = pair[\"choices\"]\n",
    "        label = pair[\"label\"]\n",
    "\n",
    "        # Format the prompt with text and hypothesis\n",
    "        prompt_formatted = f\"La frase {text} sostiene la frase {hypothesis}?\"\n",
    "        \n",
    "        # Assuming classifier is a function that returns the output sequence and scores\n",
    "        outputs = classifier_comprehend(prompt_formatted, choices, multi_label=False)\n",
    "        for output in outputs:\n",
    "            selected_label = output['label']\n",
    "            accuracy = output['score']\n",
    "    \n",
    "        # Accumulate statistics\n",
    "        if selected_label == choices[label]:\n",
    "            total_correct += 1\n",
    "        total_pairs += 1\n",
    "\n",
    "    # Calculate overall statistics only if at least one pair was processed\n",
    "    if total_pairs > 0:\n",
    "        accuracy = total_correct / total_pairs * 100\n",
    "\n",
    "        # Print statistics\n",
    "        print(\"Overall Statistics:\")\n",
    "        print(\"Total Pairs:\", total_pairs)\n",
    "        print(\"Total Correct:\", total_correct)\n",
    "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
    "    else:\n",
    "        print(\"No pairs found in the JSONL file.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
