{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FvZqaH1qRX"
      },
      "source": [
        "# **Homework 1 - task 28**\n",
        "The main goal of this homework is to transform existing evaluation datasets into a format suitable for evaluating the linguistic skills of Large Language Models (LLMs) by reframing tasks as multi-choice Question Answering (QA) tasks, providing effective prompts, and generating distractors where necessary, all formatted in JSON Lines standard for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwL2mdH9KgG",
        "outputId": "2b4cad03-1874-4172-e207-09efca0537f0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_rtvMA3HO2"
      },
      "source": [
        "# **1. Data Loading:**\n",
        "\n",
        "\n",
        "First we need to download the datasets from evalita to our python eviroment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc5CmH4wIq",
        "outputId": "0ded34fb-6853-4b38-81cf-d7ddc76fd932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\SemEval2022Task3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'SemEval2022Task3'...\n",
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "##### Data Reader -- English/French/Italian\n",
        "!git clone https://github.com/shammur/SemEval2022Task3.git\n",
        "%cd ./SemEval2022Task3/\n",
        "#data/train/train_subtask-2/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sub Task 1 \n",
        "\n",
        "**binary classification sub-task**, which consists in predicting the acceptability label assigned to each sentence of the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemLwOEanl_P"
      },
      "source": [
        "## 1.1 visualize initial data\n",
        "\n",
        "**dev.xml** is the training (development) dataset.\n",
        "\n",
        "\n",
        "**test_gold.xml** is the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PypMtd44ij",
        "outputId": "49483cc0-909e-4732-d177-4d3cee53c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tConstruction\tSentence\tLabels\n",
            "it_0\tprefer\tNon amo i campi , preferisco i condomini .\t1\n",
            "it_1\tparticular\tPosso capire le emozioni , e in particolare la tristezza .\t1\n",
            "it_2\tgenerally\tApprezzo i giochi da tavolo , e più in generale il calcio .\t0\n",
            "it_3\tgenerally\tAmo i film , e più in generale i western .\t0\n"
          ]
        }
      ],
      "source": [
        "def visualize_original_data(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for i in range(5):  \n",
        "                line = file.readline()\n",
        "                print(line.rstrip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Check test data\n",
        "file_path = r'C:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv'\n",
        "visualize_original_data(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3stMZxk6ZPK",
        "outputId": "890bf8b7-0a86-419d-e438-eca20df4c7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "5491\tAmo i cartoni animati , ma non i sussidiari .\t1\n",
            "5084\tApprezzo il vino , ma non il Chianti .\t1\n",
            "1677\tAmo i manuali , un tipo interessante di dipinto .\t0\n",
            "1959\tAmo gli arbusti , ed anche le querce .\t1\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'C:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(r'c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join the fold training data to a single file\n",
        "\n",
        "def join_tsv_files(file_paths, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "\n",
        "# Example usage\n",
        "file1 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv'\n",
        "file2 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_1.tsv'\n",
        "file3 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_2.tsv'\n",
        "output_file = 'It-Subtask1-train.tsv'\n",
        "\n",
        "join_tsv_files([file1, file2, file3], output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "5491\tAmo i cartoni animati , ma non i sussidiari .\t1\n",
            "5084\tApprezzo il vino , ma non il Chianti .\t1\n",
            "1677\tAmo i manuali , un tipo interessante di dipinto .\t0\n",
            "1959\tAmo gli arbusti , ed anche le querce .\t1\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'It-Subtask1-train.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRnPwyNp7H9S"
      },
      "source": [
        "## 1.2 Data Reframing:\n",
        "  First we need to change the type of the file from xml to json. Then we want to convert that json into the following format:\n",
        "```JSON\n",
        "{\n",
        "    \"id\":       int,\n",
        "    \"text\":     str,\n",
        "    \"choices\":  list[str],\n",
        "    \"label\":    int\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mxqt1Jkc75Iq",
        "outputId": "737e1828-4a5e-406e-fa90-2c107ab25b82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xmltodict is already installed.\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "\n",
        "# Check if the package is installed, and install it if it is not\n",
        "def check_and_install(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"{package} is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"{package} is not installed. Installing...\")\n",
        "        import subprocess\n",
        "        subprocess.check_call([\"pip\", \"install\", package])\n",
        "\n",
        "check_and_install('xmltodict')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "31jYexPXZnYI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def tsv_to_jsonl(input_file, output_file):\n",
        "    with open(input_file, 'r', newline='') as tsvfile:\n",
        "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
        "        data = list(reader)\n",
        "\n",
        "    with open(output_file, 'w') as jsonfile:\n",
        "        json.dump(data, jsonfile, indent=4)\n",
        "\n",
        "tsv_file = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv'\n",
        "jsonl_file = 'It-Subtask1-test.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "tsv_file = 'It-Subtask1-train.tsv'\n",
        "jsonl_file = 'It-Subtask1-train.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXDo77YX9bA"
      },
      "source": [
        "## 1.3 Visualize jsonl information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuqfLHqczdA",
        "outputId": "6d7ea7af-81ab-4927-ad97-6b2e227c132a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ID\": \"it_0\",\n",
            "    \"Construction\": \"prefer\",\n",
            "    \"Sentence\": \"Non amo i campi , preferisco i condomini .\",\n",
            "    \"Labels\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"it_1\",\n",
            "    \"Construction\": \"particular\",\n",
            "    \"Sentence\": \"Posso capire le emozioni , e in particolare la tristezza .\",\n",
            "    \"Labels\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"it_2\",\n",
            "    \"Construction\": \"generally\",\n",
            "    \"Sentence\": \"Apprezzo i giochi da tavolo , e più in generale il calcio .\",\n",
            "    \"Labels\": \"0\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import json\n",
        "\n",
        "def visualize_jsonl_data(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        \n",
        "    for entry in data[:3]:  \n",
        "        print(\"{\")\n",
        "        for key, value in entry.items():\n",
        "            print(f'    \"{key}\": \"{value}\",')\n",
        "        print(\"},\")\n",
        "\n",
        "json_file = 'It-Subtask1-test.jsonl'\n",
        "visualize_jsonl_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvfeYF7L8cuz",
        "outputId": "227a2ba4-268f-4e1f-f5fc-c4fddfc72101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ID\": \"5491\",\n",
            "    \"Sentence\": \"Amo i cartoni animati , ma non i sussidiari .\",\n",
            "    \"Labels\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"5084\",\n",
            "    \"Sentence\": \"Apprezzo il vino , ma non il Chianti .\",\n",
            "    \"Labels\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"1677\",\n",
            "    \"Sentence\": \"Amo i manuali , un tipo interessante di dipinto .\",\n",
            "    \"Labels\": \"0\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-train.jsonl'\n",
        "visualize_jsonl_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8cn4OBDRlj"
      },
      "source": [
        "\n",
        "## 1.4 Format jsonl\n",
        "\n",
        "In order to get the json file with the desired format:\n",
        "\n",
        "1. Rename ID to id.\n",
        "2. Rename Sentence to text.\n",
        "3. Add a choices key with non acceptable  or aceptable.\n",
        "4. Rename Labels to label (0- non acceptable and 1 aceptable)\n",
        "5. Delte Contruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "crG0YoFVg-YI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def rearrange_json(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "        \n",
        "    rearranged_data = []\n",
        "    for pair in data:\n",
        "        if 'ID' in pair and 'Sentence' in pair and 'Labels' in pair:\n",
        "            # Rearrange the keys\n",
        "            new_pair = {\n",
        "                \"id\": pair['ID'],  \n",
        "                \"text\": pair['Sentence'],\n",
        "                \"choices\": [\"non acceptable\", \"acceptable\"],\n",
        "                \"label\": pair['Labels']\n",
        "            }\n",
        "            rearranged_data.append(new_pair)\n",
        "    \n",
        "    # Write rearranged data back to the file\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(rearranged_data, file, indent=2)\n",
        "\n",
        "# Example usage\n",
        "file_path_test_gold = 'It-Subtask1-test.jsonl'\n",
        "rearrange_json(file_path_test_gold)\n",
        "\n",
        "file_path_dev = 'It-Subtask1-train.jsonl'\n",
        "rearrange_json(file_path_dev)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdUFiAoYJDi"
      },
      "source": [
        "## 1.5 Visualize final jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"id\": \"it_0\",\n",
            "    \"text\": \"Non amo i campi , preferisco i condomini .\",\n",
            "    \"choices\": \"['non acceptable', 'acceptable']\",\n",
            "    \"label\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"it_1\",\n",
            "    \"text\": \"Posso capire le emozioni , e in particolare la tristezza .\",\n",
            "    \"choices\": \"['non acceptable', 'acceptable']\",\n",
            "    \"label\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"it_2\",\n",
            "    \"text\": \"Apprezzo i giochi da tavolo , e più in generale il calcio .\",\n",
            "    \"choices\": \"['non acceptable', 'acceptable']\",\n",
            "    \"label\": \"0\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-test.jsonl'\n",
        "visualize_jsonl_data(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoTfnvAj-KS",
        "outputId": "559cd3c1-f6ed-4dc9-ff7a-d683b5c8ace0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"id\": \"5491\",\n",
            "    \"text\": \"Amo i cartoni animati , ma non i sussidiari .\",\n",
            "    \"choices\": \"['non acceptable', 'acceptable']\",\n",
            "    \"label\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"5084\",\n",
            "    \"text\": \"Apprezzo il vino , ma non il Chianti .\",\n",
            "    \"choices\": \"['non acceptable', 'acceptable']\",\n",
            "    \"label\": \"1\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"1677\",\n",
            "    \"text\": \"Amo i manuali , un tipo interessante di dipinto .\",\n",
            "    \"choices\": \"['non acceptable', 'acceptable']\",\n",
            "    \"label\": \"0\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-train.jsonl'\n",
        "visualize_jsonl_data(json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdoWdSVkZ6o"
      },
      "source": [
        "## **2. Promt formulation**\n",
        "\n",
        "In order to use this dataset we need to generate three prompts that can be used to get if a text is entailed to a hypothesis and then insert them into a json file. The three prompts are:\n",
        "\n",
        "**Template 1:**\n",
        "\n",
        "\n",
        "Prompt: \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Evaluate whether the following text is acceptable or not according to the context: {{text}}\"\n",
        "\n",
        "**Template 2:**\n",
        "\n",
        "Prompt: \"Determina se la seguente frase è coerente con il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Determine if the following sentence is coherent with the context: {{text}}\"\n",
        "\n",
        "**Template 3:**\n",
        "\n",
        "Prompt: \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "\n",
        "Translation: \"Decide if the provided text is congruent with the described situation: {{text}}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3r6Gp2kqX6",
        "outputId": "25bb080b-bcda-4ecc-ac78-7971eb511538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"prompt\": \"Valuta se il seguente testo \\u00e8 accettabile o meno secondo il contesto: {{text}}\"\n",
            "    },\n",
            "    {\n",
            "        \"prompt\": \"Determina se la seguente frase \\u00e8 coerente con il contesto: {{text}}\"\n",
            "    },\n",
            "    {\n",
            "        \"prompt\": \"Decidi se il testo fornito \\u00e8 congruente con la situazione descritta: {{text}}\"\n",
            "    }\n",
            "]\n",
            "JSON file 'prompts.jsonl' generated successfully with 3 prompts.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate a JSON Lines file with a list of prompts\n",
        "def generate_json(prompts, output_file):\n",
        "    data = []\n",
        "    for prompt in prompts:\n",
        "        data.append({\"prompt\": prompt})\n",
        "\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as json_file:\n",
        "        print(json_file.read())\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\",\n",
        "    \"Determina se la seguente frase è coerente con il contesto: {{text}}\",\n",
        "    \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "]\n",
        "\n",
        "output_file = \"prompts.jsonl\"\n",
        "generate_json(prompts, output_file)\n",
        "print(f\"JSON file '{output_file}' generated successfully with {len(prompts)} prompts.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpbmKeG4uai"
      },
      "source": [
        "# **3. Llama 2 set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2HXdEu6827D",
        "outputId": "3a39d619-ec8a-4785-c2c4-f62f718177a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "V1tsB0SwOl4B"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runtimeFlag = device #Run on GPU (you can't run GPTQ on cpu)\n",
        "cache_dir = None # by default, don't set a cache directory. This is automatically updated if you connect Google Drive.\n",
        "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## 3.1 Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-HYbuJ6JCgd",
        "outputId": "1d078b01-2e78-41cc-86fc-d24885b4fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.28.0)\n",
            "Requirement already satisfied: datasets in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
            "Requirement already satisfied: rouge in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.7)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.2.1+cu118)\n",
            "Requirement already satisfied: safetensors in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.39.1)\n",
            "Requirement already satisfied: peft>=0.5.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from tqdm->auto-gptq) (0.4.6)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (2.2.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "038f994b-971a-46bf-e1f6-6f908ef1d874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "##3.2 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "iO6WB6xkWCrN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# **4. Evaluate Homework prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbdUQxHVD8D"
      },
      "source": [
        "## 4.1. Test response of Promtps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSKyQeQrml_W",
        "outputId": "856c87fa-c12a-4092-fc33-1a0533429aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [3.5034202e-04 9.9919301e-01 4.5671430e-04]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Non amo i campi , preferisco i condomini .\n",
            "Prediction: {'label': 'acceptable', 'confidence': 99.9}\n",
            "Actual Label: acceptable\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.00333161 0.9951598  0.00150863]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Posso capire le emozioni , e in particolare la tristezza .\n",
            "Prediction: {'label': 'acceptable', 'confidence': 99.5}\n",
            "Actual Label: acceptable\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.01640908 0.9175491  0.06604195]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Apprezzo i giochi da tavolo , e più in generale il calcio .\n",
            "Prediction: {'label': 'acceptable', 'confidence': 91.8}\n",
            "Actual Label: non acceptable\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.00220563 0.9563486  0.04144568]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo i film , e più in generale i western .\n",
            "Prediction: {'label': 'acceptable', 'confidence': 95.6}\n",
            "Actual Label: non acceptable\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.00394856 0.18735428 0.8086971 ]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: A differenza dello shopping , il basket è menzionato spesso in questo testo .\n",
            "Prediction: {'label': 'acceptable', 'confidence': 18.7}\n",
            "Actual Label: acceptable\n",
            "Correct Prediction: True\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "# Load your JSON file and process each pair\n",
        "with open('It-Subtask1-test.jsonl', 'r') as file:\n",
        "    data = json.load(file)[:5]  # Load only the first 5 pairs\n",
        "    for pair in data:\n",
        "        id = pair['id']\n",
        "        text = pair['text']\n",
        "        choices = pair['choices']\n",
        "        label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "        label = choices[label_index]  # Get the actual label using label_index\n",
        "\n",
        "        # Format the prompt with text and hypothesis\n",
        "        prompt_formatted = f\"Valuta se il seguente testo è accettabile o meno secondo il contesto: {text}\"\n",
        "\n",
        "        # Assuming you have tokenizer_llama and model_llama defined elsewhere\n",
        "        input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "        output = model_llama(**input)\n",
        "        logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "        # Obtain the predicted label directly from logits\n",
        "        probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "        print(f\"Probabilities: {probabilities}\")\n",
        "        prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "        prediction_label = choices[prediction_index]\n",
        "        prediction_confidence = probabilities[prediction_index] * 100\n",
        "        prediction = {\"label\": prediction_label, \"confidence\": round(prediction_confidence, 1)}\n",
        "\n",
        "        # Compare prediction with actual label\n",
        "        is_correct = prediction_label == label\n",
        "\n",
        "        # Visualize the formatted prompt and the prediction details\n",
        "        print(\"Prompt:\", prompt_formatted)\n",
        "        print(\"Prediction:\", prediction)\n",
        "        print(\"Actual Label:\", label)\n",
        "        print(\"Correct Prediction:\", is_correct)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_9dG1CVHo0"
      },
      "source": [
        "##4.2. Compare the diferent prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-4DiwuQPpb",
        "outputId": "1d5a9e15-7957-424f-d7a9-0a73ff3056ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Statistics:\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6852\n",
            "Accuracy: 47.06 %\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "total_correct = 0\n",
        "total_pairs = 0\n",
        "\n",
        "# Load the JSONL file and process each pair\n",
        "with open('It-Subtask1-test.jsonl', 'r') as file:\n",
        "    data = json.load(file) # Load only the first 5 pairs\n",
        "    for pair in data:\n",
        "        id = pair['id']\n",
        "        text = pair['text']\n",
        "        choices = pair['choices']\n",
        "        label = int(pair['label'])  # Convert label_index to an integer\n",
        "        #label = choices[label_index]  # Get the actual label using label_index\n",
        "        # Format the prompt with actual text and hypothesis\n",
        "        prompt_formatted = f\"Valuta se il seguente testo \\u00e8 accettabile o meno secondo il contesto: {text}\"\n",
        "\n",
        "        input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "        output = model_llama(**input)\n",
        "        logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "        # Obtain the predicted label directly from logits\n",
        "        probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "        prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "        prediction_label = choices[prediction_index]\n",
        "\n",
        "        # Compare prediction with actual label\n",
        "        is_correct = prediction_label == choices[label]\n",
        "\n",
        "        # Accumulate statistics\n",
        "        total_correct += is_correct\n",
        "        total_pairs += 1\n",
        "\n",
        "# Calculate overall statistics only if at least one pair was processed\n",
        "if total_pairs > 0:\n",
        "    accuracy = total_correct / total_pairs * 100\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"Overall Statistics:\")\n",
        "    print(\"Total Pairs:\", total_pairs)\n",
        "    print(\"Total Correct:\", total_correct)\n",
        "    print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "else:\n",
        "    print(\"No pairs found in the JSONL file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6852\n",
            "Accuracy: 47.06 %\n",
            "Mean Confidence: 0.93\n",
            "\n",
            "Prompt: Determina se la seguente frase è coerente con il contesto: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6852\n",
            "Accuracy: 47.06 %\n",
            "Mean Confidence: 0.97\n",
            "\n",
            "Prompt: Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6848\n",
            "Accuracy: 47.03 %\n",
            "Mean Confidence: 0.79\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def prompt_testing(prompt):\n",
        "    total_correct = 0\n",
        "    total_pairs = 0\n",
        "    total_confidence = 0\n",
        "\n",
        "    # Load the JSONL file and process each pair\n",
        "    with open('It-Subtask1-test.jsonl', 'r') as file:\n",
        "        data = json.load(file)  # Load only the first 5 pairs\n",
        "        for pair in data:\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label = int(pair['label'])  # Convert label_index to an integer\n",
        "            \n",
        "            # Format with text and hypothesis \n",
        "            prompt_formatted = prompt.replace('{{text}}', text)\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == choices[label]\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "            total_confidence += probabilities[prediction_index]\n",
        "\n",
        "    # Calculate statistics \n",
        "    if total_pairs > 0:\n",
        "        accuracy = total_correct / total_pairs * 100\n",
        "        mean_confidence = total_confidence / total_pairs\n",
        "\n",
        "        # Print statistics\n",
        "        print(\"Total Pairs:\", total_pairs)\n",
        "        print(\"Total Correct:\", total_correct)\n",
        "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "        print(\"Mean Confidence:\", round(mean_confidence, 2))\n",
        "    else:\n",
        "        print(\"No pairs found in the JSONL file.\")\n",
        "\n",
        "# Load prompts from the JSONL file and test each prompt\n",
        "with open('prompts.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = json.load(prompts_file)\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        prompt_testing(prompt)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
