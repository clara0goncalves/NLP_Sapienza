{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FvZqaH1qRX"
      },
      "source": [
        "# **Homework 1 - task 28**\n",
        "The main goal of this homework is to transform existing evaluation datasets into a format suitable for evaluating the linguistic skills of Large Language Models (LLMs) by reframing tasks as multi-choice Question Answering (QA) tasks, providing effective prompts, and generating distractors where necessary, all formatted in JSON Lines standard for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwL2mdH9KgG",
        "outputId": "2b4cad03-1874-4172-e207-09efca0537f0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_rtvMA3HO2"
      },
      "source": [
        "# **1. Data Loading:**\n",
        "\n",
        "\n",
        "First we need to download the datasets from evalita to our python eviroment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 245,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc5CmH4wIq",
        "outputId": "0ded34fb-6853-4b38-81cf-d7ddc76fd932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_2\\SemEval2022Task3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'SemEval2022Task3' already exists and is not an empty directory.\n",
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "##### Data Reader -- English/French/Italian\n",
        "!git clone https://github.com/shammur/SemEval2022Task3.git\n",
        "%cd ./SemEval2022Task3/\n",
        "#data/train/train_subtask-2/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sub Task 2\n",
        "\n",
        "**Regression sub-task**, which consists in predicting the average score assigned by human annotators on a seven point Likert-scale with respect to the subset of data evaluated via crowdsourcing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemLwOEanl_P"
      },
      "source": [
        "## 1.1 visualize initial data\n",
        "\n",
        "**dev.xml** is the training (development) dataset.\n",
        "\n",
        "\n",
        "**test_gold.xml** is the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(r'c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_2')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 247,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PypMtd44ij",
        "outputId": "49483cc0-909e-4732-d177-4d3cee53c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tConstruction\tSentence\tScores\n",
            "it_0\tingeneral\tAmo gli arbusti, e le querce in generale.\t2.91\n",
            "it_1\tparticular\tAmo le montagne, e in particolare i centri commerciali.\t1.3\n",
            "it_2\tcomparatives\tAmo le foreste più delle case.\t5.92\n",
            "it_3\tandtoo\tAmo le foreste, ed anche i centri commerciali.\t5.6\n"
          ]
        }
      ],
      "source": [
        "def visualize_original_data(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for i in range(5):  \n",
        "                line = file.readline()\n",
        "                print(line.rstrip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Check test data\n",
        "file_path = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-2\\It-Subtask2-scores.tsv'\n",
        "visualize_original_data(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 248,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join the fold training data to a single file\n",
        "\n",
        "def join_tsv_files(file_paths, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "\n",
        "# Example usage\n",
        "file1 = r'SemEval2022Task3\\data\\train\\train_subtask-2\\it\\It-Subtask2-fold_0.tsv'\n",
        "file2 = r'SemEval2022Task3\\data\\train\\train_subtask-2\\it\\It-Subtask2-fold_1.tsv'\n",
        "output_file = 'It-Subtask2-train.tsv'\n",
        "\n",
        "join_tsv_files([file1, file2], output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tScore\n",
            "73\tAdoro gli abeti, ma non i cespugli.\t4.33\n",
            "106\tAdoro il vitello, ma non i granchi.\t3.33\n",
            "203\tAdoro gli animali, un tipo interessante di abete.\t1.25\n",
            "466\tAmo gli abeti più dei cespugli.\t5.18\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'It-Subtask2-train.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRnPwyNp7H9S"
      },
      "source": [
        "## 1.2 Data Reframing:\n",
        "  First we need to change the type of the file from xml to json. Then we want to convert that json into the following format:\n",
        "```JSON\n",
        "{\n",
        "    \"id\":       int,\n",
        "    \"text\":     str,\n",
        "    \"choices\":  list[str|int],\n",
        "    \"label\":    int\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {
        "id": "31jYexPXZnYI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def tsv_to_jsonl(input_file, output_file):\n",
        "    with open(input_file, 'r', newline='') as tsvfile:\n",
        "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
        "        data = list(reader)\n",
        "\n",
        "    with open(output_file, 'w') as jsonfile:\n",
        "        json.dump(data, jsonfile, indent=4)\n",
        "\n",
        "tsv_file = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-2\\It-Subtask2-scores.tsv'\n",
        "jsonl_file = 'It-Subtask2-test.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [],
      "source": [
        "tsv_file = 'It-Subtask2-train.tsv'\n",
        "jsonl_file = 'It-Subtask2-train.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXDo77YX9bA"
      },
      "source": [
        "## 1.3 Visualize jsonl information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuqfLHqczdA",
        "outputId": "6d7ea7af-81ab-4927-ad97-6b2e227c132a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ID\": \"it_0\",\n",
            "    \"Construction\": \"ingeneral\",\n",
            "    \"Sentence\": \"Amo gli arbusti, e le querce in generale.\",\n",
            "    \"Scores\": \"2.91\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"it_1\",\n",
            "    \"Construction\": \"particular\",\n",
            "    \"Sentence\": \"Amo le montagne, e in particolare i centri commerciali.\",\n",
            "    \"Scores\": \"1.3\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"it_2\",\n",
            "    \"Construction\": \"comparatives\",\n",
            "    \"Sentence\": \"Amo le foreste più delle case.\",\n",
            "    \"Scores\": \"5.92\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import json\n",
        "\n",
        "def visualize_jsonl_data(json_file):\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        \n",
        "    for entry in data[:3]:  \n",
        "        print(\"{\")\n",
        "        for key, value in entry.items():\n",
        "            print(f'    \"{key}\": \"{value}\",')\n",
        "        print(\"},\")\n",
        "\n",
        "json_file = 'It-Subtask2-test.jsonl'\n",
        "visualize_jsonl_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvfeYF7L8cuz",
        "outputId": "227a2ba4-268f-4e1f-f5fc-c4fddfc72101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"ID\": \"73\",\n",
            "    \"Sentence\": \"Adoro gli abeti, ma non i cespugli.\",\n",
            "    \"Score\": \"4.33\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"106\",\n",
            "    \"Sentence\": \"Adoro il vitello, ma non i granchi.\",\n",
            "    \"Score\": \"3.33\",\n",
            "},\n",
            "{\n",
            "    \"ID\": \"203\",\n",
            "    \"Sentence\": \"Adoro gli animali, un tipo interessante di abete.\",\n",
            "    \"Score\": \"1.25\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask2-train.jsonl'\n",
        "visualize_jsonl_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8cn4OBDRlj"
      },
      "source": [
        "\n",
        "## 1.4 Format jsonl\n",
        "\n",
        "In order to get the json file with the desired format:\n",
        "\n",
        "1. Rename ID to id. (int)\n",
        "2. Rename Sentence to text.  (str)\n",
        "3. Add a choices key with 4 of the 7  possible labels as values. One must be correct.\n",
        "4. Rename Labels to label (to the position of the correct choice).\n",
        "5. Delete Contruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {
        "id": "crG0YoFVg-YI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Invalid score value\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "def rearrange_json(file_path):\n",
        "    # Mapping scores to Italian descriptors used in Likert scales\n",
        "    score_to_descriptor = {\n",
        "        1: \"Pessimo\",\n",
        "        2: \"Scarso\",\n",
        "        3: \"Sufficiente\",\n",
        "        4: \"Buono\",\n",
        "        5: \"Molto buono\",\n",
        "        6: \"Ottimo\",\n",
        "        7: \"Eccellente\"\n",
        "    }\n",
        "\n",
        "    with open(file_path, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "        rearranged_data = []\n",
        "        for pair in data:\n",
        "            if 'ID' in pair and 'Sentence' in pair:\n",
        "                score_key = 'Scores' if 'Scores' in pair else 'Score'\n",
        "                try:\n",
        "                    score = int(round(float(pair[score_key])))\n",
        "                except ValueError:\n",
        "                    print(f\"Invalid score value\")\n",
        "                    continue\n",
        "                new_pair = {\n",
        "                    \"id\": int(pair['ID'].split('_')[-1]),  # Extracting the integer part of the ID\n",
        "                    \"text\": pair['Sentence'],\n",
        "                    \"choices\": [],  \n",
        "                    \"label\": None  \n",
        "                }\n",
        "                # Generating choices based on list of descriptors\n",
        "                choices = [score_to_descriptor[i] for i in range(1, 8) if i != score]\n",
        "                # Ensure unique choices including the correct answer\n",
        "                choices = random.sample(choices, 3)\n",
        "                choices.append(score_to_descriptor[score])\n",
        "                # Shuffle the choices to randomize the position of the correct answer\n",
        "                random.shuffle(choices)\n",
        "                new_pair[\"choices\"] = choices\n",
        "                # Generate label within the range of choices\n",
        "                new_pair[\"label\"] = new_pair[\"choices\"].index(score_to_descriptor[score])\n",
        "                rearranged_data.append(new_pair)\n",
        "    # Write rearranged data back to the file\n",
        "    with open(file_path, \"w\") as file:\n",
        "        json.dump(rearranged_data, file, indent=2)\n",
        "\n",
        "file_path_test_gold = 'It-Subtask2-test.jsonl'\n",
        "rearrange_json(file_path_test_gold)\n",
        "\n",
        "\n",
        "file_path_dev = 'It-Subtask2-train.jsonl'\n",
        "rearrange_json(file_path_dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdUFiAoYJDi"
      },
      "source": [
        "## 1.5 Visualize final jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"id\": \"0\",\n",
            "    \"text\": \"Amo gli arbusti, e le querce in generale.\",\n",
            "    \"choices\": \"['Molto buono', 'Pessimo', 'Eccellente', 'Sufficiente']\",\n",
            "    \"label\": \"3\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"1\",\n",
            "    \"text\": \"Amo le montagne, e in particolare i centri commerciali.\",\n",
            "    \"choices\": \"['Eccellente', 'Ottimo', 'Pessimo', 'Molto buono']\",\n",
            "    \"label\": \"2\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"2\",\n",
            "    \"text\": \"Amo le foreste più delle case.\",\n",
            "    \"choices\": \"['Buono', 'Ottimo', 'Scarso', 'Sufficiente']\",\n",
            "    \"label\": \"1\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask2-test.jsonl'\n",
        "visualize_jsonl_data(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoTfnvAj-KS",
        "outputId": "559cd3c1-f6ed-4dc9-ff7a-d683b5c8ace0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"id\": \"73\",\n",
            "    \"text\": \"Adoro gli abeti, ma non i cespugli.\",\n",
            "    \"choices\": \"['Scarso', 'Pessimo', 'Molto buono', 'Buono']\",\n",
            "    \"label\": \"3\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"106\",\n",
            "    \"text\": \"Adoro il vitello, ma non i granchi.\",\n",
            "    \"choices\": \"['Scarso', 'Pessimo', 'Sufficiente', 'Eccellente']\",\n",
            "    \"label\": \"2\",\n",
            "},\n",
            "{\n",
            "    \"id\": \"203\",\n",
            "    \"text\": \"Adoro gli animali, un tipo interessante di abete.\",\n",
            "    \"choices\": \"['Ottimo', 'Eccellente', 'Pessimo', 'Molto buono']\",\n",
            "    \"label\": \"2\",\n",
            "},\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask2-train.jsonl'\n",
        "visualize_jsonl_data(json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdoWdSVkZ6o"
      },
      "source": [
        "## **2. Promt formulation**\n",
        "\n",
        "In order to use this dataset we need to generate three prompts that can be used to get if a text is entailed to a hypothesis and then insert them into a json file. The three prompts are:\n",
        "\n",
        "**Template 1:**\n",
        "\n",
        "\n",
        "Prompt: \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Evaluate whether the following text is acceptable or not according to the context: {{text}}\"\n",
        "\n",
        "**Template 2:**\n",
        "\n",
        "Prompt: \"Determina se la seguente frase è coerente con il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Determine if the following sentence is coherent with the context: {{text}}\"\n",
        "\n",
        "**Template 3:**\n",
        "\n",
        "Prompt: \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "\n",
        "Translation: \"Decide if the provided text is congruent with the described situation: {{text}}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3r6Gp2kqX6",
        "outputId": "25bb080b-bcda-4ecc-ac78-7971eb511538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"prompt\": \"Qual \\u00e8 il tuo giudizio sull'accettabilit\\u00e0 di {{text}}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\"\n",
            "    },\n",
            "    {\n",
            "        \"prompt\": \"Considerando {{text}}, quanto lo trovi accettabile? Esprimi la tua valutazione su una scala da 1 a 7, dove '1' corrisponde a 'Pessimo' e '7' a 'Eccellente'.\"\n",
            "    },\n",
            "    {\n",
            "        \"prompt\": \"Su una scala da 1 a 7, con '1' rappresentante 'Pessimo' e '7' 'Eccellente', quanto accettabile trovi {{text}}?\"\n",
            "    }\n",
            "]\n",
            "JSON file 'prompts.jsonl' generated successfully with 3 prompts.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate a JSON Lines file with a list of prompts\n",
        "def generate_json(prompts, output_file):\n",
        "    data = []\n",
        "    for prompt in prompts:\n",
        "        data.append({\"prompt\": prompt})\n",
        "\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(data, json_file, indent=4)\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as json_file:\n",
        "        print(json_file.read())\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"Qual è il tuo giudizio sull'accettabilità di {{text}}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\",\n",
        "    \"Considerando {{text}}, quanto lo trovi accettabile? Esprimi la tua valutazione su una scala da 1 a 7, dove '1' corrisponde a 'Pessimo' e '7' a 'Eccellente'.\",\n",
        "    \"Su una scala da 1 a 7, con '1' rappresentante 'Pessimo' e '7' 'Eccellente', quanto accettabile trovi {{text}}?\"\n",
        "]\n",
        "\n",
        "output_file = \"prompts.jsonl\"\n",
        "generate_json(prompts, output_file)\n",
        "print(f\"JSON file '{output_file}' generated successfully with {len(prompts)} prompts.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpbmKeG4uai"
      },
      "source": [
        "# **3. Llama 2 set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2HXdEu6827D",
        "outputId": "3a39d619-ec8a-4785-c2c4-f62f718177a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 269,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "V1tsB0SwOl4B"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runtimeFlag = device #Run on GPU (you can't run GPTQ on cpu)\n",
        "cache_dir = None # by default, don't set a cache directory. This is automatically updated if you connect Google Drive.\n",
        "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## 3.1 Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-HYbuJ6JCgd",
        "outputId": "1d078b01-2e78-41cc-86fc-d24885b4fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.28.0)\n",
            "Requirement already satisfied: datasets in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
            "Requirement already satisfied: rouge in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.7)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.2.1+cu118)\n",
            "Requirement already satisfied: safetensors in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.39.1)\n",
            "Requirement already satisfied: peft>=0.5.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from tqdm->auto-gptq) (0.4.6)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (2.2.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "038f994b-971a-46bf-e1f6-6f908ef1d874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "##3.2 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "iO6WB6xkWCrN"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# **4. Evaluate Homework prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbdUQxHVD8D"
      },
      "source": [
        "## 4.1. Test response of Promtps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSKyQeQrml_W",
        "outputId": "856c87fa-c12a-4092-fc33-1a0533429aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [0.01952113 0.9744667  0.00601211]\n",
            "ID: 0\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo gli arbusti, e le querce in generale., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': 'Pessimo', 'confidence': 97.4}\n",
            "Actual Label: Sufficiente\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.01955767 0.9729056  0.00753672]\n",
            "ID: 1\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo le montagne, e in particolare i centri commerciali., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': 'Ottimo', 'confidence': 97.3}\n",
            "Actual Label: Pessimo\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.02535288 0.9697725  0.00487464]\n",
            "ID: 2\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo le foreste più delle case., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': 'Ottimo', 'confidence': 97.0}\n",
            "Actual Label: Ottimo\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.01362344 0.983266   0.00311051]\n",
            "ID: 3\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo le foreste, ed anche i centri commerciali., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': 'Scarso', 'confidence': 98.3}\n",
            "Actual Label: Ottimo\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.01673029 0.9787921  0.00447766]\n",
            "ID: 4\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo i merli, e i gatti in generale., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': 'Buono', 'confidence': 97.9}\n",
            "Actual Label: Sufficiente\n",
            "Correct Prediction: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "# Load your JSON file and process each pair\n",
        "with open('It-Subtask2-test.jsonl', 'r') as file:\n",
        "    data = json.load(file)[:5]  # Load only the first 5 pairs\n",
        "    for pair in data:\n",
        "        id = pair['id']\n",
        "        text = pair['text']\n",
        "        choices = pair['choices']\n",
        "        label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "        label = choices[label_index]  # Get the actual label using label_index\n",
        "\n",
        "        # Format the prompt with text and hypothesis\n",
        "        prompt_formatted = f\"Qual è il tuo giudizio sull'accettabilità di {text}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\"\n",
        "\n",
        "        # Assuming you have tokenizer_llama and model_llama defined elsewhere\n",
        "        input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "        output = model_llama(**input)\n",
        "        logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "        # Obtain the predicted label directly from logits\n",
        "        probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "        print(f\"Probabilities: {probabilities}\")\n",
        "        prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "        prediction_label = choices[prediction_index]\n",
        "        prediction_confidence = probabilities[prediction_index] * 100\n",
        "        prediction = {\"label\": prediction_label, \"confidence\": round(prediction_confidence, 1)}\n",
        "\n",
        "        # Compare prediction with actual label\n",
        "        is_correct = prediction_label == label\n",
        "\n",
        "        # Visualize the formatted prompt and the prediction details\n",
        "        print(f\"ID: {id}\")\n",
        "        print(\"Prompt:\", prompt_formatted)\n",
        "        print(\"Prediction:\", prediction)\n",
        "        print(\"Actual Label:\", label)\n",
        "        print(\"Correct Prediction:\", is_correct)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_9dG1CVHo0"
      },
      "source": [
        "##4.2. Compare the diferent prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-4DiwuQPpb",
        "outputId": "1d5a9e15-7957-424f-d7a9-0a73ff3056ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Statistics:\n",
            "Total Pairs: 1009\n",
            "Total Correct: 257\n",
            "Accuracy: 25.47 %\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "total_correct = 0\n",
        "total_pairs = 0\n",
        "\n",
        "# Load the JSONL file and process each pair\n",
        "with open('It-Subtask2-test.jsonl', 'r') as file:\n",
        "    data = json.load(file) # Load only the first 5 pairs\n",
        "    for pair in data:\n",
        "        id = pair['id']\n",
        "        text = pair['text']\n",
        "        choices = pair['choices']\n",
        "        label = int(pair['label']) \n",
        "        \n",
        "        # Format the prompt with actual text and hypothesis\n",
        "        prompt_formatted = f\"Qual è il tuo giudizio sull'accettabilità di {text}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\"\n",
        "\n",
        "        input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "        output = model_llama(**input)\n",
        "        logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "        # Obtain the predicted label directly from logits\n",
        "        probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "        prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "        prediction_label = choices[prediction_index]\n",
        "\n",
        "        # Compare prediction with actual label\n",
        "        is_correct = prediction_label == choices[label]\n",
        "\n",
        "        # Accumulate statistics\n",
        "        total_correct += is_correct\n",
        "        total_pairs += 1\n",
        "\n",
        "# Calculate overall statistics only if at least one pair was processed\n",
        "if total_pairs > 0:\n",
        "    accuracy = total_correct / total_pairs * 100\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"Overall Statistics:\")\n",
        "    print(\"Total Pairs:\", total_pairs)\n",
        "    print(\"Total Correct:\", total_correct)\n",
        "    print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "else:\n",
        "    print(\"No pairs found in the JSONL file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di {{text}}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Total Pairs: 524\n",
            "Total Correct: 149\n",
            "Accuracy: 28.44 %\n",
            "Mean Confidence: 0.98\n",
            "\n",
            "Prompt: Considerando {{text}}, quanto lo trovi accettabile? Esprimi la tua valutazione su una scala da 1 a 7, dove '1' corrisponde a 'Pessimo' e '7' a 'Eccellente'.\n",
            "Total Pairs: 524\n",
            "Total Correct: 149\n",
            "Accuracy: 28.44 %\n",
            "Mean Confidence: 0.98\n",
            "\n",
            "Prompt: Su una scala da 1 a 7, con '1' rappresentante 'Pessimo' e '7' 'Eccellente', quanto accettabile trovi {{text}}?\n",
            "Total Pairs: 524\n",
            "Total Correct: 149\n",
            "Accuracy: 28.44 %\n",
            "Mean Confidence: 0.83\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def prompt_testing(prompt):\n",
        "    total_correct = 0\n",
        "    total_pairs = 0\n",
        "    total_confidence = 0\n",
        "\n",
        "    # Load the JSONL file and process each pair\n",
        "    with open('It-Subtask2-train.jsonl', 'r') as file:\n",
        "        data = json.load(file)  # Load only the first 5 pairs\n",
        "        for pair in data:\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label = int(pair['label'])  # Convert label_index to an integer\n",
        "            \n",
        "            # Format with text and hypothesis \n",
        "            prompt_formatted = prompt.replace('{{text}}', text)\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == choices[label]\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "            total_confidence += probabilities[prediction_index]\n",
        "\n",
        "    # Calculate statistics \n",
        "    if total_pairs > 0:\n",
        "        accuracy = total_correct / total_pairs * 100\n",
        "        mean_confidence = total_confidence / total_pairs\n",
        "\n",
        "        # Print statistics\n",
        "        print(\"Total Pairs:\", total_pairs)\n",
        "        print(\"Total Correct:\", total_correct)\n",
        "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "        print(\"Mean Confidence:\", round(mean_confidence, 2))\n",
        "    else:\n",
        "        print(\"No pairs found in the JSONL file.\")\n",
        "\n",
        "# Load prompts from the JSONL file and test each prompt\n",
        "with open('prompts.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = json.load(prompts_file)\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        prompt_testing(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Prompt:** Quanto ritieni accettabile {{text}}, su una scala da 1 a 7, dove 1 è \"Pessimo\" e 7 è \"Eccellente\"?\n",
        "\n",
        "Total Pairs: 1009\n",
        "\n",
        "Total Correct: 257\n",
        "\n",
        "Accuracy: 25.47 %\n",
        "\n",
        "Mean Confidence: 0.98\n",
        "\n",
        "**Prompt:** Considerando {{text}}, quanto lo valuteresti in termini di accettabilità, che va da \"Pessimo\" a \"Eccellente\"?\n",
        "\n",
        "Total Pairs: 1009\n",
        "\n",
        "Total Correct: 256\n",
        "\n",
        "Accuracy: 25.37 %\n",
        "\n",
        "Mean Confidence: 0.83\n",
        "\n",
        "**Prompt:** Su una scala da 1 a 7, con \"Pessimo\" come 1 e \"Eccellente\" come 7, quanto accettabile trovi {{text}}?\n",
        "\n",
        "Total Pairs: 1009\n",
        "\n",
        "Total Correct: 257\n",
        "\n",
        "Accuracy: 25.47 %\n",
        "\n",
        "Mean Confidence: 0.7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
