{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FvZqaH1qRX"
      },
      "source": [
        "# **Homework 1 - task 28**\n",
        "The main goal of this homework is to transform existing evaluation datasets into a format suitable for evaluating the linguistic skills of Large Language Models (LLMs) by reframing tasks as multi-choice Question Answering (QA) tasks, providing effective prompts, and generating distractors where necessary, all formatted in JSON Lines standard for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwL2mdH9KgG",
        "outputId": "2b4cad03-1874-4172-e207-09efca0537f0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_rtvMA3HO2"
      },
      "source": [
        "# **1. Data Loading:**\n",
        "\n",
        "\n",
        "First we need to download the datasets from evalita to our python eviroment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc5CmH4wIq",
        "outputId": "0ded34fb-6853-4b38-81cf-d7ddc76fd932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_2\\SemEval2022Task3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'SemEval2022Task3' already exists and is not an empty directory.\n",
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "##### Data Reader -- English/French/Italian\n",
        "!git clone https://github.com/shammur/SemEval2022Task3.git\n",
        "%cd ./SemEval2022Task3/\n",
        "#data/train/train_subtask-2/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sub Task 2\n",
        "\n",
        "**Regression sub-task**, which consists in predicting the average score assigned by human annotators on a seven point Likert-scale with respect to the subset of data evaluated via crowdsourcing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemLwOEanl_P"
      },
      "source": [
        "## 1.1 visualize initial data\n",
        "\n",
        "**dev.xml** is the training (development) dataset.\n",
        "\n",
        "\n",
        "**test_gold.xml** is the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(r'c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_2')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PypMtd44ij",
        "outputId": "49483cc0-909e-4732-d177-4d3cee53c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tConstruction\tSentence\tScores\n",
            "it_0\tingeneral\tAmo gli arbusti, e le querce in generale.\t2.91\n",
            "it_1\tparticular\tAmo le montagne, e in particolare i centri commerciali.\t1.3\n",
            "it_2\tcomparatives\tAmo le foreste più delle case.\t5.92\n",
            "it_3\tandtoo\tAmo le foreste, ed anche i centri commerciali.\t5.6\n"
          ]
        }
      ],
      "source": [
        "def visualize_original_data(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for i in range(5):  \n",
        "                line = file.readline()\n",
        "                print(line.rstrip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Check test data\n",
        "file_path = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-2\\It-Subtask2-scores.tsv'\n",
        "visualize_original_data(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tScore\n",
            "73\tAdoro gli abeti, ma non i cespugli.\t4.33\n",
            "106\tAdoro il vitello, ma non i granchi.\t3.33\n",
            "203\tAdoro gli animali, un tipo interessante di abete.\t1.25\n",
            "466\tAmo gli abeti più dei cespugli.\t5.18\n"
          ]
        }
      ],
      "source": [
        "file_path = r'SemEval2022Task3\\data\\train\\train_subtask-2\\it\\It-Subtask2-fold_0.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"id\": 523,\n",
            "    \"text\": \"Amo le ostriche, ed anche il tacchino.\",\n",
            "    \"choices\": [\n"
          ]
        }
      ],
      "source": [
        "file_path = r'SemEval2022Task3\\data\\train\\train_subtask-2\\it\\It-Subtask2-fold_1.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRnPwyNp7H9S"
      },
      "source": [
        "## 1.2 Data Reframing:\n",
        "  First we need to change the type of the file from xml to json. Then we want to convert that json into the following format:\n",
        "```JSON\n",
        "{\n",
        "    \"id\":       int,\n",
        "    \"text\":     str,\n",
        "    \"choices\":  list[str|int],\n",
        "    \"label\":    int\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "31jYexPXZnYI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def tsv_to_jsonl(input_file, output_file):\n",
        "    with open(input_file, 'r', newline='') as tsvfile:\n",
        "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
        "        with open(output_file, 'w') as jsonlfile:\n",
        "            for row in reader:\n",
        "                json.dump(row, jsonlfile)\n",
        "                jsonlfile.write('\\n')\n",
        "\n",
        "tsv_file = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-2\\It-Subtask2-scores.tsv'\n",
        "jsonl_file = 'It-Subtask2-test.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "file1 = r'SemEval2022Task3\\data\\train\\train_subtask-2\\it\\It-Subtask2-fold_0.tsv'\n",
        "file2 = r'SemEval2022Task3\\data\\train\\train_subtask-2\\it\\It-Subtask2-fold_1.tsv'\n",
        "jsonl_file1 = 'It-Subtask2-fold_0.jsonl'\n",
        "jsonl_file2 = 'It-Subtask2-fold_1.jsonl'\n",
        "tsv_to_jsonl(file1, jsonl_file1)\n",
        "tsv_to_jsonl(file2, jsonl_file2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Join the 2 jsonl file in order to have a unique training dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join the fold training data to a single file\n",
        "\n",
        "def join_jsonl_files(file_paths, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "\n",
        "file1 = r'It-Subtask2-fold_0.jsonl'\n",
        "file2 = r'It-Subtask2-fold_1.jsonl'\n",
        "output_file = 'It-Subtask2-train.jsonl'\n",
        "\n",
        "join_jsonl_files([file1, file2], output_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXDo77YX9bA"
      },
      "source": [
        "## 1.3 Visualize jsonl information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuqfLHqczdA",
        "outputId": "6d7ea7af-81ab-4927-ad97-6b2e227c132a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': 'it_0', 'Construction': 'ingeneral', 'Sentence': 'Amo gli arbusti, e le querce in generale.', 'Scores': '2.91'}\n",
            "{'ID': 'it_1', 'Construction': 'particular', 'Sentence': 'Amo le montagne, e in particolare i centri commerciali.', 'Scores': '1.3'}\n",
            "{'ID': 'it_2', 'Construction': 'comparatives', 'Sentence': 'Amo le foreste più delle case.', 'Scores': '5.92'}\n",
            "{'ID': 'it_3', 'Construction': 'andtoo', 'Sentence': 'Amo le foreste, ed anche i centri commerciali.', 'Scores': '5.6'}\n",
            "{'ID': 'it_4', 'Construction': 'ingeneral', 'Sentence': 'Amo i merli, e i gatti in generale.', 'Scores': '3.22'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def visualize_data(jsonl_file):\n",
        "    with open(jsonl_file, 'r') as f:\n",
        "        line_count = 0\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            print(data)\n",
        "            line_count += 1\n",
        "            if line_count == 5:\n",
        "                break\n",
        "\n",
        "json_file = 'It-Subtask2-test.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvfeYF7L8cuz",
        "outputId": "227a2ba4-268f-4e1f-f5fc-c4fddfc72101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': '73', 'Sentence': 'Adoro gli abeti, ma non i cespugli.', 'Score': '4.33'}\n",
            "{'ID': '106', 'Sentence': 'Adoro il vitello, ma non i granchi.', 'Score': '3.33'}\n",
            "{'ID': '203', 'Sentence': 'Adoro gli animali, un tipo interessante di abete.', 'Score': '1.25'}\n",
            "{'ID': '466', 'Sentence': 'Amo gli abeti più dei cespugli.', 'Score': '5.18'}\n",
            "{'ID': '137', 'Sentence': 'Amo i criceti, un tipo interessante di pappagallo.', 'Score': '1.36'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask2-train.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8cn4OBDRlj"
      },
      "source": [
        "\n",
        "## 1.4 Format jsonl\n",
        "\n",
        "In order to get the json file with the desired format:\n",
        "\n",
        "1. Rename ID to id. (int)\n",
        "2. Rename Sentence to text.  (str)\n",
        "3. Add a choices key with 4 of the 7  possible labels as values. One must be correct.\n",
        "4. Rename Labels to label (to the position of the correct choice).\n",
        "5. Delete Contruction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "def rearrange_json(input_file_path, output_file_path):\n",
        "    id_counter = 0\n",
        "    # Mapping scores to Italian descriptors used in Likert scales\n",
        "    score_to_descriptor = {\n",
        "        1: \"Pessimo\",\n",
        "        2: \"Scarso\",\n",
        "        3: \"Sufficiente\",\n",
        "        4: \"Buono\",\n",
        "        5: \"Molto buono\",\n",
        "        6: \"Ottimo\",\n",
        "        7: \"Eccellente\"\n",
        "    }\n",
        "    rearranged_data = []  # Initialize outside the loop\n",
        "    with open(input_file_path, \"r\") as input_file:\n",
        "        for line in input_file:\n",
        "            data = json.loads(line)\n",
        "            if 'ID' in data and 'Sentence' in data:\n",
        "                score_key = 'Scores' if 'Scores' in data else 'Score'\n",
        "                try:\n",
        "                    score = int(round(float(data[score_key])))\n",
        "                except ValueError:\n",
        "                    print(f\"Invalid score value\")\n",
        "                    continue\n",
        "                new_pair = {\n",
        "                    \"id\": id_counter,\n",
        "                    #\"id\": int(pair['ID'].split('_')[-1]),  # Extracting the integer part of the ID\n",
        "                    \"text\": data['Sentence'],\n",
        "                    \"choices\": [],  \n",
        "                    \"label\": None  \n",
        "                }\n",
        "                # Generating choices based on list of descriptors\n",
        "                choices = [(score_to_descriptor[i], i) for i in range(1, 8) if i != score]\n",
        "                # Ensure unique choices including the correct answer\n",
        "                choices = random.sample(choices, 3)\n",
        "                choices.append((score_to_descriptor[score], score))\n",
        "                # Shuffle the choices to randomize the position of the correct answer\n",
        "                random.shuffle(choices)\n",
        "                new_pair[\"choices\"] = choices\n",
        "                # Generate label within the range of choices\n",
        "                new_pair[\"label\"] = [choice[1] for choice in new_pair[\"choices\"]].index(score)\n",
        "                rearranged_data.append(new_pair)\n",
        "                id_counter += 1\n",
        "\n",
        "    # Write rearranged data back to the file\n",
        "    with open(output_file_path, \"w\") as output_file:\n",
        "        for data_entry in rearranged_data:\n",
        "            json.dump(data_entry, output_file)\n",
        "            output_file.write('\\n')\n",
        "\n",
        "input_file_path = 'It-Subtask2-test.jsonl'\n",
        "output_file_path = 'PreTENS-task2-test-data.jsonl'\n",
        "rearrange_json(input_file_path, output_file_path)\n",
        "\n",
        "\n",
        "input_file_path = 'It-Subtask2-train.jsonl'\n",
        "output_file_path = 'PreTENS-task2-train-data.jsonl'\n",
        "rearrange_json(input_file_path, output_file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdUFiAoYJDi"
      },
      "source": [
        "## 1.5 Visualize final jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'text': 'Amo gli arbusti, e le querce in generale.', 'choices': [['Ottimo', 6], ['Eccellente', 7], ['Sufficiente', 3], ['Buono', 4]], 'label': 2}\n",
            "{'id': 1, 'text': 'Amo le montagne, e in particolare i centri commerciali.', 'choices': [['Molto buono', 5], ['Ottimo', 6], ['Eccellente', 7], ['Pessimo', 1]], 'label': 3}\n",
            "{'id': 2, 'text': 'Amo le foreste più delle case.', 'choices': [['Sufficiente', 3], ['Eccellente', 7], ['Molto buono', 5], ['Ottimo', 6]], 'label': 3}\n",
            "{'id': 3, 'text': 'Amo le foreste, ed anche i centri commerciali.', 'choices': [['Sufficiente', 3], ['Ottimo', 6], ['Pessimo', 1], ['Scarso', 2]], 'label': 1}\n",
            "{'id': 4, 'text': 'Amo i merli, e i gatti in generale.', 'choices': [['Pessimo', 1], ['Molto buono', 5], ['Buono', 4], ['Sufficiente', 3]], 'label': 3}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'PreTENS-task2-test-data.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoTfnvAj-KS",
        "outputId": "559cd3c1-f6ed-4dc9-ff7a-d683b5c8ace0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'text': 'Adoro gli abeti, ma non i cespugli.', 'choices': [['Buono', 4], ['Scarso', 2], ['Eccellente', 7], ['Molto buono', 5]], 'label': 0}\n",
            "{'id': 1, 'text': 'Adoro il vitello, ma non i granchi.', 'choices': [['Sufficiente', 3], ['Scarso', 2], ['Pessimo', 1], ['Buono', 4]], 'label': 0}\n",
            "{'id': 2, 'text': 'Adoro gli animali, un tipo interessante di abete.', 'choices': [['Scarso', 2], ['Ottimo', 6], ['Pessimo', 1], ['Buono', 4]], 'label': 2}\n",
            "{'id': 3, 'text': 'Amo gli abeti più dei cespugli.', 'choices': [['Pessimo', 1], ['Scarso', 2], ['Molto buono', 5], ['Buono', 4]], 'label': 2}\n",
            "{'id': 4, 'text': 'Amo i criceti, un tipo interessante di pappagallo.', 'choices': [['Molto buono', 5], ['Pessimo', 1], ['Sufficiente', 3], ['Buono', 4]], 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'PreTENS-task2-train-data.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdoWdSVkZ6o"
      },
      "source": [
        "## **2. Promt formulation**\n",
        "\n",
        "In order to use this dataset we need to generate three prompts that can be used to get if a text is entailed to a hypothesis and then insert them into a json file. The three prompts are:\n",
        "\n",
        "**Template 1:**\n",
        "\n",
        "\n",
        "Prompt: \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Evaluate whether the following text is acceptable or not according to the context: {{text}}\"\n",
        "\n",
        "**Template 2:**\n",
        "\n",
        "Prompt: \"Determina se la seguente frase è coerente con il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Determine if the following sentence is coherent with the context: {{text}}\"\n",
        "\n",
        "**Template 3:**\n",
        "\n",
        "Prompt: \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "\n",
        "Translation: \"Decide if the provided text is congruent with the described situation: {{text}}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3r6Gp2kqX6",
        "outputId": "25bb080b-bcda-4ecc-ac78-7971eb511538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"prompt\": \"Qual \\u00e8 il tuo giudizio sull'accettabilit\\u00e0 di {{text}}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\"}\n",
            "{\"prompt\": \"Considerando {{text}}, quanto lo trovi accettabile? Esprimi la tua valutazione su una scala da 1 a 7, dove '1' corrisponde a 'Pessimo' e '7' a 'Eccellente'.\"}\n",
            "{\"prompt\": \"Su una scala da 1 a 7, con '1' rappresentante 'Pessimo' e '7' 'Eccellente', quanto accettabile trovi {{text}}?\"}\n",
            "\n",
            "JSON file 'PreTENS-task2-prompts.jsonl' generated successfully with 3 prompts.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate a JSON Lines file with a list of prompts\n",
        "def generate_prompts(prompts, output_file):\n",
        "    with open(output_file, 'w') as jsonl_file:\n",
        "        for prompt in prompts:\n",
        "            jsonl_file.write(json.dumps({\"prompt\": prompt}) + '\\n')\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as jsonl_file:\n",
        "        print(jsonl_file.read())\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"Qual è il tuo giudizio sull'accettabilità di {{text}}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\",\n",
        "    \"Considerando {{text}}, quanto lo trovi accettabile? Esprimi la tua valutazione su una scala da 1 a 7, dove '1' corrisponde a 'Pessimo' e '7' a 'Eccellente'.\",\n",
        "    \"Su una scala da 1 a 7, con '1' rappresentante 'Pessimo' e '7' 'Eccellente', quanto accettabile trovi {{text}}?\"\n",
        "]\n",
        "\n",
        "output_file = \"PreTENS-task2-prompts.jsonl\"\n",
        "generate_prompts(prompts, output_file)\n",
        "print(f\"JSON file '{output_file}' generated successfully with {len(prompts)} prompts.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpbmKeG4uai"
      },
      "source": [
        "# **3. Test Prompts with dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2HXdEu6827D",
        "outputId": "3a39d619-ec8a-4785-c2c4-f62f718177a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "V1tsB0SwOl4B"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runtimeFlag = device #Run on GPU (you can't run GPTQ on cpu)\n",
        "cache_dir = None # by default, don't set a cache directory. This is automatically updated if you connect Google Drive.\n",
        "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## 3.1 Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-HYbuJ6JCgd",
        "outputId": "1d078b01-2e78-41cc-86fc-d24885b4fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (0.7.1)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (0.28.0)\n",
            "Requirement already satisfied: datasets in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
            "Requirement already satisfied: rouge in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (1.1.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (2.2.2+cu118)\n",
            "Requirement already satisfied: safetensors in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (4.39.2)\n",
            "Requirement already satisfied: peft>=0.5.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (24.0)\n",
            "Requirement already satisfied: psutil in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.8)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.2)\n",
            "Requirement already satisfied: filelock in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.3)\n",
            "Requirement already satisfied: fsspec in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from tqdm->auto-gptq) (0.4.6)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (2.2.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "038f994b-971a-46bf-e1f6-6f908ef1d874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "## 3.2 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "iO6WB6xkWCrN"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# **4. Evaluate Homework prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbdUQxHVD8D"
      },
      "source": [
        "## 4.1. Test response of Promtps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSKyQeQrml_W",
        "outputId": "856c87fa-c12a-4092-fc33-1a0533429aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [0.01952113 0.9744667  0.00601211]\n",
            "ID: 0\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo gli arbusti, e le querce in generale., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': ['Eccellente', 7], 'confidence': 97.4}\n",
            "Actual Label: ['Sufficiente', 3]\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.01955767 0.9729056  0.00753672]\n",
            "ID: 1\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo le montagne, e in particolare i centri commerciali., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': ['Ottimo', 6], 'confidence': 97.3}\n",
            "Actual Label: ['Pessimo', 1]\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.02535288 0.9697725  0.00487464]\n",
            "ID: 2\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo le foreste più delle case., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': ['Eccellente', 7], 'confidence': 97.0}\n",
            "Actual Label: ['Ottimo', 6]\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.01362344 0.983266   0.00311051]\n",
            "ID: 3\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo le foreste, ed anche i centri commerciali., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': ['Ottimo', 6], 'confidence': 98.3}\n",
            "Actual Label: ['Ottimo', 6]\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.01673029 0.9787921  0.00447766]\n",
            "ID: 4\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di Amo i merli, e i gatti in generale., su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Prediction: {'label': ['Molto buono', 5], 'confidence': 97.9}\n",
            "Actual Label: ['Sufficiente', 3]\n",
            "Correct Prediction: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def visualize_prompt_response(jsonl_file):\n",
        "    # Load your JSON file and process each pair\n",
        "    with open(jsonl_file, 'r') as file:\n",
        "        count = 0  # Counter to track processed pairs\n",
        "        for line in file:\n",
        "            if count >= 5:  # Break the loop if 5 pairs have been processed\n",
        "                break\n",
        "            pair = json.loads(line)\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "            label = choices[label_index]  # Get the actual label using label_index\n",
        "\n",
        "            # Format the prompt with text and hypothesis\n",
        "            prompt_formatted = f\"Qual è il tuo giudizio sull'accettabilità di {text}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\"\n",
        "\n",
        "            # Assuming you have tokenizer_llama and model_llama defined elsewhere\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            print(f\"Probabilities: {probabilities}\")\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "            prediction_confidence = probabilities[prediction_index] * 100\n",
        "            prediction = {\"label\": prediction_label, \"confidence\": round(prediction_confidence, 1)}\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "            count += 1\n",
        "\n",
        "            # Visualize the formatted prompt and the prediction details\n",
        "            print(f\"ID: {id}\")\n",
        "            print(\"Prompt:\", prompt_formatted)\n",
        "            print(\"Prediction:\", prediction)\n",
        "            print(\"Actual Label:\", label)\n",
        "            print(\"Correct Prediction:\", is_correct)\n",
        "            print()\n",
        "        \n",
        "jsonl_file = 'PreTENS-task2-test-data.jsonl'\n",
        "visualize_prompt_response(jsonl_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_9dG1CVHo0"
      },
      "source": [
        "## 4.2. Compare the diferent prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-4DiwuQPpb",
        "outputId": "1d5a9e15-7957-424f-d7a9-0a73ff3056ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Statistics:\n",
            "Total Pairs: 1009\n",
            "Total Correct: 247\n",
            "Accuracy: 24.48 %\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "total_correct = 0\n",
        "total_pairs = 0\n",
        "\n",
        "# Load the JSONL file and process each pair\n",
        "with open(\"PreTENS-task2-test-data.jsonl\", 'r') as file:\n",
        "        for line in file:\n",
        "            pair = json.loads(line)\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label = int(pair['label']) \n",
        "        \n",
        "            # Format the prompt with actual text and hypothesis\n",
        "            prompt_formatted = f\"Qual è il tuo giudizio sull'accettabilità di {text}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\"\n",
        "\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == choices[label]\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "\n",
        "# Calculate overall statistics only if at least one pair was processed\n",
        "if total_pairs > 0:\n",
        "    accuracy = total_correct / total_pairs * 100\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"Overall Statistics:\")\n",
        "    print(\"Total Pairs:\", total_pairs)\n",
        "    print(\"Total Correct:\", total_correct)\n",
        "    print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "else:\n",
        "    print(\"No pairs found in the JSONL file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Qual è il tuo giudizio sull'accettabilità di {{text}}, su una scala da 1 a 7? ('1' indica 'Pessimo' e '7' indica 'Eccellente').\n",
            "Total Pairs: 1009\n",
            "Total Correct: 247\n",
            "Accuracy: 24.48 %\n",
            "Mean Confidence: 0.95\n",
            "\n",
            "Prompt: Considerando {{text}}, quanto lo trovi accettabile? Esprimi la tua valutazione su una scala da 1 a 7, dove '1' corrisponde a 'Pessimo' e '7' a 'Eccellente'.\n",
            "Total Pairs: 1009\n",
            "Total Correct: 247\n",
            "Accuracy: 24.48 %\n",
            "Mean Confidence: 0.98\n",
            "\n",
            "Prompt: Su una scala da 1 a 7, con '1' rappresentante 'Pessimo' e '7' 'Eccellente', quanto accettabile trovi {{text}}?\n",
            "Total Pairs: 1009\n",
            "Total Correct: 252\n",
            "Accuracy: 24.98 %\n",
            "Mean Confidence: 0.74\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def prompt_testing(prompt, input_file):\n",
        "    total_correct = 0\n",
        "    total_pairs = 0\n",
        "    total_confidence = 0\n",
        "\n",
        "    # Load the JSONL file and process each pair\n",
        "    with open(input_file, 'r') as file:\n",
        "        for line in file:\n",
        "            pair = json.loads(line)\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label = int(pair['label']) \n",
        "            \n",
        "            # Format with text and hypothesis \n",
        "            prompt_formatted = prompt.replace('{{text}}', text)\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == choices[label]\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "            total_confidence += probabilities[prediction_index]\n",
        "\n",
        "    # Calculate statistics \n",
        "    if total_pairs > 0:\n",
        "        accuracy = total_correct / total_pairs * 100\n",
        "        mean_confidence = total_confidence / total_pairs\n",
        "\n",
        "        # Print statistics\n",
        "        print(\"Total Pairs:\", total_pairs)\n",
        "        print(\"Total Correct:\", total_correct)\n",
        "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "        print(\"Mean Confidence:\", round(mean_confidence, 2))\n",
        "    else:\n",
        "        print(\"No pairs found in the JSONL file.\")\n",
        "\n",
        "# Load prompts from the JSONL file and test each prompt\n",
        "with open(\"PreTENS-task2-prompts.jsonl\", 'r') as prompts_file:\n",
        "        for line in prompts_file:\n",
        "            prompt_data = json.loads(line)\n",
        "            prompt = prompt_data['prompt']\n",
        "            input_file = \"PreTENS-task2-test-data.jsonl\"\n",
        "            print(\"\\nPrompt:\", prompt)\n",
        "            prompt_testing(prompt, input_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Prompt:** Quanto ritieni accettabile {{text}}, su una scala da 1 a 7, dove 1 è \"Pessimo\" e 7 è \"Eccellente\"?\n",
        "\n",
        "Total Pairs: 1009\n",
        "\n",
        "Total Correct: 257\n",
        "\n",
        "Accuracy: 25.47 %\n",
        "\n",
        "Mean Confidence: 0.98\n",
        "\n",
        "**Prompt:** Considerando {{text}}, quanto lo valuteresti in termini di accettabilità, che va da \"Pessimo\" a \"Eccellente\"?\n",
        "\n",
        "Total Pairs: 1009\n",
        "\n",
        "Total Correct: 256\n",
        "\n",
        "Accuracy: 25.37 %\n",
        "\n",
        "Mean Confidence: 0.83\n",
        "\n",
        "**Prompt:** Su una scala da 1 a 7, con \"Pessimo\" come 1 e \"Eccellente\" come 7, quanto accettabile trovi {{text}}?\n",
        "\n",
        "Total Pairs: 1009\n",
        "\n",
        "Total Correct: 257\n",
        "\n",
        "Accuracy: 25.47 %\n",
        "\n",
        "Mean Confidence: 0.7"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
