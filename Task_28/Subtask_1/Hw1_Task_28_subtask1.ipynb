{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FvZqaH1qRX"
      },
      "source": [
        "# **Homework 1 - task 28**\n",
        "The main goal of this homework is to transform existing evaluation datasets into a format suitable for evaluating the linguistic skills of Large Language Models (LLMs) by reframing tasks as multi-choice Question Answering (QA) tasks, providing effective prompts, and generating distractors where necessary, all formatted in JSON Lines standard for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwL2mdH9KgG",
        "outputId": "2b4cad03-1874-4172-e207-09efca0537f0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_rtvMA3HO2"
      },
      "source": [
        "# **1. Data Loading:**\n",
        "\n",
        "\n",
        "First we need to download the datasets from evalita to our python eviroment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc5CmH4wIq",
        "outputId": "0ded34fb-6853-4b38-81cf-d7ddc76fd932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_1\\SemEval2022Task3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'SemEval2022Task3' already exists and is not an empty directory.\n",
            "c:\\Users\\35193\\miniconda3\\envs\\cuda_env\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n",
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "##### Data Reader -- English/French/Italian\n",
        "!git clone https://github.com/shammur/SemEval2022Task3.git\n",
        "%cd ./SemEval2022Task3/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sub Task 1 \n",
        "\n",
        "**binary classification sub-task**, which consists in predicting the acceptability label assigned to each sentence of the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemLwOEanl_P"
      },
      "source": [
        "## 1.1 visualize initial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(r'c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_1')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PypMtd44ij",
        "outputId": "49483cc0-909e-4732-d177-4d3cee53c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tConstruction\tSentence\tLabels\n",
            "it_0\tprefer\tNon amo i campi , preferisco i condomini .\t1\n",
            "it_1\tparticular\tPosso capire le emozioni , e in particolare la tristezza .\t1\n",
            "it_2\tgenerally\tApprezzo i giochi da tavolo , e piÃ¹ in generale il calcio .\t0\n",
            "it_3\tgenerally\tAmo i film , e piÃ¹ in generale i western .\t0\n"
          ]
        }
      ],
      "source": [
        "def visualize_original_data(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for i in range(5):  \n",
        "                line = file.readline()\n",
        "                print(line.rstrip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Check test data\n",
        "file_path = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv'\n",
        "visualize_original_data(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3stMZxk6ZPK",
        "outputId": "890bf8b7-0a86-419d-e438-eca20df4c7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "5491\tAmo i cartoni animati , ma non i sussidiari .\t1\n",
            "5084\tApprezzo il vino , ma non il Chianti .\t1\n",
            "1677\tAmo i manuali , un tipo interessante di dipinto .\t0\n",
            "1959\tAmo gli arbusti , ed anche le querce .\t1\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "2625\tHo incontrato gli astronomi , ed anche gli scienziati .\t0\n",
            "3069\tNon amo le carote , preferisco il prosciutto cotto .\t1\n",
            "2414\tSi fida dei sensi , ed anche della vista .\t0\n",
            "2239\tAmo i granchi , ed anche il pollo .\t1\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_1.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "5407\tSi fida dell' udito , ma non le dicerie .\t1\n",
            "322\tAmo i sussidiari , e piÃ¹ specificamente i thriller .\t0\n",
            "2162\tApprezzo i giochi da tavolo , ed anche il blues .\t1\n",
            "377\tAmo le collane , e piÃ¹ specificamente i gioielli .\t0\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_2.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRnPwyNp7H9S"
      },
      "source": [
        "## 1.2 Data Reframing:\n",
        "  First we need to change the type of the file from xml to json. Then we want to convert that json into the following format:\n",
        "```JSON\n",
        "{\n",
        "    \"id\":       int,\n",
        "    \"text\":     str,\n",
        "    \"choices\":  list[str],\n",
        "    \"label\":    int\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "31jYexPXZnYI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def tsv_to_jsonl(input_file, output_file):\n",
        "    with open(input_file, 'r', newline='') as tsvfile:\n",
        "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
        "        with open(output_file, 'w') as jsonlfile:\n",
        "            for row in reader:\n",
        "                json.dump(row, jsonlfile)\n",
        "                jsonlfile.write('\\n')\n",
        "\n",
        "tsv_file = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv'\n",
        "jsonl_file = 'It-Subtask1-test.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "file1 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv'\n",
        "out_file1 = 'It-Subtask1-fold_0.jsonl'\n",
        "file2 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_1.tsv'\n",
        "out_file2 = 'It-Subtask1-fold_1.jsonl'\n",
        "file3 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_2.tsv'\n",
        "out_file3 = 'It-Subtask1-fold_2.jsonl'\n",
        "\n",
        "tsv_to_jsonl(file1, out_file1)\n",
        "tsv_to_jsonl(file2, out_file2)\n",
        "tsv_to_jsonl(file3, out_file3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join the fold training data to a single file\n",
        "\n",
        "def join_files(file_paths, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "\n",
        "# Example usage\n",
        "file1 = 'It-Subtask1-fold_0.jsonl'\n",
        "file2 = 'It-Subtask1-fold_1.jsonl'\n",
        "file3 = 'It-Subtask1-fold_2.jsonl'\n",
        "output_file = 'It-Subtask1-train.jsonl'\n",
        "\n",
        "join_files([file1, file2, file3], output_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXDo77YX9bA"
      },
      "source": [
        "## 1.3 Visualize jsonl information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuqfLHqczdA",
        "outputId": "6d7ea7af-81ab-4927-ad97-6b2e227c132a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': 'it_0', 'Construction': 'prefer', 'Sentence': 'Non amo i campi , preferisco i condomini .', 'Labels': '1'}\n",
            "{'ID': 'it_1', 'Construction': 'particular', 'Sentence': 'Posso capire le emozioni , e in particolare la tristezza .', 'Labels': '1'}\n",
            "{'ID': 'it_2', 'Construction': 'generally', 'Sentence': 'Apprezzo i giochi da tavolo , e piÃ¹ in generale il calcio .', 'Labels': '0'}\n",
            "{'ID': 'it_3', 'Construction': 'generally', 'Sentence': 'Amo i film , e piÃ¹ in generale i western .', 'Labels': '0'}\n",
            "{'ID': 'it_4', 'Construction': 'unlike', 'Sentence': 'A differenza dello shopping , il basket Ã¨ menzionato spesso in questo testo .', 'Labels': '1'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def visualize_data(jsonl_file):\n",
        "    with open(jsonl_file, 'r') as f:\n",
        "        line_count = 0\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            print(data)\n",
        "            line_count += 1\n",
        "            if line_count == 5:\n",
        "                break\n",
        "\n",
        "json_file = 'It-Subtask1-test.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvfeYF7L8cuz",
        "outputId": "227a2ba4-268f-4e1f-f5fc-c4fddfc72101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': '5491', 'Sentence': 'Amo i cartoni animati , ma non i sussidiari .', 'Labels': '1'}\n",
            "{'ID': '5084', 'Sentence': 'Apprezzo il vino , ma non il Chianti .', 'Labels': '1'}\n",
            "{'ID': '1677', 'Sentence': 'Amo i manuali , un tipo interessante di dipinto .', 'Labels': '0'}\n",
            "{'ID': '1959', 'Sentence': 'Amo gli arbusti , ed anche le querce .', 'Labels': '1'}\n",
            "{'ID': '731', 'Sentence': 'Uso il poliestere , eccetto il vetro .', 'Labels': '0'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-train.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8cn4OBDRlj"
      },
      "source": [
        "\n",
        "## 1.4 Format jsonl\n",
        "\n",
        "In order to get the json file with the desired format:\n",
        "\n",
        "1. Rename ID to id.\n",
        "2. Rename Sentence to text.\n",
        "3. Add a choices key with non acceptable  or aceptable.\n",
        "4. Rename Labels to label (0- non acceptable and 1 aceptable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "crG0YoFVg-YI"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import json\n",
        "\n",
        "# Rearrange the JSONL file content and write to another JSONL file\n",
        "def rearrange_jsonl(input_file_path, output_file_path):\n",
        "    id_counter = 0\n",
        "    with open(input_file_path, \"r\") as input_file:\n",
        "        with open(output_file_path, \"w\") as output_file:\n",
        "            for line in input_file:\n",
        "                data = json.loads(line)\n",
        "                new_data = {\n",
        "                    #\"id\": int(data['id']),\n",
        "                    \"id\": id_counter, # set id as an increment int\n",
        "                    \"text\": data['Sentence'],\n",
        "                    \"choices\": [\"non accettabile\", \"accettabile\"],\n",
        "                    \"label\": data['Labels']\n",
        "                }\n",
        "                id_counter += 1\n",
        "                json.dump(new_data, output_file)\n",
        "                output_file.write('\\n')\n",
        "\n",
        "input_jsonl_file = 'It-Subtask1-test.jsonl'\n",
        "output_jsonl_file = 'PreTENS-task1-test-data.jsonl'\n",
        "rearrange_jsonl(input_jsonl_file, output_jsonl_file)\n",
        "\n",
        "input_jsonl_file = 'It-Subtask1-train.jsonl'\n",
        "output_jsonl_file = 'PreTENS-task1-train-data.jsonl'\n",
        "rearrange_jsonl(input_jsonl_file, output_jsonl_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdUFiAoYJDi"
      },
      "source": [
        "## 1.5 Visualize final jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'text': 'Non amo i campi , preferisco i condomini .', 'choices': ['non accettabile', 'accettabile'], 'label': '1'}\n",
            "{'id': 1, 'text': 'Posso capire le emozioni , e in particolare la tristezza .', 'choices': ['non accettabile', 'accettabile'], 'label': '1'}\n",
            "{'id': 2, 'text': 'Apprezzo i giochi da tavolo , e piÃ¹ in generale il calcio .', 'choices': ['non accettabile', 'accettabile'], 'label': '0'}\n",
            "{'id': 3, 'text': 'Amo i film , e piÃ¹ in generale i western .', 'choices': ['non accettabile', 'accettabile'], 'label': '0'}\n",
            "{'id': 4, 'text': 'A differenza dello shopping , il basket Ã¨ menzionato spesso in questo testo .', 'choices': ['non accettabile', 'accettabile'], 'label': '1'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'PreTENS-task1-test-data.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoTfnvAj-KS",
        "outputId": "559cd3c1-f6ed-4dc9-ff7a-d683b5c8ace0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': 0, 'text': 'Amo i cartoni animati , ma non i sussidiari .', 'choices': ['non accettabile', 'accettabile'], 'label': '1'}\n",
            "{'id': 1, 'text': 'Apprezzo il vino , ma non il Chianti .', 'choices': ['non accettabile', 'accettabile'], 'label': '1'}\n",
            "{'id': 2, 'text': 'Amo i manuali , un tipo interessante di dipinto .', 'choices': ['non accettabile', 'accettabile'], 'label': '0'}\n",
            "{'id': 3, 'text': 'Amo gli arbusti , ed anche le querce .', 'choices': ['non accettabile', 'accettabile'], 'label': '1'}\n",
            "{'id': 4, 'text': 'Uso il poliestere , eccetto il vetro .', 'choices': ['non accettabile', 'accettabile'], 'label': '0'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'PreTENS-task1-train-data.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.6 Compare distribution of the dataset\n",
        "Allow to check in information was mantain after the Reframing of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def Check_dataset_JSONL(jsonl_file):\n",
        "    total = 0\n",
        "    accettabile_count = 0\n",
        "    non_accettabile_count = 0\n",
        "\n",
        "    with open(jsonl_file, \"r\") as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            choice = data['choices'][int(data['label'])]  # Convert 'label' to an integer\n",
        "            if choice == \"accettabile\":\n",
        "                accettabile_count += 1\n",
        "            elif choice == \"non accettabile\":\n",
        "                non_accettabile_count += 1\n",
        "            total += 1\n",
        "\n",
        "    return accettabile_count, non_accettabile_count, total\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Check_dataset_TSV(tsv_file):\n",
        "    total = 0\n",
        "    label_1_count = 0\n",
        "    label_0_count = 0\n",
        "\n",
        "    with open(tsv_file, \"r\") as file:\n",
        "        next(file)  # Skip header line\n",
        "        for line in file:\n",
        "            data = line.strip().split(\"\\t\")\n",
        "            label = int(data[-1])  # Convert label to integer\n",
        "            if label == 1:\n",
        "                label_1_count += 1\n",
        "            elif label == 0:\n",
        "                label_0_count += 1\n",
        "            total += 1\n",
        "\n",
        "    return label_1_count, label_0_count, total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONL Output file:\n",
            "accettabile: 6853\n",
            "non accettabile: 7707\n",
            "Total: 14560\n",
            "\n",
            "TSV Output file:\n",
            "accettabile: 6853\n",
            "non accettabile: 7707\n",
            "Total: 14560\n"
          ]
        }
      ],
      "source": [
        "# Check test the dataset distribution \n",
        "jsonl_file = \"PreTENS-task1-test-data.jsonl\"  # Update with your actual file path\n",
        "accettabile_count, non_accettabile_count, total = Check_dataset_JSONL(jsonl_file)\n",
        "print(\"JSONL Output file:\")\n",
        "print(\"accettabile:\", accettabile_count)\n",
        "print(\"non accettabile:\", non_accettabile_count)\n",
        "print(\"Total:\", total)\n",
        "\n",
        "tsv_file = r\"SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv\"  # Update with your actual file path\n",
        "label_1_count, label_0_count, total = Check_dataset_TSV(tsv_file)\n",
        "print(\"\\nTSV Output file:\")\n",
        "print(\"accettabile:\", label_1_count)\n",
        "print(\"non accettabile:\", label_0_count)\n",
        "print(\"Total:\", total)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JSONL Output file:\n",
            "accettabile: 3029\n",
            "non accettabile: 2808\n",
            "Total: 5837\n",
            "\n",
            "TSV Output file:\n",
            "accettabile: 3029\n",
            "non accettabile: 2808\n",
            "Total: 5837\n"
          ]
        }
      ],
      "source": [
        "# Check test the dataset distribution \n",
        "jsonl_file = \"PreTENS-task1-train-data.jsonl\" \n",
        "accettabile_count, non_accettabile_count, total = Check_dataset_JSONL(jsonl_file)\n",
        "print(\"JSONL Output file:\")\n",
        "print(\"accettabile:\", accettabile_count)\n",
        "print(\"non accettabile:\", non_accettabile_count)\n",
        "print(\"Total:\", total)\n",
        "\n",
        "tsv_file_0 = r\"SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv\"  \n",
        "tsv_file_1 = r\"SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_1.tsv\"\n",
        "tsv_file_2 = r\"SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_2.tsv\"  \n",
        "label_1_count_0, label_0_count_0, total_0 = Check_dataset_TSV(tsv_file_0)\n",
        "label_1_count_1, label_0_count_1, total_1 = Check_dataset_TSV(tsv_file_1)\n",
        "label_1_count_2, label_0_count_2, total_2 = Check_dataset_TSV(tsv_file_2)\n",
        "# Combine all folds into one single\n",
        "print(\"\\nTSV Output file:\")\n",
        "print(\"accettabile:\", label_1_count_0 + label_1_count_1 + label_1_count_2)\n",
        "print(\"non accettabile:\", label_0_count_1 + label_0_count_1 + label_0_count_2)\n",
        "print(\"Total:\", total_0 + total_1 + total_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdoWdSVkZ6o"
      },
      "source": [
        "## **2. Promt formulation**\n",
        "\n",
        "In order to use this dataset we need to generate three prompts that can be used to get if a text is acceptable within the context of the sentence and then insert them into a json file. The three prompts are:\n",
        "\n",
        "**Template 1:**\n",
        "\n",
        "\n",
        "Prompt: \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Evaluate whether the following text is acceptable or not according to the context: {{text}}\"\n",
        "\n",
        "**Template 2:**\n",
        "\n",
        "Prompt: \"Determina se la seguente frase è coerente con il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Determine if the following sentence is coherent with the context: {{text}}\"\n",
        "\n",
        "**Template 3:**\n",
        "\n",
        "Prompt: \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "\n",
        "Translation: \"Decide if the provided text is congruent with the described situation: {{text}}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3r6Gp2kqX6",
        "outputId": "25bb080b-bcda-4ecc-ac78-7971eb511538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"prompt\": \"Valuta se il seguente testo \\u00e8 accettabile o meno secondo il contesto: {{text}}\"}\n",
            "{\"prompt\": \"Determina se la seguente frase \\u00e8 coerente con il contesto: {{text}}\"}\n",
            "{\"prompt\": \"Decidi se il testo fornito \\u00e8 congruente con la situazione descritta: {{text}}\"}\n",
            "\n",
            "JSON Lines file 'PreTENS-task1-prompt.jsonl' generated successfully with 3 prompts.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate a JSON Lines file with a list of prompts\n",
        "def generate_prompts(prompts, output_file):\n",
        "    with open(output_file, 'w') as jsonl_file:\n",
        "        for prompt in prompts:\n",
        "            jsonl_file.write(json.dumps({\"prompt\": prompt}) + '\\n')\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as jsonl_file:\n",
        "        print(jsonl_file.read())\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\",\n",
        "    \"Determina se la seguente frase è coerente con il contesto: {{text}}\",\n",
        "    \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "]\n",
        "\n",
        "output_file = \"PreTENS-task1-prompt.jsonl\"\n",
        "generate_prompts(prompts, output_file)\n",
        "print(f\"JSON Lines file '{output_file}' generated successfully with {len(prompts)} prompts.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **3. Test if all the JSONL files are valid**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def is_jsonl_valid(filename):\n",
        "    try:\n",
        "        with open(filename, 'r') as f:\n",
        "            for line in f:\n",
        "                json.loads(line)\n",
        "    except ValueError as e:\n",
        "        print(f\"Invalid JSONL format: {e}\")\n",
        "        return False\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The JSONL file is valid.\n"
          ]
        }
      ],
      "source": [
        "# Train dataset\n",
        "filename = 'PreTENS-task1-train-data.jsonl' \n",
        "if is_jsonl_valid(filename):\n",
        "    print(\"The JSONL file is valid.\")\n",
        "else:\n",
        "    print(\"The JSONL file is not valid.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The JSONL file is valid.\n"
          ]
        }
      ],
      "source": [
        "# Test dataset\n",
        "filename = 'PreTENS-task1-test-data.jsonl' \n",
        "if is_jsonl_valid(filename):\n",
        "    print(\"The JSONL file is valid.\")\n",
        "else:\n",
        "    print(\"The JSONL file is not valid.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The JSONL file is valid.\n"
          ]
        }
      ],
      "source": [
        "# Prompt dataset\n",
        "filename = 'PreTENS-task1-prompt.jsonl'\n",
        "if is_jsonl_valid(filename):\n",
        "    print(\"The JSONL file is valid.\")\n",
        "else:\n",
        "    print(\"The JSONL file is not valid.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpbmKeG4uai"
      },
      "source": [
        "# **4. Test Prompts with dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2HXdEu6827D",
        "outputId": "3a39d619-ec8a-4785-c2c4-f62f718177a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "V1tsB0SwOl4B"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runtimeFlag = device #Run on GPU (you can't run GPTQ on cpu)\n",
        "cache_dir = None # by default, don't set a cache directory. This is automatically updated if you connect Google Drive.\n",
        "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## 3.1 Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-HYbuJ6JCgd",
        "outputId": "1d078b01-2e78-41cc-86fc-d24885b4fe9c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe and accelerate.exe are installed in 'C:\\Users\\35193\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\35193\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (0.29.1)\n",
            "Requirement already satisfied: datasets in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (1.26.0)\n",
            "Requirement already satisfied: rouge in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (1.0.7)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (2.0.1)\n",
            "Requirement already satisfied: safetensors in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (4.39.3)\n",
            "Requirement already satisfied: peft>=0.5.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from auto-gptq) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from accelerate>=0.26.0->auto-gptq) (21.3)\n",
            "Requirement already satisfied: psutil in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.3)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13.0->auto-gptq) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13.0->auto-gptq) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13.0->auto-gptq) (3.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers>=4.31.0->auto-gptq) (2.28.1)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm->auto-gptq) (0.4.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (1.5.3)\n",
            "Requirement already satisfied: xxhash in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets->auto-gptq) (2024.2.0)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets->auto-gptq) (22.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets->auto-gptq) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets->auto-gptq) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets->auto-gptq) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from packaging>=20.0->accelerate>=0.26.0->auto-gptq) (3.0.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2022.9.24)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas->datasets->auto-gptq) (2022.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\35193\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "038f994b-971a-46bf-e1f6-6f908ef1d874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "## 3.2 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "iO6WB6xkWCrN"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# **4. Evaluate Homework prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbdUQxHVD8D"
      },
      "source": [
        "## 4.1. Test response of Promtps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSKyQeQrml_W",
        "outputId": "856c87fa-c12a-4092-fc33-1a0533429aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [9.091138e-04 9.982749e-01 8.160408e-04]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo i cartoni animati , ma non i sussidiari .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 99.8}\n",
            "Actual Label: accettabile\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [2.1190660e-03 9.9753678e-01 3.4415934e-04]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Apprezzo il vino , ma non il Chianti .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 99.8}\n",
            "Actual Label: accettabile\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.00779086 0.9831966  0.00901253]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo i manuali , un tipo interessante di dipinto .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 98.3}\n",
            "Actual Label: non accettabile\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.0090381 0.986324  0.0046379]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo gli arbusti , ed anche le querce .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 98.6}\n",
            "Actual Label: accettabile\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.03575339 0.7079393  0.25630733]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Uso il poliestere , eccetto il vetro .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 70.8}\n",
            "Actual Label: non accettabile\n",
            "Correct Prediction: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def visualize_prompt_response(jsonl_file):\n",
        "    # Load your JSON file and process each pair\n",
        "    with open(jsonl_file, 'r') as file:\n",
        "        count = 0  # Counter to track processed pairs\n",
        "        for line in file:\n",
        "            if count >= 5:  # Break the loop if 5 pairs have been processed\n",
        "                break\n",
        "            pair = json.loads(line)\n",
        "            #id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "            label = choices[label_index]  # Get the actual label using label_index\n",
        "\n",
        "            # Format the prompt with text and hypothesis\n",
        "            prompt_formatted = f\"Valuta se il seguente testo è accettabile o meno secondo il contesto: {text}\"\n",
        "\n",
        "            # Assuming you have tokenizer_llama and model_llama defined elsewhere\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            print(f\"Probabilities: {probabilities}\")\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "            prediction_confidence = probabilities[prediction_index] * 100\n",
        "            prediction = {\"label\": prediction_label, \"confidence\": round(prediction_confidence, 1)}\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "\n",
        "            # Visualize the formatted prompt and the prediction details\n",
        "            print(\"Prompt:\", prompt_formatted)\n",
        "            print(\"Prediction:\", prediction)\n",
        "            print(\"Actual Label:\", label)\n",
        "            print(\"Correct Prediction:\", is_correct)\n",
        "            print()\n",
        "            count += 1  # Increment the counter for processed pairs\n",
        "\n",
        "jsonl_file = 'PreTENS-task1-train-data.jsonl'\n",
        "visualize_prompt_response(jsonl_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_9dG1CVHo0"
      },
      "source": [
        "## 4.2. Compare the diferent prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prevent CUDA from running out of memory \n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-4DiwuQPpb",
        "outputId": "1d5a9e15-7957-424f-d7a9-0a73ff3056ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall Statistics:\n",
            "Total Pairs: 5837\n",
            "Total Correct: 3029\n",
            "Accuracy: 51.89 %\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "total_correct = 0\n",
        "total_pairs = 0\n",
        "\n",
        "# Load the JSONL file and process each pair\n",
        "with open(\"PreTENS-task1-train-data.jsonl\", 'r') as file:\n",
        "        for line in file:\n",
        "            pair = json.loads(line)\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "            label = choices[label_index]  # Get the actual label using label_index\n",
        "    \n",
        "            # Format the prompt with actual text and hypothesis\n",
        "            prompt_formatted = f\"Valuta se il seguente testo \\u00e8 accettabile o meno secondo il contesto: {text}\"\n",
        "\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "\n",
        "# Calculate overall statistics only if at least one pair was processed\n",
        "if total_pairs > 0:\n",
        "    accuracy = total_correct / total_pairs * 100\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"Overall Statistics:\")\n",
        "    print(\"Total Pairs:\", total_pairs)\n",
        "    print(\"Total Correct:\", total_correct)\n",
        "    print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "else:\n",
        "    print(\"No pairs found in the JSONL file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\n",
            "Total Pairs: 5837\n",
            "Total Correct: 3029\n",
            "Accuracy: 51.89 %\n",
            "Mean Confidence: 0.92\n",
            "\n",
            "Prompt: Determina se la seguente frase è coerente con il contesto: {{text}}\n",
            "Total Pairs: 5837\n",
            "Total Correct: 3029\n",
            "Accuracy: 51.89 %\n",
            "Mean Confidence: 0.97\n",
            "\n",
            "Prompt: Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\n",
            "Total Pairs: 5837\n",
            "Total Correct: 3029\n",
            "Accuracy: 51.89 %\n",
            "Mean Confidence: 0.8\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def prompt_testing(prompt, file_path):\n",
        "    total_correct = 0\n",
        "    total_pairs = 0\n",
        "    total_confidence = 0\n",
        "\n",
        "    # Load the JSONL file and process each pair\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            pair = json.loads(line)\n",
        "            #id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  \n",
        "            label = choices[label_index]  \n",
        "            \n",
        "            # Format with text and hypothesis \n",
        "            prompt_formatted = prompt.replace('{{text}}', text)\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "            total_confidence += probabilities[prediction_index]\n",
        "\n",
        "    # Calculate statistics \n",
        "    if total_pairs > 0:\n",
        "        accuracy = total_correct / total_pairs * 100\n",
        "        mean_confidence = total_confidence / total_pairs\n",
        "\n",
        "        # Print statistics\n",
        "        print(\"Total Pairs:\", total_pairs)\n",
        "        print(\"Total Correct:\", total_correct)\n",
        "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "        print(\"Mean Confidence:\", round(mean_confidence, 2))\n",
        "    else:\n",
        "        print(\"No pairs found in the JSONL file.\")\n",
        "\n",
        "# Load prompts from the JSONL file and test each prompt\n",
        "with open('PreTENS-task1-prompt.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = []\n",
        "    for line in prompts_file:\n",
        "        prompt_item = json.loads(line)\n",
        "        prompt_data.append(prompt_item)\n",
        "\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        file_path = 'PreTENS-task1-train-data.jsonl'\n",
        "        prompt_testing(prompt, file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Check the difference of accuracy between test and trainind dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6851\n",
            "Accuracy: 47.05 %\n",
            "Mean Confidence: 0.92\n",
            "\n",
            "Prompt: Determina se la seguente frase è coerente con il contesto: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6852\n",
            "Accuracy: 47.06 %\n",
            "Mean Confidence: 0.97\n",
            "\n",
            "Prompt: Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6848\n",
            "Accuracy: 47.03 %\n",
            "Mean Confidence: 0.79\n"
          ]
        }
      ],
      "source": [
        "with open('PreTENS-task1-prompt.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = []\n",
        "    for line in prompts_file:\n",
        "        prompt_item = json.loads(line)\n",
        "        prompt_data.append(prompt_item)\n",
        "\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        file_path = 'PreTENS-task1-test-data.jsonl'\n",
        "        prompt_testing(prompt, file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
