{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9FvZqaH1qRX"
      },
      "source": [
        "# **Homework 1 - task 28**\n",
        "The main goal of this homework is to transform existing evaluation datasets into a format suitable for evaluating the linguistic skills of Large Language Models (LLMs) by reframing tasks as multi-choice Question Answering (QA) tasks, providing effective prompts, and generating distractors where necessary, all formatted in JSON Lines standard for submission."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KLwL2mdH9KgG",
        "outputId": "2b4cad03-1874-4172-e207-09efca0537f0"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sz_rtvMA3HO2"
      },
      "source": [
        "# **1. Data Loading:**\n",
        "\n",
        "\n",
        "First we need to download the datasets from evalita to our python eviroment\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03Uc5CmH4wIq",
        "outputId": "0ded34fb-6853-4b38-81cf-d7ddc76fd932"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_1\\SemEval2022Task3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'SemEval2022Task3' already exists and is not an empty directory.\n",
            "'ls' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "##### Data Reader -- English/French/Italian\n",
        "!git clone https://github.com/shammur/SemEval2022Task3.git\n",
        "%cd ./SemEval2022Task3/\n",
        "#data/train/train_subtask-2/\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sub Task 1 \n",
        "\n",
        "**binary classification sub-task**, which consists in predicting the acceptability label assigned to each sentence of the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KemLwOEanl_P"
      },
      "source": [
        "## 1.1 visualize initial data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Change the working directory\n",
        "os.chdir(r'c:\\Users\\35193\\Desktop\\Sapienza\\1 year\\Second Semester\\Multilingual Natural Language Processing\\Homework 1\\GitHub\\NLP_Sapienza\\Task_28\\Subtask_1')\n",
        "\n",
        "# Verify the current working directory\n",
        "print(\"Current working directory:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45PypMtd44ij",
        "outputId": "49483cc0-909e-4732-d177-4d3cee53c9b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tConstruction\tSentence\tLabels\n",
            "it_0\tprefer\tNon amo i campi , preferisco i condomini .\t1\n",
            "it_1\tparticular\tPosso capire le emozioni , e in particolare la tristezza .\t1\n",
            "it_2\tgenerally\tApprezzo i giochi da tavolo , e più in generale il calcio .\t0\n",
            "it_3\tgenerally\tAmo i film , e più in generale i western .\t0\n"
          ]
        }
      ],
      "source": [
        "def visualize_original_data(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            for i in range(5):  \n",
        "                line = file.readline()\n",
        "                print(line.rstrip())\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "\n",
        "# Check test data\n",
        "file_path = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv'\n",
        "visualize_original_data(file_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3stMZxk6ZPK",
        "outputId": "890bf8b7-0a86-419d-e438-eca20df4c7b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "5491\tAmo i cartoni animati , ma non i sussidiari .\t1\n",
            "5084\tApprezzo il vino , ma non il Chianti .\t1\n",
            "1677\tAmo i manuali , un tipo interessante di dipinto .\t0\n",
            "1959\tAmo gli arbusti , ed anche le querce .\t1\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# join the fold training data to a single file\n",
        "\n",
        "def join_tsv_files(file_paths, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for file_path in file_paths:\n",
        "            with open(file_path, 'r') as infile:\n",
        "                for line in infile:\n",
        "                    outfile.write(line)\n",
        "\n",
        "# Example usage\n",
        "file1 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_0.tsv'\n",
        "file2 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_1.tsv'\n",
        "file3 = r'SemEval2022Task3\\data\\train\\train_subtask-1\\it\\It-Subtask1-fold_2.tsv'\n",
        "output_file = 'It-Subtask1-train.tsv'\n",
        "\n",
        "join_tsv_files([file1, file2, file3], output_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID\tSentence\tLabels\n",
            "5491\tAmo i cartoni animati , ma non i sussidiari .\t1\n",
            "5084\tApprezzo il vino , ma non il Chianti .\t1\n",
            "1677\tAmo i manuali , un tipo interessante di dipinto .\t0\n",
            "1959\tAmo gli arbusti , ed anche le querce .\t1\n"
          ]
        }
      ],
      "source": [
        "# Check info in train data\n",
        "file_path = r'It-Subtask1-train.tsv'\n",
        "visualize_original_data(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRnPwyNp7H9S"
      },
      "source": [
        "## 1.2 Data Reframing:\n",
        "  First we need to change the type of the file from xml to json. Then we want to convert that json into the following format:\n",
        "```JSON\n",
        "{\n",
        "    \"id\":       int,\n",
        "    \"text\":     str,\n",
        "    \"choices\":  list[str],\n",
        "    \"label\":    int\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "31jYexPXZnYI"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import json\n",
        "\n",
        "def tsv_to_jsonl(input_file, output_file):\n",
        "    with open(input_file, 'r', newline='') as tsvfile:\n",
        "        reader = csv.DictReader(tsvfile, delimiter='\\t')\n",
        "        with open(output_file, 'w') as jsonlfile:\n",
        "            for row in reader:\n",
        "                json.dump(row, jsonlfile)\n",
        "                jsonlfile.write('\\n')\n",
        "\n",
        "tsv_file = r'SemEval2022Task3\\data\\test\\official_test_set_with_labels\\subtask-1\\It-Subtask1-labels.tsv'\n",
        "jsonl_file = 'It-Subtask1-test.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "tsv_file = 'It-Subtask1-train.tsv'\n",
        "jsonl_file = 'It-Subtask1-train.jsonl'\n",
        "tsv_to_jsonl(tsv_file, jsonl_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyXDo77YX9bA"
      },
      "source": [
        "## 1.3 Visualize jsonl information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOuqfLHqczdA",
        "outputId": "6d7ea7af-81ab-4927-ad97-6b2e227c132a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': 'it_0', 'Construction': 'prefer', 'Sentence': 'Non amo i campi , preferisco i condomini .', 'Labels': '1'}\n",
            "{'ID': 'it_1', 'Construction': 'particular', 'Sentence': 'Posso capire le emozioni , e in particolare la tristezza .', 'Labels': '1'}\n",
            "{'ID': 'it_2', 'Construction': 'generally', 'Sentence': 'Apprezzo i giochi da tavolo , e più in generale il calcio .', 'Labels': '0'}\n",
            "{'ID': 'it_3', 'Construction': 'generally', 'Sentence': 'Amo i film , e più in generale i western .', 'Labels': '0'}\n",
            "{'ID': 'it_4', 'Construction': 'unlike', 'Sentence': 'A differenza dello shopping , il basket è menzionato spesso in questo testo .', 'Labels': '1'}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def visualize_data(jsonl_file):\n",
        "    with open(jsonl_file, 'r') as f:\n",
        "        line_count = 0\n",
        "        for line in f:\n",
        "            data = json.loads(line)\n",
        "            print(data)\n",
        "            line_count += 1\n",
        "            if line_count == 5:\n",
        "                break\n",
        "\n",
        "json_file = 'It-Subtask1-test.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvfeYF7L8cuz",
        "outputId": "227a2ba4-268f-4e1f-f5fc-c4fddfc72101"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': '5491', 'Sentence': 'Amo i cartoni animati , ma non i sussidiari .', 'Labels': '1'}\n",
            "{'ID': '5084', 'Sentence': 'Apprezzo il vino , ma non il Chianti .', 'Labels': '1'}\n",
            "{'ID': '1677', 'Sentence': 'Amo i manuali , un tipo interessante di dipinto .', 'Labels': '0'}\n",
            "{'ID': '1959', 'Sentence': 'Amo gli arbusti , ed anche le querce .', 'Labels': '1'}\n",
            "{'ID': '731', 'Sentence': 'Uso il poliestere , eccetto il vetro .', 'Labels': '0'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-train.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK8cn4OBDRlj"
      },
      "source": [
        "\n",
        "## 1.4 Format jsonl\n",
        "\n",
        "In order to get the json file with the desired format:\n",
        "\n",
        "1. Rename ID to id.\n",
        "2. Rename Sentence to text.\n",
        "3. Add a choices key with non acceptable  or aceptable.\n",
        "4. Rename Labels to label (0- non acceptable and 1 aceptable)\n",
        "5. Delte Contruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "crG0YoFVg-YI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: 'Labels' value 'Labels' is not convertible to int. Setting label to 0.\n",
            "Error: 'Labels' value 'Labels' is not convertible to int. Setting label to 0.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "import json\n",
        "\n",
        "# Rearrange the JSONL file content and write to another JSONL file\n",
        "def rearrange_jsonl(input_file_path, output_file_path):\n",
        "    id_counter = 0\n",
        "    with open(input_file_path, \"r\") as input_file:\n",
        "        with open(output_file_path, \"w\") as output_file:\n",
        "            for line in input_file:\n",
        "                data = json.loads(line)\n",
        "                # Check if 'Labels' key exists and if its value is convertible to an integer\n",
        "                # label = 0  # Default value if 'Labels' key is missing or not convertible to int\n",
        "                # if 'Labels' in data:\n",
        "                #     try:\n",
        "                #         label = int(data['Labels'])\n",
        "                #     except ValueError:\n",
        "                #         print(f\"Error: 'Labels' value '{data['Labels']}' is not convertible to int. Setting label to 0.\")\n",
        "                new_data = {\n",
        "                    #\"id\": int(data['id']),\n",
        "                    \"id\": id_counter, # set id as an increment int\n",
        "                    \"text\": data['Sentence'],\n",
        "                    \"choices\": [\"non accettabile\", \"accettabile\"],\n",
        "                    #\"label\": label\n",
        "                    \"label\": 0 if data['Labels'] == '0' else 1\n",
        "                }\n",
        "                id_counter += 1\n",
        "                json.dump(new_data, output_file)\n",
        "                output_file.write('\\n')\n",
        "\n",
        "input_jsonl_file = 'It-Subtask1-test.jsonl'\n",
        "output_jsonl_file = 'PreTENS-task1-test-data.jsonl'\n",
        "rearrange_jsonl(input_jsonl_file, output_jsonl_file)\n",
        "\n",
        "input_jsonl_file = 'It-Subtask1-train.jsonl'\n",
        "output_jsonl_file = 'PreTENS-task1-train-data.jsonl'\n",
        "rearrange_jsonl(input_jsonl_file, output_jsonl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Check the format of the JSONL file is accorcing to the desired format\n",
        "\n",
        "def check_jsonl_format(jsonl_file):\n",
        "    with open(jsonl_file, 'r') as file:\n",
        "        line_number = 0\n",
        "        for line in file:\n",
        "            line_number += 1\n",
        "            try:\n",
        "                data = json.loads(line.strip())\n",
        "                if not isinstance(data, dict):\n",
        "                    print(f\"Error in line {line_number}: Not a dictionary.\")\n",
        "                    continue\n",
        "                \n",
        "                if not all(key in data for key in ['id', 'text', 'choices', 'label']):\n",
        "                    print(f\"Error in line {line_number}: Missing keys.\")\n",
        "                    continue\n",
        "                \n",
        "                if not isinstance(data['id'], int):\n",
        "                    print(f\"Error in line {line_number}: 'id' is not an integer.\")\n",
        "                \n",
        "                if not isinstance(data['text'], str):\n",
        "                    print(f\"Error in line {line_number}: 'text' is not a string.\")\n",
        "                \n",
        "                if not isinstance(data['choices'], list) or not all(isinstance(choice, str) for choice in data['choices']):\n",
        "                    print(f\"Error in line {line_number}: 'choices' is not a list of strings.\")\n",
        "                \n",
        "                if not isinstance(data['label'], int):\n",
        "                    print(f\"Error in line {line_number}: 'label' is not an integer.\")\n",
        "                \n",
        "            except json.JSONDecodeError:\n",
        "                print(f\"Error in line {line_number}: Not valid JSON.\")\n",
        "                continue\n",
        "\n",
        "\n",
        "jsonl_file = 'PreTENS-task1-test-data.jsonl'\n",
        "check_jsonl_format(jsonl_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UdUFiAoYJDi"
      },
      "source": [
        "## 1.5 Visualize final jsonl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': 'it_0', 'Construction': 'prefer', 'Sentence': 'Non amo i campi , preferisco i condomini .', 'Labels': '1'}\n",
            "{'ID': 'it_1', 'Construction': 'particular', 'Sentence': 'Posso capire le emozioni , e in particolare la tristezza .', 'Labels': '1'}\n",
            "{'ID': 'it_2', 'Construction': 'generally', 'Sentence': 'Apprezzo i giochi da tavolo , e più in generale il calcio .', 'Labels': '0'}\n",
            "{'ID': 'it_3', 'Construction': 'generally', 'Sentence': 'Amo i film , e più in generale i western .', 'Labels': '0'}\n",
            "{'ID': 'it_4', 'Construction': 'unlike', 'Sentence': 'A differenza dello shopping , il basket è menzionato spesso in questo testo .', 'Labels': '1'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-test.jsonl'\n",
        "visualize_data(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CoTfnvAj-KS",
        "outputId": "559cd3c1-f6ed-4dc9-ff7a-d683b5c8ace0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'ID': '5491', 'Sentence': 'Amo i cartoni animati , ma non i sussidiari .', 'Labels': '1'}\n",
            "{'ID': '5084', 'Sentence': 'Apprezzo il vino , ma non il Chianti .', 'Labels': '1'}\n",
            "{'ID': '1677', 'Sentence': 'Amo i manuali , un tipo interessante di dipinto .', 'Labels': '0'}\n",
            "{'ID': '1959', 'Sentence': 'Amo gli arbusti , ed anche le querce .', 'Labels': '1'}\n",
            "{'ID': '731', 'Sentence': 'Uso il poliestere , eccetto il vetro .', 'Labels': '0'}\n"
          ]
        }
      ],
      "source": [
        "json_file = 'It-Subtask1-train.jsonl'\n",
        "visualize_data(json_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRdoWdSVkZ6o"
      },
      "source": [
        "## **2. Promt formulation**\n",
        "\n",
        "In order to use this dataset we need to generate three prompts that can be used to get if a text is entailed to a hypothesis and then insert them into a json file. The three prompts are:\n",
        "\n",
        "**Template 1:**\n",
        "\n",
        "\n",
        "Prompt: \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Evaluate whether the following text is acceptable or not according to the context: {{text}}\"\n",
        "\n",
        "**Template 2:**\n",
        "\n",
        "Prompt: \"Determina se la seguente frase è coerente con il contesto: {{text}}\"\n",
        "\n",
        "Translation: \"Determine if the following sentence is coherent with the context: {{text}}\"\n",
        "\n",
        "**Template 3:**\n",
        "\n",
        "Prompt: \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "\n",
        "Translation: \"Decide if the provided text is congruent with the described situation: {{text}}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3r6Gp2kqX6",
        "outputId": "25bb080b-bcda-4ecc-ac78-7971eb511538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"prompt\": \"Valuta se il seguente testo \\u00e8 accettabile o meno secondo il contesto: {{text}}\"}\n",
            "{\"prompt\": \"Determina se la seguente frase \\u00e8 coerente con il contesto: {{text}}\"}\n",
            "{\"prompt\": \"Decidi se il testo fornito \\u00e8 congruente con la situazione descritta: {{text}}\"}\n",
            "\n",
            "JSON Lines file 'PreTENS-task1-prompt.jsonl' generated successfully with 3 prompts.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Function to generate a JSON Lines file with a list of prompts\n",
        "def generate_prompts(prompts, output_file):\n",
        "    with open(output_file, 'w') as jsonl_file:\n",
        "        for prompt in prompts:\n",
        "            jsonl_file.write(json.dumps({\"prompt\": prompt}) + '\\n')\n",
        "\n",
        "    with open(output_file, 'r', encoding='utf-8') as jsonl_file:\n",
        "        print(jsonl_file.read())\n",
        "\n",
        "\n",
        "prompts = [\n",
        "    \"Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\",\n",
        "    \"Determina se la seguente frase è coerente con il contesto: {{text}}\",\n",
        "    \"Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\"\n",
        "]\n",
        "\n",
        "output_file = \"PreTENS-task1-prompt.jsonl\"\n",
        "generate_prompts(prompts, output_file)\n",
        "print(f\"JSON Lines file '{output_file}' generated successfully with {len(prompts)} prompts.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrpbmKeG4uai"
      },
      "source": [
        "# **3. Llama 2 set up**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_2HXdEu6827D",
        "outputId": "3a39d619-ec8a-4785-c2c4-f62f718177a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cuda'"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Device agnostic code\n",
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "V1tsB0SwOl4B"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "runtimeFlag = device #Run on GPU (you can't run GPTQ on cpu)\n",
        "cache_dir = None # by default, don't set a cache directory. This is automatically updated if you connect Google Drive.\n",
        "scaling_factor = 1.0 # allows for a max sequence length of 16384*6 = 98304! Unfortunately, requires Colab Pro and a V100 or A100 to have sufficient RAM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWc_szlJP-Gx"
      },
      "source": [
        "## 3.1 Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-HYbuJ6JCgd",
        "outputId": "1d078b01-2e78-41cc-86fc-d24885b4fe9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
            "Requirement already satisfied: auto-gptq in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (0.7.1+cu118)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.28.0)\n",
            "Requirement already satisfied: datasets in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.18.0)\n",
            "Requirement already satisfied: sentencepiece in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.2.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.26.4)\n",
            "Requirement already satisfied: rouge in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.1)\n",
            "Requirement already satisfied: gekko in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (1.0.7)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (2.2.1+cu118)\n",
            "Requirement already satisfied: safetensors in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.4.2)\n",
            "Requirement already satisfied: transformers>=4.31.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.39.1)\n",
            "Requirement already satisfied: peft>=0.5.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (0.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from auto-gptq) (4.66.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (23.2)\n",
            "Requirement already satisfied: psutil in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from accelerate>=0.26.0->auto-gptq) (0.22.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (4.9.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from torch>=1.13.0->auto-gptq) (2024.2.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2023.12.25)\n",
            "Requirement already satisfied: requests in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from transformers>=4.31.0->auto-gptq) (0.15.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from tqdm->auto-gptq) (0.4.6)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (15.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.3.8)\n",
            "Requirement already satisfied: pandas in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (2.2.1)\n",
            "Requirement already satisfied: xxhash in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from datasets->auto-gptq) (3.9.3)\n",
            "Requirement already satisfied: six in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from rouge->auto-gptq) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (1.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from requests->transformers>=4.31.0->auto-gptq) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from pandas->datasets->auto-gptq) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum\n",
        "\n",
        "!pip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpbHJwz5jIRc",
        "outputId": "038f994b-971a-46bf-e1f6-6f908ef1d874"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Invalid requirement: '#'\n"
          ]
        }
      ],
      "source": [
        "!pip install -q -U pdfminer.six # could maybe add pre-built wheels to speed this up."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "## 3.2 Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "iO6WB6xkWCrN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "import torch\n",
        "import json\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "model_id = \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\"\n",
        "tokenizer_llama = AutoTokenizer.from_pretrained(model_id)\n",
        "model_llama = AutoModelForSequenceClassification.from_pretrained(model_id).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiAEnqKuPGoR"
      },
      "source": [
        "# **4. Evaluate Homework prompts**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kbdUQxHVD8D"
      },
      "source": [
        "## 4.1. Test response of Promtps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSKyQeQrml_W",
        "outputId": "856c87fa-c12a-4092-fc33-1a0533429aec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probabilities: [9.091138e-04 9.982749e-01 8.160408e-04]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo i cartoni animati , ma non i sussidiari .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 99.8}\n",
            "Actual Label: accettabile\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [2.1190660e-03 9.9753678e-01 3.4415934e-04]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Apprezzo il vino , ma non il Chianti .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 99.8}\n",
            "Actual Label: accettabile\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.00779086 0.9831966  0.00901253]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo i manuali , un tipo interessante di dipinto .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 98.3}\n",
            "Actual Label: non accettabile\n",
            "Correct Prediction: False\n",
            "\n",
            "Probabilities: [0.0090381 0.986324  0.0046379]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Amo gli arbusti , ed anche le querce .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 98.6}\n",
            "Actual Label: accettabile\n",
            "Correct Prediction: True\n",
            "\n",
            "Probabilities: [0.03575339 0.7079393  0.25630733]\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: Uso il poliestere , eccetto il vetro .\n",
            "Prediction: {'label': 'accettabile', 'confidence': 70.8}\n",
            "Actual Label: non accettabile\n",
            "Correct Prediction: False\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def visualize_prompt_response(jsonl_file):\n",
        "    # Load your JSON file and process each pair\n",
        "    with open(jsonl_file, 'r') as file:\n",
        "        count = 0  # Counter to track processed pairs\n",
        "        for line in file:\n",
        "            if count >= 5:  # Break the loop if 5 pairs have been processed\n",
        "                break\n",
        "            pair = json.loads(line)\n",
        "            #id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "            label = choices[label_index]  # Get the actual label using label_index\n",
        "\n",
        "            # Format the prompt with text and hypothesis\n",
        "            prompt_formatted = f\"Valuta se il seguente testo è accettabile o meno secondo il contesto: {text}\"\n",
        "\n",
        "            # Assuming you have tokenizer_llama and model_llama defined elsewhere\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            print(f\"Probabilities: {probabilities}\")\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "            prediction_confidence = probabilities[prediction_index] * 100\n",
        "            prediction = {\"label\": prediction_label, \"confidence\": round(prediction_confidence, 1)}\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "\n",
        "            # Visualize the formatted prompt and the prediction details\n",
        "            print(\"Prompt:\", prompt_formatted)\n",
        "            print(\"Prediction:\", prediction)\n",
        "            print(\"Actual Label:\", label)\n",
        "            print(\"Correct Prediction:\", is_correct)\n",
        "            print()\n",
        "            count += 1  # Increment the counter for processed pairs\n",
        "\n",
        "jsonl_file = 'PreTENS-task1-train-data.jsonl'\n",
        "visualize_prompt_response(jsonl_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5_9dG1CVHo0"
      },
      "source": [
        "## 4.2. Compare the diferent prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prevent CUDA from running out of memory \n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z-4DiwuQPpb",
        "outputId": "1d5a9e15-7957-424f-d7a9-0a73ff3056ae"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "total_correct = 0\n",
        "total_pairs = 0\n",
        "\n",
        "# Load the JSONL file and process each pair\n",
        "with open(jsonl_file, 'r') as file:\n",
        "        for line in file:\n",
        "            pair = json.loads(line)\n",
        "            id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  # Convert label_index to an integer\n",
        "            label = choices[label_index]  # Get the actual label using label_index\n",
        "    \n",
        "            # Format the prompt with actual text and hypothesis\n",
        "            prompt_formatted = f\"Valuta se il seguente testo \\u00e8 accettabile o meno secondo il contesto: {text}\"\n",
        "\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "\n",
        "# Calculate overall statistics only if at least one pair was processed\n",
        "if total_pairs > 0:\n",
        "    accuracy = total_correct / total_pairs * 100\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"Overall Statistics:\")\n",
        "    print(\"Total Pairs:\", total_pairs)\n",
        "    print(\"Total Correct:\", total_correct)\n",
        "    print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "else:\n",
        "    print(\"No pairs found in the JSONL file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6852\n",
            "Accuracy: 47.06 %\n",
            "Mean Confidence: 0.93\n",
            "\n",
            "Prompt: Determina se la seguente frase è coerente con il contesto: {{text}}\n",
            "Total Pairs: 14560\n",
            "Total Correct: 6852\n",
            "Accuracy: 47.06 %\n",
            "Mean Confidence: 0.97\n",
            "\n",
            "Prompt: Decidi se il testo fornito è congruente con la situazione descritta: {{text}}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[59], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt)\n\u001b[0;32m     57\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreTENS-task1-test-data.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 58\u001b[0m \u001b[43mprompt_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[59], line 22\u001b[0m, in \u001b[0;36mprompt_testing\u001b[1;34m(prompt, file_path)\u001b[0m\n\u001b[0;32m     20\u001b[0m prompt_formatted \u001b[38;5;241m=\u001b[39m prompt\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{{\u001b[39m\u001b[38;5;124mtext}}\u001b[39m\u001b[38;5;124m'\u001b[39m, text)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tokenizer_llama(prompt_formatted, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 22\u001b[0m output \u001b[38;5;241m=\u001b[39m model_llama(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m     23\u001b[0m logits \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mlogits[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Obtain the predicted label directly from logits\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1300\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1294\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1298\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1300\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1311\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1312\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:1070\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1060\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m   1062\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1063\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1064\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1067\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1068\u001b[0m )\n\u001b[1;32m-> 1070\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1072\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1079\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:514\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[1;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[0;32m    504\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    505\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    506\u001b[0m         next_kv,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         output_attentions,\n\u001b[0;32m    512\u001b[0m     )\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 514\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    524\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:362\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[1;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    354\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    355\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    360\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m ):\n\u001b[1;32m--> 362\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    371\u001b[0m         attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:293\u001b[0m, in \u001b[0;36mDebertaV2Attention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    286\u001b[0m     hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m     rel_embeddings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    292\u001b[0m ):\n\u001b[1;32m--> 293\u001b[0m     self_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    298\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[0;32m    302\u001b[0m         self_output, att_matrix \u001b[38;5;241m=\u001b[39m self_output\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:721\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[0m\n\u001b[0;32m    719\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention:\n\u001b[0;32m    720\u001b[0m     rel_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_dropout(rel_embeddings)\n\u001b[1;32m--> 721\u001b[0m     rel_att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisentangled_attention_bias\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    726\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m rel_att\n",
            "File \u001b[1;32mc:\\Users\\35193\\miniconda3\\envs\\gpukernel\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:817\u001b[0m, in \u001b[0;36mDisentangledSelfAttention.disentangled_attention_bias\u001b[1;34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    815\u001b[0m     r_pos \u001b[38;5;241m=\u001b[39m relative_pos\n\u001b[1;32m--> 817\u001b[0m p2c_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mr_pos\u001b[49m \u001b[38;5;241m+\u001b[39m att_span, \u001b[38;5;241m0\u001b[39m, att_span \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    818\u001b[0m p2c_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(key_layer, pos_query_layer\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    819\u001b[0m p2c_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(\n\u001b[0;32m    820\u001b[0m     p2c_att,\n\u001b[0;32m    821\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    822\u001b[0m     index\u001b[38;5;241m=\u001b[39mp2c_pos\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand([query_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), key_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), key_layer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)]),\n\u001b[0;32m    823\u001b[0m )\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import json\n",
        "import torch\n",
        "\n",
        "def prompt_testing(prompt, file_path):\n",
        "    total_correct = 0\n",
        "    total_pairs = 0\n",
        "    total_confidence = 0\n",
        "\n",
        "    # Load the JSONL file and process each pair\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            pair = json.loads(line)\n",
        "            #id = pair['id']\n",
        "            text = pair['text']\n",
        "            choices = pair['choices']\n",
        "            label_index = int(pair['label'])  \n",
        "            label = choices[label_index]  \n",
        "            \n",
        "            # Format with text and hypothesis \n",
        "            prompt_formatted = prompt.replace('{{text}}', text)\n",
        "            input = tokenizer_llama(prompt_formatted, return_tensors=\"pt\").to(device)\n",
        "            output = model_llama(**input)\n",
        "            logits = output.logits[0].detach().cpu().numpy()\n",
        "\n",
        "            # Obtain the predicted label directly from logits\n",
        "            probabilities = torch.softmax(torch.tensor(logits), -1).detach().cpu().numpy()\n",
        "            prediction_index = 1 if probabilities[1] > probabilities[0] else 0\n",
        "            prediction_label = choices[prediction_index]\n",
        "\n",
        "            # Compare prediction with actual label\n",
        "            is_correct = prediction_label == label\n",
        "\n",
        "            # Accumulate statistics\n",
        "            total_correct += is_correct\n",
        "            total_pairs += 1\n",
        "            total_confidence += probabilities[prediction_index]\n",
        "\n",
        "    # Calculate statistics \n",
        "    if total_pairs > 0:\n",
        "        accuracy = total_correct / total_pairs * 100\n",
        "        mean_confidence = total_confidence / total_pairs\n",
        "\n",
        "        # Print statistics\n",
        "        print(\"Total Pairs:\", total_pairs)\n",
        "        print(\"Total Correct:\", total_correct)\n",
        "        print(\"Accuracy:\", round(accuracy, 2), \"%\")\n",
        "        print(\"Mean Confidence:\", round(mean_confidence, 2))\n",
        "    else:\n",
        "        print(\"No pairs found in the JSONL file.\")\n",
        "\n",
        "# Load prompts from the JSONL file and test each prompt\n",
        "with open('prompts.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = json.load(prompts_file)\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        file_path = 'PreTENS-task1-test-data.jsonl'\n",
        "        prompt_testing(prompt, file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4.3 Check the difference of accuracy between test and trainind dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Prompt: Valuta se il seguente testo è accettabile o meno secondo il contesto: {{text}}\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'prompt_testing' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPrompt:\u001b[39m\u001b[38;5;124m\"\u001b[39m, prompt)\n\u001b[0;32m      6\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPreTENS-task1-train-data.jsonl\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mprompt_testing\u001b[49m(prompt, file_path)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'prompt_testing' is not defined"
          ]
        }
      ],
      "source": [
        "with open('prompts.jsonl', 'r') as prompts_file:\n",
        "    prompt_data = json.load(prompts_file)\n",
        "    for prompt_item in prompt_data:\n",
        "        prompt = prompt_item[\"prompt\"]\n",
        "        print(\"\\nPrompt:\", prompt)\n",
        "        file_path = 'PreTENS-task1-train-data.jsonl'\n",
        "        prompt_testing(prompt, file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
